{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RBhoWzYFtMl",
        "outputId": "dd3d2484-89b7-43a1-ea7b-322a1f14c763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe Data"
      ],
      "metadata": {
        "id": "NefZU4gWFMrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "print(train_data.describe())\n",
        "print(train_data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fEQ6_SeFI6m",
        "outputId": "30a8c1c8-65c5-447b-c9a0-75aaa3f8acbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                id     margin1     margin2     margin3     margin4  \\\n",
            "count   990.000000  990.000000  990.000000  990.000000  990.000000   \n",
            "mean    799.595960    0.017412    0.028539    0.031988    0.023280   \n",
            "std     452.477568    0.019739    0.038855    0.025847    0.028411   \n",
            "min       1.000000    0.000000    0.000000    0.000000    0.000000   \n",
            "25%     415.250000    0.001953    0.001953    0.013672    0.005859   \n",
            "50%     802.500000    0.009766    0.011719    0.025391    0.013672   \n",
            "75%    1195.500000    0.025391    0.041016    0.044922    0.029297   \n",
            "max    1584.000000    0.087891    0.205080    0.156250    0.169920   \n",
            "\n",
            "          margin5     margin6     margin7     margin8     margin9  ...  \\\n",
            "count  990.000000  990.000000  990.000000  990.000000  990.000000  ...   \n",
            "mean     0.014264    0.038579    0.019202    0.001083    0.007167  ...   \n",
            "std      0.018390    0.052030    0.017511    0.002743    0.008933  ...   \n",
            "min      0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
            "25%      0.001953    0.000000    0.005859    0.000000    0.001953  ...   \n",
            "50%      0.007812    0.015625    0.015625    0.000000    0.005859  ...   \n",
            "75%      0.017578    0.056153    0.029297    0.000000    0.007812  ...   \n",
            "max      0.111330    0.310550    0.091797    0.031250    0.076172  ...   \n",
            "\n",
            "        texture55   texture56   texture57   texture58   texture59   texture60  \\\n",
            "count  990.000000  990.000000  990.000000  990.000000  990.000000  990.000000   \n",
            "mean     0.036501    0.005024    0.015944    0.011586    0.016108    0.014017   \n",
            "std      0.063403    0.019321    0.023214    0.025040    0.015335    0.060151   \n",
            "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
            "25%      0.000000    0.000000    0.000977    0.000000    0.004883    0.000000   \n",
            "50%      0.004883    0.000000    0.005859    0.000977    0.012695    0.000000   \n",
            "75%      0.043701    0.000000    0.022217    0.009766    0.021484    0.000000   \n",
            "max      0.429690    0.202150    0.172850    0.200200    0.106450    0.578130   \n",
            "\n",
            "        texture61   texture62   texture63   texture64  \n",
            "count  990.000000  990.000000  990.000000  990.000000  \n",
            "mean     0.002688    0.020291    0.008989    0.019420  \n",
            "std      0.011415    0.039040    0.013791    0.022768  \n",
            "min      0.000000    0.000000    0.000000    0.000000  \n",
            "25%      0.000000    0.000000    0.000000    0.000977  \n",
            "50%      0.000000    0.003906    0.002930    0.011719  \n",
            "75%      0.000000    0.023438    0.012695    0.029297  \n",
            "max      0.151370    0.375980    0.086914    0.141600  \n",
            "\n",
            "[8 rows x 193 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 990 entries, 0 to 989\n",
            "Columns: 194 entries, id to texture64\n",
            "dtypes: float64(192), int64(1), object(1)\n",
            "memory usage: 1.5+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look For Null Values"
      ],
      "metadata": {
        "id": "8JbPYKyfFWJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMissing values in the dataset:\")\n",
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYPG9oYYFUFp",
        "outputId": "216bd8b9-e926-4b7e-d363-1009fabf79d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values in the dataset:\n",
            "id           0\n",
            "species      0\n",
            "margin1      0\n",
            "margin2      0\n",
            "margin3      0\n",
            "            ..\n",
            "texture60    0\n",
            "texture61    0\n",
            "texture62    0\n",
            "texture63    0\n",
            "texture64    0\n",
            "Length: 194, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove duplicates"
      ],
      "metadata": {
        "id": "Ntn1ec9cFbbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "duplicates = train_data.duplicated()\n",
        "print(f\"\\nNumber of duplicate rows: {duplicates.sum()}\")\n",
        "\n",
        "# Remove duplicates if any\n",
        "data = train_data.drop_duplicates()\n",
        "print(f\"Dataset after removing duplicates: {data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5YYwBI9FbA_",
        "outputId": "32ea18c6-bc62-47db-840a-7d7ab8f3b8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of duplicate rows: 0\n",
            "Dataset after removing duplicates: (990, 194)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data & Images"
      ],
      "metadata": {
        "id": "0Lryg6OtGD65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the image folder\n",
        "image_folder = '/content/drive/MyDrive/Leaf Classification Images/images'\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_image(img_path, target_size=(192, 192)):\n",
        "    img_full_path = os.path.join(image_folder, str(img_path) + '.jpg')  # Full path to the image\n",
        "    img = image.load_img(img_full_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array /= 255.0  # Normalize to [0, 1]\n",
        "    return img_array\n",
        "\n",
        "\n",
        "image_ids = train_data['id']\n",
        "labels = train_data['species']\n",
        "tabular_features = train_data.drop(columns=['id', 'species'])\n",
        "#apply correlation analysis and remove duplicate features\n",
        "correlation_matrix = tabular_features.corr()\n",
        "threshold=0.85\n",
        "to_drop=set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            to_drop.add(colname)\n",
        "tabular_features = tabular_features.drop(columns=to_drop)\n",
        "print(f\"Dropped Features: {to_drop}\")\n",
        "\n",
        "# Normalize tabular features\n",
        "scaler = StandardScaler()\n",
        "tabular_features = scaler.fit_transform(tabular_features)\n",
        "\n",
        "# 3. Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "tabular_features = pca.fit_transform(tabular_features)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split data\n",
        "X_train_img, X_test_img, y_train, y_test = train_test_split(image_ids, labels_encoded, test_size=0.2, random_state=42)\n",
        "X_train_tabular, X_test_tabular = train_test_split(tabular_features, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load and preprocess images for training and testing\n",
        "X_train_images = np.array([load_image(img) for img in X_train_img])\n",
        "X_test_images = np.array([load_image(img) for img in X_test_img])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1msFQ88OGDWp",
        "outputId": "ad21ced6-6b0e-411e-fc47-9844e5165af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped Features: {'shape48', 'shape34', 'shape37', 'shape10', 'shape53', 'shape6', 'shape52', 'shape60', 'shape55', 'shape57', 'shape25', 'shape46', 'shape13', 'shape28', 'shape40', 'shape15', 'shape14', 'shape4', 'shape44', 'shape18', 'shape61', 'shape51', 'shape58', 'shape12', 'shape16', 'shape9', 'shape41', 'shape62', 'shape56', 'shape45', 'shape59', 'shape47', 'shape43', 'shape50', 'shape11', 'shape27', 'shape64', 'shape30', 'shape7', 'shape29', 'shape31', 'shape21', 'shape49', 'margin39', 'shape5', 'shape17', 'shape3', 'shape19', 'shape22', 'shape23', 'shape32', 'shape33', 'shape8', 'shape39', 'shape35', 'margin49', 'shape2', 'shape38', 'shape63', 'shape24', 'shape36', 'shape20', 'shape26', 'shape54', 'shape42'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Some Images"
      ],
      "metadata": {
        "id": "L0S2M4RlH7Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and display an image\n",
        "def display_images(image_ids, rows=2, cols=5):\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 6))\n",
        "    for i, img_id in enumerate(image_ids[:rows * cols]):\n",
        "        img_path = os.path.join(image_folder, str(img_id) + '.jpg')\n",
        "        img = image.load_img(img_path, target_size=(64, 64))\n",
        "        axes[i // cols, i % cols].imshow(img)\n",
        "        axes[i // cols, i % cols].axis('off')\n",
        "        axes[i // cols, i % cols].set_title(f'ID: {img_id}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display the first 10 images\n",
        "display_images(data['id'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "_KGnWFXwH6f2",
        "outputId": "ee35da4d-07b2-4468-d5c4-466bc8a97e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pixels Histogram"
      ],
      "metadata": {
        "id": "D8t3YxGGv6FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten all images to a 1D array\n",
        "pixel_values = np.array(X_train_images).flatten()\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(pixel_values, bins=30, color='blue', alpha=0.7)\n",
        "plt.title(\"Pixel Value Distribution\")\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xyAJ2lhFLQ6l",
        "outputId": "f44d1a5c-af52-45ad-f679-403eb179d377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architecture"
      ],
      "metadata": {
        "id": "0uqxJR2TIJ9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs, batch_size, learning_rate, dropout, optimizer_name, l2_factor):\n",
        "    # Define the CNN model for image input\n",
        "    image_input = Input(shape=(192, 192, 3))\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "\n",
        "    # Define the Dense model for tabular input\n",
        "    tabular_input = Input(shape=(X_train_tabular.shape[1],))\n",
        "    y = Dense(128, activation='relu')(tabular_input)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = Dense(64, activation='relu')(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "\n",
        "    # Combine image and tabular models\n",
        "    combined = Concatenate()([x, y])\n",
        "    z = Dense(512, activation='relu')(combined)\n",
        "    z = Dropout(dropout)(z)\n",
        "    z = Dense(256, activation='relu')(z)\n",
        "    output = Dense(len(np.unique(y_train)), activation='softmax')(z)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizers_dict = {\n",
        "        'adam': Adam(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "        'sgd': SGD(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "        'rmsprop': RMSprop(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "    }\n",
        "    optimizer = optimizers_dict.get(optimizer_name.lower())\n",
        "    if not optimizer:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer_name}'. Available: {list(optimizers_dict.keys())}\")\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(filepath='best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "    log_dir = f\"logs/fit/optimizer_{optimizer_name}_l2_{l2_factor}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_train_images, X_train_tabular],\n",
        "        y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint_callback, tensorboard_callback]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_loss, train_acc = model.evaluate([X_train_images, X_train_tabular], y_train, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate([X_test_images, X_test_tabular], y_test, verbose=0)\n",
        "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "def training2(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs, batch_size, learning_rate, dropout, optimizer_name, l2_factor):\n",
        "    # Define the CNN model for image input\n",
        "    image_input = Input(shape=(192, 192, 3))\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "\n",
        "    # Define the Dense model for tabular input\n",
        "    tabular_input = Input(shape=(X_train_tabular.shape[1],))\n",
        "    y = Dense(128, activation='relu')(tabular_input)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = Dense(64, activation='relu')(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "\n",
        "    # Combine image and tabular models\n",
        "    combined = Concatenate()([x, y])\n",
        "    z = Dense(512, activation='relu')(combined)\n",
        "    z = Dropout(dropout)(z)\n",
        "    z = Dense(256, activation='relu')(z)\n",
        "    output = Dense(len(np.unique(y_train)), activation='softmax')(z)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizers_dict = {\n",
        "        'adam': Adam(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "        'sgd': SGD(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "        'rmsprop': RMSprop(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "    }\n",
        "    optimizer = optimizers_dict.get(optimizer_name.lower())\n",
        "    if not optimizer:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer_name}'. Available: {list(optimizers_dict.keys())}\")\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(filepath='best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "    log_dir = f\"logs/fit/optimizer_{optimizer_name}_l2_{l2_factor}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_train_images, X_train_tabular],\n",
        "        y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint_callback, tensorboard_callback]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_loss, train_acc = model.evaluate([X_train_images, X_train_tabular], y_train, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate([X_test_images, X_test_tabular], y_test, verbose=0)\n",
        "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training3(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs, batch_size, learning_rate, dropout, optimizer_name, l2_factor):\n",
        "    # Define the CNN model for image input\n",
        "    image_input = Input(shape=(192, 192, 3))\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "\n",
        "    # Define the Dense model for tabular input\n",
        "    tabular_input = Input(shape=(X_train_tabular.shape[1],))\n",
        "    y = Dense(128, activation='relu')(tabular_input)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = Dense(64, activation='relu')(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "\n",
        "    # Combine image and tabular models\n",
        "    combined = Concatenate()([x, y])\n",
        "    z = Dense(512, activation='relu')(combined)\n",
        "    z = Dropout(dropout)(z)\n",
        "    z = Dense(256, activation='relu')(z)\n",
        "    output = Dense(len(np.unique(y_train)), activation='softmax')(z)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizers_dict = {\n",
        "        'adam': Adam(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "        'sgd': SGD(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "        'rmsprop': RMSprop(learning_rate=learning_rate, weight_decay=l2_factor),\n",
        "    }\n",
        "    optimizer = optimizers_dict.get(optimizer_name.lower())\n",
        "    if not optimizer:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer_name}'. Available: {list(optimizers_dict.keys())}\")\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(filepath='best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "    log_dir = f\"logs/fit/optimizer_{optimizer_name}_l2_{l2_factor}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        [X_train_images, X_train_tabular],\n",
        "        y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint_callback, tensorboard_callback]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    train_loss, train_acc = model.evaluate([X_train_images, X_train_tabular], y_train, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate([X_test_images, X_test_tabular], y_test, verbose=0)\n",
        "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T6GUgFPvNKDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Different Learning Rates"
      ],
      "metadata": {
        "id": "rjIX5XWtwpRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of learning rates to experiment with\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "\n",
        "# Loop through each learning rate and train the model\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "    training2(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs=55, batch_size=32, learning_rate=lr,dropout=0.2, optimizer_name=\"adam\", l2_factor=0.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UewHNWXgNtea",
        "outputId": "e23fc8c5-20d4-4ee7-c1e4-ab6272de370d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with learning rate: 0.0001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_30             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_45     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_45           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_46     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_46           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_31             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_47     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_75 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_47           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_61 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_76 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_60 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_62 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_15             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_77 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_63 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_78 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_79 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_30             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_45     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_45           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_46     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_46           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_31             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_47     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_47           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_15             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.0195 - loss: 4.6423\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 456ms/step - accuracy: 0.0194 - loss: 4.6421 - val_accuracy: 0.0126 - val_loss: 4.5798\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0300 - loss: 4.5197\n",
            "Epoch 2: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.0299 - loss: 4.5204 - val_accuracy: 0.0126 - val_loss: 4.5665\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.0562 - loss: 4.4469\n",
            "Epoch 3: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.0561 - loss: 4.4472 - val_accuracy: 0.0063 - val_loss: 4.5540\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0650 - loss: 4.3780\n",
            "Epoch 4: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.0642 - loss: 4.3789 - val_accuracy: 0.0126 - val_loss: 4.5430\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0609 - loss: 4.3515\n",
            "Epoch 5: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0609 - loss: 4.3506 - val_accuracy: 0.0126 - val_loss: 4.5324\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0715 - loss: 4.2621\n",
            "Epoch 6: val_accuracy improved from 0.01258 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0722 - loss: 4.2607 - val_accuracy: 0.0252 - val_loss: 4.5216\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.1024 - loss: 4.1586\n",
            "Epoch 7: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.1021 - loss: 4.1598 - val_accuracy: 0.0252 - val_loss: 4.5063\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.1084 - loss: 4.1030\n",
            "Epoch 8: val_accuracy improved from 0.02516 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.1087 - loss: 4.1021 - val_accuracy: 0.0377 - val_loss: 4.4935\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1572 - loss: 4.0330\n",
            "Epoch 9: val_accuracy improved from 0.03774 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.1586 - loss: 4.0304 - val_accuracy: 0.0440 - val_loss: 4.4754\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.1797 - loss: 3.8914\n",
            "Epoch 10: val_accuracy improved from 0.04403 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.1808 - loss: 3.8897 - val_accuracy: 0.0503 - val_loss: 4.4486\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2369 - loss: 3.6826\n",
            "Epoch 11: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.2368 - loss: 3.6837 - val_accuracy: 0.0314 - val_loss: 4.4145\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.3112 - loss: 3.5554\n",
            "Epoch 12: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.3103 - loss: 3.5554 - val_accuracy: 0.0314 - val_loss: 4.3981\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3428 - loss: 3.3658\n",
            "Epoch 13: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.3412 - loss: 3.3667 - val_accuracy: 0.0377 - val_loss: 4.3417\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3557 - loss: 3.2417\n",
            "Epoch 14: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.3554 - loss: 3.2408 - val_accuracy: 0.0314 - val_loss: 4.3148\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4005 - loss: 3.0521\n",
            "Epoch 15: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.4008 - loss: 3.0513 - val_accuracy: 0.0503 - val_loss: 4.2320\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4685 - loss: 2.7711\n",
            "Epoch 16: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.4671 - loss: 2.7729 - val_accuracy: 0.0314 - val_loss: 4.2113\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5033 - loss: 2.5921\n",
            "Epoch 17: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5022 - loss: 2.5931 - val_accuracy: 0.0503 - val_loss: 4.1146\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5139 - loss: 2.4775\n",
            "Epoch 18: val_accuracy improved from 0.05031 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5135 - loss: 2.4760 - val_accuracy: 0.0755 - val_loss: 3.9905\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5701 - loss: 2.3077\n",
            "Epoch 19: val_accuracy did not improve from 0.07547\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.5689 - loss: 2.3083 - val_accuracy: 0.0692 - val_loss: 4.0653\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6006 - loss: 2.0964\n",
            "Epoch 20: val_accuracy improved from 0.07547 to 0.11950, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.6006 - loss: 2.0959 - val_accuracy: 0.1195 - val_loss: 3.6778\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5579 - loss: 2.0814\n",
            "Epoch 21: val_accuracy improved from 0.11950 to 0.17610, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5601 - loss: 2.0773 - val_accuracy: 0.1761 - val_loss: 3.5479\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6492 - loss: 1.7909\n",
            "Epoch 22: val_accuracy improved from 0.17610 to 0.19497, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6484 - loss: 1.7916 - val_accuracy: 0.1950 - val_loss: 3.4636\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6733 - loss: 1.6748\n",
            "Epoch 23: val_accuracy improved from 0.19497 to 0.23270, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.6730 - loss: 1.6745 - val_accuracy: 0.2327 - val_loss: 3.2716\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6973 - loss: 1.5511\n",
            "Epoch 24: val_accuracy improved from 0.23270 to 0.23899, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6962 - loss: 1.5529 - val_accuracy: 0.2390 - val_loss: 3.1073\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7235 - loss: 1.4912\n",
            "Epoch 25: val_accuracy improved from 0.23899 to 0.33333, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7227 - loss: 1.4911 - val_accuracy: 0.3333 - val_loss: 2.8003\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7170 - loss: 1.4184\n",
            "Epoch 26: val_accuracy improved from 0.33333 to 0.45283, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7169 - loss: 1.4181 - val_accuracy: 0.4528 - val_loss: 2.3598\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7119 - loss: 1.2853\n",
            "Epoch 27: val_accuracy did not improve from 0.45283\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7120 - loss: 1.2863 - val_accuracy: 0.3962 - val_loss: 2.4880\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7116 - loss: 1.2811\n",
            "Epoch 28: val_accuracy improved from 0.45283 to 0.47170, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.7121 - loss: 1.2799 - val_accuracy: 0.4717 - val_loss: 2.2831\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7217 - loss: 1.2778\n",
            "Epoch 29: val_accuracy improved from 0.47170 to 0.52201, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.7232 - loss: 1.2728 - val_accuracy: 0.5220 - val_loss: 2.0768\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7611 - loss: 1.0985\n",
            "Epoch 30: val_accuracy improved from 0.52201 to 0.59748, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7611 - loss: 1.0996 - val_accuracy: 0.5975 - val_loss: 1.7533\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7805 - loss: 1.0562\n",
            "Epoch 31: val_accuracy did not improve from 0.59748\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7796 - loss: 1.0570 - val_accuracy: 0.5660 - val_loss: 1.7641\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7936 - loss: 0.9640\n",
            "Epoch 32: val_accuracy did not improve from 0.59748\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7934 - loss: 0.9653 - val_accuracy: 0.5975 - val_loss: 1.5194\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8090 - loss: 0.9415\n",
            "Epoch 33: val_accuracy improved from 0.59748 to 0.66667, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8084 - loss: 0.9409 - val_accuracy: 0.6667 - val_loss: 1.3988\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8356 - loss: 0.8566\n",
            "Epoch 34: val_accuracy improved from 0.66667 to 0.74214, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8345 - loss: 0.8573 - val_accuracy: 0.7421 - val_loss: 1.1826\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8315 - loss: 0.8226\n",
            "Epoch 35: val_accuracy did not improve from 0.74214\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.8313 - loss: 0.8234 - val_accuracy: 0.7107 - val_loss: 1.1875\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8430 - loss: 0.7732\n",
            "Epoch 36: val_accuracy improved from 0.74214 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8431 - loss: 0.7727 - val_accuracy: 0.8176 - val_loss: 0.9343\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8356 - loss: 0.7077\n",
            "Epoch 37: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8355 - loss: 0.7094 - val_accuracy: 0.8176 - val_loss: 0.9057\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8614 - loss: 0.7131\n",
            "Epoch 38: val_accuracy improved from 0.81761 to 0.84277, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8604 - loss: 0.7142 - val_accuracy: 0.8428 - val_loss: 0.8880\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8772 - loss: 0.6470\n",
            "Epoch 39: val_accuracy did not improve from 0.84277\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8771 - loss: 0.6478 - val_accuracy: 0.8176 - val_loss: 0.9003\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8464 - loss: 0.6913\n",
            "Epoch 40: val_accuracy improved from 0.84277 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8469 - loss: 0.6899 - val_accuracy: 0.8491 - val_loss: 0.7677\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8701 - loss: 0.5970\n",
            "Epoch 41: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8699 - loss: 0.5972 - val_accuracy: 0.8365 - val_loss: 0.7307\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8836 - loss: 0.5610\n",
            "Epoch 42: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8842 - loss: 0.5607 - val_accuracy: 0.8365 - val_loss: 0.7397\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8779 - loss: 0.5706\n",
            "Epoch 43: val_accuracy improved from 0.84906 to 0.86164, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8781 - loss: 0.5696 - val_accuracy: 0.8616 - val_loss: 0.7079\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8833 - loss: 0.5578\n",
            "Epoch 44: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.8831 - loss: 0.5581 - val_accuracy: 0.7925 - val_loss: 0.7855\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8885 - loss: 0.5501\n",
            "Epoch 45: val_accuracy improved from 0.86164 to 0.86792, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8883 - loss: 0.5501 - val_accuracy: 0.8679 - val_loss: 0.6360\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9001 - loss: 0.4862\n",
            "Epoch 46: val_accuracy improved from 0.86792 to 0.88679, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.8999 - loss: 0.4866 - val_accuracy: 0.8868 - val_loss: 0.5816\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9335 - loss: 0.4301\n",
            "Epoch 47: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9325 - loss: 0.4312 - val_accuracy: 0.8553 - val_loss: 0.5970\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9011 - loss: 0.4466\n",
            "Epoch 48: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9010 - loss: 0.4473 - val_accuracy: 0.8302 - val_loss: 0.7173\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8874 - loss: 0.5053\n",
            "Epoch 49: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8874 - loss: 0.5040 - val_accuracy: 0.8868 - val_loss: 0.5346\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9218 - loss: 0.4103\n",
            "Epoch 50: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9213 - loss: 0.4114 - val_accuracy: 0.8742 - val_loss: 0.4903\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8910 - loss: 0.4566\n",
            "Epoch 51: val_accuracy improved from 0.88679 to 0.91195, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.8912 - loss: 0.4562 - val_accuracy: 0.9119 - val_loss: 0.4964\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9332 - loss: 0.3736\n",
            "Epoch 52: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9333 - loss: 0.3739 - val_accuracy: 0.8868 - val_loss: 0.5090\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9409 - loss: 0.3466\n",
            "Epoch 53: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9404 - loss: 0.3475 - val_accuracy: 0.9057 - val_loss: 0.5074\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9210 - loss: 0.3646\n",
            "Epoch 54: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9209 - loss: 0.3649 - val_accuracy: 0.9057 - val_loss: 0.4713\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9167 - loss: 0.3789\n",
            "Epoch 55: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9163 - loss: 0.3807 - val_accuracy: 0.8931 - val_loss: 0.4296\n",
            "Train Accuracy: 0.9785, Test Accuracy: 0.9444\n",
            "Training with learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_16\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_16\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_32             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_50 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_48     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_48           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_51 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_49     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_49           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_52 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_33             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_50     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_80 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_50           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_65 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_81 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_64 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_66 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_16             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_82 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_67 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_83 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_84 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_32             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_48     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_48           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_49     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_49           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_33             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_50     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_50           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_16             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.0152 - loss: 4.6093\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 475ms/step - accuracy: 0.0156 - loss: 4.6059 - val_accuracy: 0.0063 - val_loss: 4.5572\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0994 - loss: 4.0763\n",
            "Epoch 2: val_accuracy improved from 0.00629 to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.1005 - loss: 4.0690 - val_accuracy: 0.0126 - val_loss: 4.9215\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2631 - loss: 3.2222\n",
            "Epoch 3: val_accuracy improved from 0.01258 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.2654 - loss: 3.2107 - val_accuracy: 0.0566 - val_loss: 5.5615\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.4963 - loss: 2.1270\n",
            "Epoch 4: val_accuracy improved from 0.05660 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.4966 - loss: 2.1223 - val_accuracy: 0.0629 - val_loss: 6.4314\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6495 - loss: 1.3873\n",
            "Epoch 5: val_accuracy improved from 0.06289 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6483 - loss: 1.3862 - val_accuracy: 0.0818 - val_loss: 6.9560\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6878 - loss: 1.0493\n",
            "Epoch 6: val_accuracy did not improve from 0.08176\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6885 - loss: 1.0465 - val_accuracy: 0.0629 - val_loss: 8.7484\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8168 - loss: 0.7164\n",
            "Epoch 7: val_accuracy improved from 0.08176 to 0.11950, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.8167 - loss: 0.7155 - val_accuracy: 0.1195 - val_loss: 8.1396\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8100 - loss: 0.6432\n",
            "Epoch 8: val_accuracy did not improve from 0.11950\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8112 - loss: 0.6401 - val_accuracy: 0.1006 - val_loss: 8.5394\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8613 - loss: 0.4447\n",
            "Epoch 9: val_accuracy improved from 0.11950 to 0.19497, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8616 - loss: 0.4443 - val_accuracy: 0.1950 - val_loss: 7.9587\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8682 - loss: 0.4379\n",
            "Epoch 10: val_accuracy did not improve from 0.19497\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8687 - loss: 0.4365 - val_accuracy: 0.1447 - val_loss: 9.0741\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9233 - loss: 0.3198\n",
            "Epoch 11: val_accuracy did not improve from 0.19497\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9232 - loss: 0.3199 - val_accuracy: 0.1761 - val_loss: 8.8856\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9418 - loss: 0.2433\n",
            "Epoch 12: val_accuracy did not improve from 0.19497\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9410 - loss: 0.2448 - val_accuracy: 0.1384 - val_loss: 8.4374\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9285 - loss: 0.2414\n",
            "Epoch 13: val_accuracy improved from 0.19497 to 0.24528, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.9284 - loss: 0.2420 - val_accuracy: 0.2453 - val_loss: 6.2939\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8969 - loss: 0.2661\n",
            "Epoch 14: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8978 - loss: 0.2651 - val_accuracy: 0.1698 - val_loss: 10.0492\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9263 - loss: 0.2146\n",
            "Epoch 15: val_accuracy improved from 0.24528 to 0.35849, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9267 - loss: 0.2145 - val_accuracy: 0.3585 - val_loss: 4.1530\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9432 - loss: 0.1789\n",
            "Epoch 16: val_accuracy did not improve from 0.35849\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9428 - loss: 0.1797 - val_accuracy: 0.3333 - val_loss: 3.9157\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9515 - loss: 0.1723\n",
            "Epoch 17: val_accuracy did not improve from 0.35849\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9509 - loss: 0.1739 - val_accuracy: 0.3019 - val_loss: 4.3908\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9176 - loss: 0.2044\n",
            "Epoch 18: val_accuracy improved from 0.35849 to 0.53459, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9179 - loss: 0.2046 - val_accuracy: 0.5346 - val_loss: 2.7306\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9477 - loss: 0.1748\n",
            "Epoch 19: val_accuracy did not improve from 0.53459\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9475 - loss: 0.1753 - val_accuracy: 0.5031 - val_loss: 2.3729\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9481 - loss: 0.1507\n",
            "Epoch 20: val_accuracy improved from 0.53459 to 0.55346, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9481 - loss: 0.1514 - val_accuracy: 0.5535 - val_loss: 2.5464\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9597 - loss: 0.1456\n",
            "Epoch 21: val_accuracy improved from 0.55346 to 0.64780, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9596 - loss: 0.1453 - val_accuracy: 0.6478 - val_loss: 1.7479\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9678 - loss: 0.0966\n",
            "Epoch 22: val_accuracy improved from 0.64780 to 0.71698, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9675 - loss: 0.0976 - val_accuracy: 0.7170 - val_loss: 1.3827\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9688 - loss: 0.1144\n",
            "Epoch 23: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9686 - loss: 0.1151 - val_accuracy: 0.6730 - val_loss: 1.5550\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9704 - loss: 0.1011\n",
            "Epoch 24: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9702 - loss: 0.1016 - val_accuracy: 0.6918 - val_loss: 1.4093\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9688 - loss: 0.1008\n",
            "Epoch 25: val_accuracy improved from 0.71698 to 0.75472, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9687 - loss: 0.1013 - val_accuracy: 0.7547 - val_loss: 1.2474\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9713 - loss: 0.0818\n",
            "Epoch 26: val_accuracy did not improve from 0.75472\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9713 - loss: 0.0823 - val_accuracy: 0.6918 - val_loss: 1.2859\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9743 - loss: 0.1047\n",
            "Epoch 27: val_accuracy did not improve from 0.75472\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9744 - loss: 0.1043 - val_accuracy: 0.7170 - val_loss: 1.1498\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9767 - loss: 0.0707\n",
            "Epoch 28: val_accuracy improved from 0.75472 to 0.77987, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9766 - loss: 0.0711 - val_accuracy: 0.7799 - val_loss: 1.0812\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9805 - loss: 0.0768\n",
            "Epoch 29: val_accuracy did not improve from 0.77987\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9805 - loss: 0.0769 - val_accuracy: 0.7673 - val_loss: 0.8894\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9869 - loss: 0.0528\n",
            "Epoch 30: val_accuracy did not improve from 0.77987\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9865 - loss: 0.0535 - val_accuracy: 0.6101 - val_loss: 1.8915\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9793 - loss: 0.0704\n",
            "Epoch 31: val_accuracy did not improve from 0.77987\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9790 - loss: 0.0714 - val_accuracy: 0.7421 - val_loss: 0.9773\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9758 - loss: 0.0765\n",
            "Epoch 32: val_accuracy did not improve from 0.77987\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9755 - loss: 0.0772 - val_accuracy: 0.7610 - val_loss: 0.9750\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9848 - loss: 0.0721\n",
            "Epoch 33: val_accuracy improved from 0.77987 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9842 - loss: 0.0733 - val_accuracy: 0.8176 - val_loss: 0.6850\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9839 - loss: 0.0710\n",
            "Epoch 34: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9834 - loss: 0.0722 - val_accuracy: 0.7925 - val_loss: 0.8790\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9570 - loss: 0.1033\n",
            "Epoch 35: val_accuracy improved from 0.81761 to 0.89308, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9573 - loss: 0.1030 - val_accuracy: 0.8931 - val_loss: 0.4215\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9581 - loss: 0.0987\n",
            "Epoch 36: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9582 - loss: 0.0990 - val_accuracy: 0.8868 - val_loss: 0.2917\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9686 - loss: 0.0907\n",
            "Epoch 37: val_accuracy improved from 0.89308 to 0.91195, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9684 - loss: 0.0913 - val_accuracy: 0.9119 - val_loss: 0.3010\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9755 - loss: 0.0906\n",
            "Epoch 38: val_accuracy improved from 0.91195 to 0.93082, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9751 - loss: 0.0912 - val_accuracy: 0.9308 - val_loss: 0.2676\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9731 - loss: 0.0827\n",
            "Epoch 39: val_accuracy improved from 0.93082 to 0.95597, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9734 - loss: 0.0827 - val_accuracy: 0.9560 - val_loss: 0.2193\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9809 - loss: 0.0616\n",
            "Epoch 40: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9809 - loss: 0.0617 - val_accuracy: 0.8994 - val_loss: 0.3846\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9702 - loss: 0.0848\n",
            "Epoch 41: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9708 - loss: 0.0835 - val_accuracy: 0.9057 - val_loss: 0.4306\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9968 - loss: 0.0251\n",
            "Epoch 42: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9966 - loss: 0.0259 - val_accuracy: 0.8679 - val_loss: 0.4688\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9889 - loss: 0.0406\n",
            "Epoch 43: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9888 - loss: 0.0413 - val_accuracy: 0.8553 - val_loss: 0.5792\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9917 - loss: 0.0385\n",
            "Epoch 44: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9915 - loss: 0.0389 - val_accuracy: 0.8553 - val_loss: 0.5164\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9753 - loss: 0.0575\n",
            "Epoch 45: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9755 - loss: 0.0575 - val_accuracy: 0.7862 - val_loss: 0.7624\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9872 - loss: 0.0518\n",
            "Epoch 46: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9871 - loss: 0.0519 - val_accuracy: 0.8239 - val_loss: 0.8287\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9810 - loss: 0.0580\n",
            "Epoch 47: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9810 - loss: 0.0577 - val_accuracy: 0.8931 - val_loss: 0.4990\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9806 - loss: 0.0436\n",
            "Epoch 48: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9809 - loss: 0.0435 - val_accuracy: 0.9245 - val_loss: 0.2415\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9888 - loss: 0.0452\n",
            "Epoch 49: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9889 - loss: 0.0453 - val_accuracy: 0.9497 - val_loss: 0.2424\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9746 - loss: 0.0625\n",
            "Epoch 50: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9749 - loss: 0.0617 - val_accuracy: 0.9371 - val_loss: 0.2536\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9851 - loss: 0.0527\n",
            "Epoch 51: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9850 - loss: 0.0528 - val_accuracy: 0.9434 - val_loss: 0.2169\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9923 - loss: 0.0340\n",
            "Epoch 52: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9922 - loss: 0.0343 - val_accuracy: 0.9057 - val_loss: 0.3436\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9803 - loss: 0.0637\n",
            "Epoch 53: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9805 - loss: 0.0632 - val_accuracy: 0.8616 - val_loss: 0.5815\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9918 - loss: 0.0291\n",
            "Epoch 54: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9917 - loss: 0.0293 - val_accuracy: 0.8365 - val_loss: 0.7060\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9924 - loss: 0.0392\n",
            "Epoch 55: val_accuracy did not improve from 0.95597\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9922 - loss: 0.0396 - val_accuracy: 0.8805 - val_loss: 0.4868\n",
            "Train Accuracy: 0.9710, Test Accuracy: 0.9444\n",
            "Training with learning rate: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_17\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_17\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_34             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_53 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_51     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_51           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_54 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_52     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_52           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_55 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_35             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_53     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_85 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_53           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_69 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_86 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_68 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_70 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_17             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_87 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_71 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_88 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_89 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_34             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_51     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_51           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_52     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_52           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_35             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_53     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_53           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_17             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.0143 - loss: 4.8391\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 401ms/step - accuracy: 0.0150 - loss: 4.8269 - val_accuracy: 0.0000e+00 - val_loss: 55.1742\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0938 - loss: 3.8841\n",
            "Epoch 2: val_accuracy improved from 0.00000 to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0955 - loss: 3.8745 - val_accuracy: 0.0063 - val_loss: 35.0630\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2730 - loss: 2.8032\n",
            "Epoch 3: val_accuracy improved from 0.00629 to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.2734 - loss: 2.7996 - val_accuracy: 0.0126 - val_loss: 33.9470\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3377 - loss: 2.5044\n",
            "Epoch 4: val_accuracy improved from 0.01258 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.3383 - loss: 2.5020 - val_accuracy: 0.0377 - val_loss: 16.3752\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4424 - loss: 2.0642\n",
            "Epoch 5: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.4420 - loss: 2.0649 - val_accuracy: 0.0314 - val_loss: 12.0825\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5246 - loss: 1.7626\n",
            "Epoch 6: val_accuracy improved from 0.03774 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.5228 - loss: 1.7715 - val_accuracy: 0.0566 - val_loss: 9.1882\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5679 - loss: 1.5725\n",
            "Epoch 7: val_accuracy improved from 0.05660 to 0.12579, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5667 - loss: 1.5760 - val_accuracy: 0.1258 - val_loss: 7.2723\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6127 - loss: 1.5842\n",
            "Epoch 8: val_accuracy improved from 0.12579 to 0.25157, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.6102 - loss: 1.5945 - val_accuracy: 0.2516 - val_loss: 3.3457\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5530 - loss: 1.4893\n",
            "Epoch 9: val_accuracy improved from 0.25157 to 0.29560, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5535 - loss: 1.4886 - val_accuracy: 0.2956 - val_loss: 4.8039\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5966 - loss: 1.4303\n",
            "Epoch 10: val_accuracy did not improve from 0.29560\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.5970 - loss: 1.4333 - val_accuracy: 0.1887 - val_loss: 6.0749\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5868 - loss: 1.5505\n",
            "Epoch 11: val_accuracy improved from 0.29560 to 0.33962, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.5874 - loss: 1.5487 - val_accuracy: 0.3396 - val_loss: 3.3805\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6938 - loss: 1.0806\n",
            "Epoch 12: val_accuracy did not improve from 0.33962\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6930 - loss: 1.0835 - val_accuracy: 0.2893 - val_loss: 5.1636\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6786 - loss: 1.3007\n",
            "Epoch 13: val_accuracy improved from 0.33962 to 0.45912, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6777 - loss: 1.2984 - val_accuracy: 0.4591 - val_loss: 2.7387\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6837 - loss: 1.2486\n",
            "Epoch 14: val_accuracy did not improve from 0.45912\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6828 - loss: 1.2579 - val_accuracy: 0.2390 - val_loss: 4.3853\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6776 - loss: 1.1633\n",
            "Epoch 15: val_accuracy did not improve from 0.45912\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6764 - loss: 1.1695 - val_accuracy: 0.0755 - val_loss: 13.8996\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6790 - loss: 1.3727\n",
            "Epoch 16: val_accuracy did not improve from 0.45912\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.6784 - loss: 1.3718 - val_accuracy: 0.3774 - val_loss: 4.1463\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6917 - loss: 1.0810\n",
            "Epoch 17: val_accuracy did not improve from 0.45912\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6903 - loss: 1.0862 - val_accuracy: 0.3145 - val_loss: 4.6728\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7033 - loss: 1.0440\n",
            "Epoch 18: val_accuracy did not improve from 0.45912\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7024 - loss: 1.0470 - val_accuracy: 0.3899 - val_loss: 3.5116\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6903 - loss: 1.3027\n",
            "Epoch 19: val_accuracy did not improve from 0.45912\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.6904 - loss: 1.3010 - val_accuracy: 0.3711 - val_loss: 3.7096\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7222 - loss: 1.0474\n",
            "Epoch 20: val_accuracy improved from 0.45912 to 0.50943, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7204 - loss: 1.0557 - val_accuracy: 0.5094 - val_loss: 2.2608\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6979 - loss: 1.2577\n",
            "Epoch 21: val_accuracy improved from 0.50943 to 0.56604, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6974 - loss: 1.2572 - val_accuracy: 0.5660 - val_loss: 1.8957\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7325 - loss: 1.1680\n",
            "Epoch 22: val_accuracy did not improve from 0.56604\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7312 - loss: 1.1708 - val_accuracy: 0.4906 - val_loss: 2.4875\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7074 - loss: 1.2385\n",
            "Epoch 23: val_accuracy improved from 0.56604 to 0.64780, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.7065 - loss: 1.2405 - val_accuracy: 0.6478 - val_loss: 1.5068\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7229 - loss: 1.1123\n",
            "Epoch 24: val_accuracy improved from 0.64780 to 0.66667, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7220 - loss: 1.1139 - val_accuracy: 0.6667 - val_loss: 1.1640\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6959 - loss: 1.2236\n",
            "Epoch 25: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6962 - loss: 1.2240 - val_accuracy: 0.6352 - val_loss: 1.4383\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7331 - loss: 1.1454\n",
            "Epoch 26: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.7323 - loss: 1.1471 - val_accuracy: 0.5472 - val_loss: 2.1440\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7303 - loss: 1.3782\n",
            "Epoch 27: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7299 - loss: 1.3801 - val_accuracy: 0.0692 - val_loss: 77.6877\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7191 - loss: 1.2008\n",
            "Epoch 28: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.7187 - loss: 1.2033 - val_accuracy: 0.0755 - val_loss: 35.4986\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6710 - loss: 1.3119\n",
            "Epoch 29: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.6713 - loss: 1.3100 - val_accuracy: 0.1447 - val_loss: 19.6112\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7378 - loss: 1.1977\n",
            "Epoch 30: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7375 - loss: 1.1974 - val_accuracy: 0.1321 - val_loss: 14.1400\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7773 - loss: 0.8353\n",
            "Epoch 31: val_accuracy improved from 0.66667 to 0.67296, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7767 - loss: 0.8442 - val_accuracy: 0.6730 - val_loss: 1.2043\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7287 - loss: 0.9356\n",
            "Epoch 32: val_accuracy did not improve from 0.67296\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7289 - loss: 0.9385 - val_accuracy: 0.5031 - val_loss: 2.9067\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7612 - loss: 1.1710\n",
            "Epoch 33: val_accuracy improved from 0.67296 to 0.71698, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.7600 - loss: 1.1710 - val_accuracy: 0.7170 - val_loss: 1.0424\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7575 - loss: 1.0473\n",
            "Epoch 34: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7564 - loss: 1.0510 - val_accuracy: 0.6226 - val_loss: 1.9153\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7150 - loss: 1.3879\n",
            "Epoch 35: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7142 - loss: 1.3870 - val_accuracy: 0.6541 - val_loss: 1.7494\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7510 - loss: 1.0506\n",
            "Epoch 36: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7507 - loss: 1.0534 - val_accuracy: 0.2453 - val_loss: 9.0339\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7564 - loss: 1.0831\n",
            "Epoch 37: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7550 - loss: 1.0918 - val_accuracy: 0.3270 - val_loss: 8.9952\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7176 - loss: 1.2623\n",
            "Epoch 38: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7163 - loss: 1.2717 - val_accuracy: 0.6101 - val_loss: 1.7821\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7239 - loss: 1.1593\n",
            "Epoch 39: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7229 - loss: 1.1672 - val_accuracy: 0.0881 - val_loss: 20.6739\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7304 - loss: 1.3442\n",
            "Epoch 40: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.7283 - loss: 1.3528 - val_accuracy: 0.0189 - val_loss: 141.8064\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6903 - loss: 1.4897\n",
            "Epoch 41: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.6899 - loss: 1.4920 - val_accuracy: 0.0126 - val_loss: 68.3779\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6812 - loss: 1.3625\n",
            "Epoch 42: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.6823 - loss: 1.3632 - val_accuracy: 0.0189 - val_loss: 77.6025\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6847 - loss: 1.3058\n",
            "Epoch 43: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.6849 - loss: 1.3073 - val_accuracy: 0.0881 - val_loss: 15.0196\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7037 - loss: 1.4325\n",
            "Epoch 44: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.7030 - loss: 1.4350 - val_accuracy: 0.1258 - val_loss: 14.6006\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7199 - loss: 1.2774\n",
            "Epoch 45: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7196 - loss: 1.2832 - val_accuracy: 0.5723 - val_loss: 1.9321\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7620 - loss: 1.3417\n",
            "Epoch 46: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.7602 - loss: 1.3428 - val_accuracy: 0.1950 - val_loss: 8.0811\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7620 - loss: 0.9403\n",
            "Epoch 47: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7614 - loss: 0.9443 - val_accuracy: 0.4214 - val_loss: 2.7473\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7133 - loss: 1.1455\n",
            "Epoch 48: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7129 - loss: 1.1484 - val_accuracy: 0.6541 - val_loss: 1.4549\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7668 - loss: 1.1365\n",
            "Epoch 49: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.7642 - loss: 1.1509 - val_accuracy: 0.5786 - val_loss: 2.2542\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7296 - loss: 1.2260\n",
            "Epoch 50: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.7285 - loss: 1.2354 - val_accuracy: 0.2075 - val_loss: 8.7413\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7089 - loss: 1.8194\n",
            "Epoch 51: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7083 - loss: 1.8114 - val_accuracy: 0.1761 - val_loss: 17.8832\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7329 - loss: 1.5396\n",
            "Epoch 52: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7320 - loss: 1.5424 - val_accuracy: 0.1950 - val_loss: 13.5295\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6994 - loss: 1.5813\n",
            "Epoch 53: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.6995 - loss: 1.5832 - val_accuracy: 0.1006 - val_loss: 16.7295\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7376 - loss: 1.1946\n",
            "Epoch 54: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7358 - loss: 1.2052 - val_accuracy: 0.1384 - val_loss: 14.8903\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7072 - loss: 1.5260\n",
            "Epoch 55: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7073 - loss: 1.5243 - val_accuracy: 0.1635 - val_loss: 12.6288\n",
            "Train Accuracy: 0.2626, Test Accuracy: 0.2374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Different Batch Sizes"
      ],
      "metadata": {
        "id": "H2hO3RKVwuqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of batch sizes to experiment with\n",
        "batch_sizes = [16, 32, 64]\n",
        "\n",
        "# Loop through each batch size and train the model\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"Training with batch size: {batch_size}\")\n",
        "    training2(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs=55, batch_size=batch_size, learning_rate=0.0001,dropout=0.2, optimizer_name=\"adam\", l2_factor=0.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d0eXgjg8NvSY",
        "outputId": "545ee5b0-07e6-4b10-8ecf-ff0221aeb136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with batch size: 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_36             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_56 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_54     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_54           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_57 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_55     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_55           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_58 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_37             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_56     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_90 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_56           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_73 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_91 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_72 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_74 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_18             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_92 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_75 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_93 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_94 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_36             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_54     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_54           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_55     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_55           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_37             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_56     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_56           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_18             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.0091 - loss: 4.6187\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 385ms/step - accuracy: 0.0093 - loss: 4.6187 - val_accuracy: 0.0063 - val_loss: 4.5898\n",
            "Epoch 2/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.0329 - loss: 4.5127\n",
            "Epoch 2: val_accuracy improved from 0.00629 to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 57ms/step - accuracy: 0.0331 - loss: 4.5120 - val_accuracy: 0.0126 - val_loss: 4.5817\n",
            "Epoch 3/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.0494 - loss: 4.3965\n",
            "Epoch 3: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.0493 - loss: 4.3966 - val_accuracy: 0.0126 - val_loss: 4.5648\n",
            "Epoch 4/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.0473 - loss: 4.3428\n",
            "Epoch 4: val_accuracy improved from 0.01258 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.0486 - loss: 4.3404 - val_accuracy: 0.0189 - val_loss: 4.5473\n",
            "Epoch 5/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.0951 - loss: 4.2092\n",
            "Epoch 5: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.0954 - loss: 4.2087 - val_accuracy: 0.0252 - val_loss: 4.5210\n",
            "Epoch 6/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1272 - loss: 4.0662\n",
            "Epoch 6: val_accuracy improved from 0.02516 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.1269 - loss: 4.0661 - val_accuracy: 0.0377 - val_loss: 4.5019\n",
            "Epoch 7/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1620 - loss: 3.9315\n",
            "Epoch 7: val_accuracy improved from 0.03774 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.1626 - loss: 3.9291 - val_accuracy: 0.0440 - val_loss: 4.4508\n",
            "Epoch 8/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2169 - loss: 3.7726\n",
            "Epoch 8: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.2169 - loss: 3.7694 - val_accuracy: 0.0440 - val_loss: 4.3726\n",
            "Epoch 9/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2718 - loss: 3.5165\n",
            "Epoch 9: val_accuracy improved from 0.04403 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.2720 - loss: 3.5132 - val_accuracy: 0.0566 - val_loss: 4.3122\n",
            "Epoch 10/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3833 - loss: 3.2229\n",
            "Epoch 10: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.3820 - loss: 3.2220 - val_accuracy: 0.0503 - val_loss: 4.1911\n",
            "Epoch 11/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4386 - loss: 2.9069\n",
            "Epoch 11: val_accuracy improved from 0.05660 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.4372 - loss: 2.9067 - val_accuracy: 0.0881 - val_loss: 4.0239\n",
            "Epoch 12/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4339 - loss: 2.6887\n",
            "Epoch 12: val_accuracy improved from 0.08805 to 0.13836, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.4338 - loss: 2.6890 - val_accuracy: 0.1384 - val_loss: 3.7227\n",
            "Epoch 13/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5140 - loss: 2.4152\n",
            "Epoch 13: val_accuracy improved from 0.13836 to 0.23270, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.5141 - loss: 2.4140 - val_accuracy: 0.2327 - val_loss: 3.3652\n",
            "Epoch 14/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5555 - loss: 2.1811\n",
            "Epoch 14: val_accuracy improved from 0.23270 to 0.29560, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.5547 - loss: 2.1812 - val_accuracy: 0.2956 - val_loss: 3.0320\n",
            "Epoch 15/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5994 - loss: 1.9748\n",
            "Epoch 15: val_accuracy improved from 0.29560 to 0.42138, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 1.9719 - val_accuracy: 0.4214 - val_loss: 2.6119\n",
            "Epoch 16/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6427 - loss: 1.7371\n",
            "Epoch 16: val_accuracy did not improve from 0.42138\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.6420 - loss: 1.7368 - val_accuracy: 0.3585 - val_loss: 2.5321\n",
            "Epoch 17/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6692 - loss: 1.6103\n",
            "Epoch 17: val_accuracy improved from 0.42138 to 0.55346, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.6704 - loss: 1.6049 - val_accuracy: 0.5535 - val_loss: 2.0622\n",
            "Epoch 18/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6937 - loss: 1.4361\n",
            "Epoch 18: val_accuracy improved from 0.55346 to 0.68553, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.6942 - loss: 1.4339 - val_accuracy: 0.6855 - val_loss: 1.7104\n",
            "Epoch 19/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6895 - loss: 1.3752\n",
            "Epoch 19: val_accuracy improved from 0.68553 to 0.74843, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.6897 - loss: 1.3752 - val_accuracy: 0.7484 - val_loss: 1.4547\n",
            "Epoch 20/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7329 - loss: 1.1893\n",
            "Epoch 20: val_accuracy did not improve from 0.74843\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.7319 - loss: 1.1918 - val_accuracy: 0.7421 - val_loss: 1.3756\n",
            "Epoch 21/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7627 - loss: 1.1322\n",
            "Epoch 21: val_accuracy did not improve from 0.74843\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.7625 - loss: 1.1315 - val_accuracy: 0.6604 - val_loss: 1.4052\n",
            "Epoch 22/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7995 - loss: 0.9164\n",
            "Epoch 22: val_accuracy improved from 0.74843 to 0.83019, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.7982 - loss: 0.9188 - val_accuracy: 0.8302 - val_loss: 1.0794\n",
            "Epoch 23/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8363 - loss: 0.8932\n",
            "Epoch 23: val_accuracy did not improve from 0.83019\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.8348 - loss: 0.8944 - val_accuracy: 0.8050 - val_loss: 0.9576\n",
            "Epoch 24/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7717 - loss: 0.9368\n",
            "Epoch 24: val_accuracy improved from 0.83019 to 0.85535, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.7713 - loss: 0.9369 - val_accuracy: 0.8553 - val_loss: 0.8825\n",
            "Epoch 25/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8517 - loss: 0.7837\n",
            "Epoch 25: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.8506 - loss: 0.7857 - val_accuracy: 0.8491 - val_loss: 0.8076\n",
            "Epoch 26/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8622 - loss: 0.7410\n",
            "Epoch 26: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.8608 - loss: 0.7410 - val_accuracy: 0.8365 - val_loss: 0.8073\n",
            "Epoch 27/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8220 - loss: 0.7384\n",
            "Epoch 27: val_accuracy improved from 0.85535 to 0.86792, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.8221 - loss: 0.7377 - val_accuracy: 0.8679 - val_loss: 0.7049\n",
            "Epoch 28/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8289 - loss: 0.6729\n",
            "Epoch 28: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.8294 - loss: 0.6734 - val_accuracy: 0.8616 - val_loss: 0.7219\n",
            "Epoch 29/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8750 - loss: 0.5925\n",
            "Epoch 29: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.8747 - loss: 0.5930 - val_accuracy: 0.8679 - val_loss: 0.6431\n",
            "Epoch 30/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8600 - loss: 0.5493\n",
            "Epoch 30: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.8603 - loss: 0.5509 - val_accuracy: 0.8491 - val_loss: 0.5998\n",
            "Epoch 31/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8651 - loss: 0.5861\n",
            "Epoch 31: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.8651 - loss: 0.5869 - val_accuracy: 0.8679 - val_loss: 0.6657\n",
            "Epoch 32/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8492 - loss: 0.5280\n",
            "Epoch 32: val_accuracy improved from 0.86792 to 0.88679, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.8502 - loss: 0.5287 - val_accuracy: 0.8868 - val_loss: 0.5796\n",
            "Epoch 33/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8987 - loss: 0.4797\n",
            "Epoch 33: val_accuracy improved from 0.88679 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8984 - loss: 0.4802 - val_accuracy: 0.8994 - val_loss: 0.5350\n",
            "Epoch 34/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9066 - loss: 0.4452\n",
            "Epoch 34: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9054 - loss: 0.4473 - val_accuracy: 0.8868 - val_loss: 0.5445\n",
            "Epoch 35/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9120 - loss: 0.4315\n",
            "Epoch 35: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.9120 - loss: 0.4316 - val_accuracy: 0.8931 - val_loss: 0.4910\n",
            "Epoch 36/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8847 - loss: 0.4412\n",
            "Epoch 36: val_accuracy improved from 0.89937 to 0.91195, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.8845 - loss: 0.4416 - val_accuracy: 0.9119 - val_loss: 0.4451\n",
            "Epoch 37/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9316 - loss: 0.3735\n",
            "Epoch 37: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - accuracy: 0.9306 - loss: 0.3741 - val_accuracy: 0.8805 - val_loss: 0.4979\n",
            "Epoch 38/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9168 - loss: 0.3735\n",
            "Epoch 38: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9164 - loss: 0.3740 - val_accuracy: 0.8868 - val_loss: 0.4471\n",
            "Epoch 39/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8968 - loss: 0.4069\n",
            "Epoch 39: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.8972 - loss: 0.4066 - val_accuracy: 0.9119 - val_loss: 0.4221\n",
            "Epoch 40/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9238 - loss: 0.3615\n",
            "Epoch 40: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9238 - loss: 0.3609 - val_accuracy: 0.9057 - val_loss: 0.3875\n",
            "Epoch 41/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9205 - loss: 0.3435\n",
            "Epoch 41: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9207 - loss: 0.3432 - val_accuracy: 0.9057 - val_loss: 0.4070\n",
            "Epoch 42/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9504 - loss: 0.3049\n",
            "Epoch 42: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.9501 - loss: 0.3048 - val_accuracy: 0.8491 - val_loss: 0.5129\n",
            "Epoch 43/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8979 - loss: 0.3473\n",
            "Epoch 43: val_accuracy improved from 0.91195 to 0.92453, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8989 - loss: 0.3463 - val_accuracy: 0.9245 - val_loss: 0.3825\n",
            "Epoch 44/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9311 - loss: 0.3289\n",
            "Epoch 44: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9313 - loss: 0.3285 - val_accuracy: 0.9119 - val_loss: 0.3626\n",
            "Epoch 45/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9373 - loss: 0.3180\n",
            "Epoch 45: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - accuracy: 0.9371 - loss: 0.3180 - val_accuracy: 0.8742 - val_loss: 0.4364\n",
            "Epoch 46/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9331 - loss: 0.2783\n",
            "Epoch 46: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.9331 - loss: 0.2788 - val_accuracy: 0.8868 - val_loss: 0.4247\n",
            "Epoch 47/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9407 - loss: 0.2533\n",
            "Epoch 47: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.9403 - loss: 0.2542 - val_accuracy: 0.9057 - val_loss: 0.3647\n",
            "Epoch 48/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9391 - loss: 0.2537\n",
            "Epoch 48: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9391 - loss: 0.2540 - val_accuracy: 0.9245 - val_loss: 0.3268\n",
            "Epoch 49/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9479 - loss: 0.2798\n",
            "Epoch 49: val_accuracy improved from 0.92453 to 0.93711, saving model to best_model.keras\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9479 - loss: 0.2788 - val_accuracy: 0.9371 - val_loss: 0.3338\n",
            "Epoch 50/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9497 - loss: 0.2163\n",
            "Epoch 50: val_accuracy did not improve from 0.93711\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9495 - loss: 0.2179 - val_accuracy: 0.8931 - val_loss: 0.3365\n",
            "Epoch 51/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9637 - loss: 0.2311\n",
            "Epoch 51: val_accuracy did not improve from 0.93711\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 0.9638 - loss: 0.2306 - val_accuracy: 0.9057 - val_loss: 0.3416\n",
            "Epoch 52/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9468 - loss: 0.2390\n",
            "Epoch 52: val_accuracy did not improve from 0.93711\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9471 - loss: 0.2387 - val_accuracy: 0.9119 - val_loss: 0.2981\n",
            "Epoch 53/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9693 - loss: 0.1885\n",
            "Epoch 53: val_accuracy did not improve from 0.93711\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.9686 - loss: 0.1902 - val_accuracy: 0.9057 - val_loss: 0.3248\n",
            "Epoch 54/55\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9684 - loss: 0.1793\n",
            "Epoch 54: val_accuracy did not improve from 0.93711\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9683 - loss: 0.1792 - val_accuracy: 0.8931 - val_loss: 0.3840\n",
            "Epoch 55/55\n",
            "\u001b[1m39/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9400 - loss: 0.2220\n",
            "Epoch 55: val_accuracy did not improve from 0.93711\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9406 - loss: 0.2221 - val_accuracy: 0.8491 - val_loss: 0.4480\n",
            "Train Accuracy: 0.9621, Test Accuracy: 0.8889\n",
            "Training with batch size: 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_38             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_59 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_57     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_57           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_58     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_58           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_61 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_39             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_59     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_95 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_59           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_77 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_96 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_76 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_78 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_19             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_97 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_79 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_98 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_99 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_38             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_57     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_57           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_58     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_58           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_39             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_59     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_59           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_19             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.0140 - loss: 4.6388\n",
            "Epoch 1: val_accuracy improved from -inf to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 471ms/step - accuracy: 0.0143 - loss: 4.6385 - val_accuracy: 0.0252 - val_loss: 4.5810\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0354 - loss: 4.5034\n",
            "Epoch 2: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.0350 - loss: 4.5048 - val_accuracy: 0.0189 - val_loss: 4.5718\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0374 - loss: 4.4655\n",
            "Epoch 3: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.0378 - loss: 4.4652 - val_accuracy: 0.0189 - val_loss: 4.5636\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0786 - loss: 4.3865\n",
            "Epoch 4: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.0779 - loss: 4.3865 - val_accuracy: 0.0314 - val_loss: 4.5541\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0798 - loss: 4.3156\n",
            "Epoch 5: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0796 - loss: 4.3156 - val_accuracy: 0.0314 - val_loss: 4.5448\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0939 - loss: 4.2491\n",
            "Epoch 6: val_accuracy improved from 0.03145 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.0947 - loss: 4.2482 - val_accuracy: 0.0440 - val_loss: 4.5354\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1219 - loss: 4.1681\n",
            "Epoch 7: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.1221 - loss: 4.1672 - val_accuracy: 0.0440 - val_loss: 4.5214\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1317 - loss: 4.0761\n",
            "Epoch 8: val_accuracy improved from 0.04403 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.1326 - loss: 4.0740 - val_accuracy: 0.0566 - val_loss: 4.5048\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1633 - loss: 3.9760\n",
            "Epoch 9: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.1643 - loss: 3.9737 - val_accuracy: 0.0566 - val_loss: 4.4992\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1949 - loss: 3.8508\n",
            "Epoch 10: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.1960 - loss: 3.8472 - val_accuracy: 0.0566 - val_loss: 4.4787\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2811 - loss: 3.6035\n",
            "Epoch 11: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.2804 - loss: 3.6041 - val_accuracy: 0.0377 - val_loss: 4.4609\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3608 - loss: 3.4419\n",
            "Epoch 12: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.3593 - loss: 3.4425 - val_accuracy: 0.0440 - val_loss: 4.4514\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3353 - loss: 3.3469\n",
            "Epoch 13: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.3355 - loss: 3.3457 - val_accuracy: 0.0377 - val_loss: 4.3874\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3396 - loss: 3.1868\n",
            "Epoch 14: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.3401 - loss: 3.1838 - val_accuracy: 0.0440 - val_loss: 4.3624\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4249 - loss: 2.9600\n",
            "Epoch 15: val_accuracy improved from 0.05660 to 0.06918, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.4253 - loss: 2.9578 - val_accuracy: 0.0692 - val_loss: 4.2584\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.4369 - loss: 2.8489\n",
            "Epoch 16: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.4377 - loss: 2.8443 - val_accuracy: 0.0692 - val_loss: 4.2954\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4672 - loss: 2.6218\n",
            "Epoch 17: val_accuracy improved from 0.06918 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.4678 - loss: 2.6183 - val_accuracy: 0.0881 - val_loss: 4.1194\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5446 - loss: 2.3906\n",
            "Epoch 18: val_accuracy improved from 0.08805 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.5442 - loss: 2.3893 - val_accuracy: 0.1069 - val_loss: 3.9967\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5936 - loss: 2.0961\n",
            "Epoch 19: val_accuracy did not improve from 0.10692\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5921 - loss: 2.0996 - val_accuracy: 0.1069 - val_loss: 3.9452\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6196 - loss: 2.0205\n",
            "Epoch 20: val_accuracy improved from 0.10692 to 0.13208, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.6186 - loss: 2.0213 - val_accuracy: 0.1321 - val_loss: 3.7591\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6495 - loss: 1.8999\n",
            "Epoch 21: val_accuracy improved from 0.13208 to 0.17610, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.6497 - loss: 1.8985 - val_accuracy: 0.1761 - val_loss: 3.5644\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6715 - loss: 1.7120\n",
            "Epoch 22: val_accuracy improved from 0.17610 to 0.25157, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.6711 - loss: 1.7129 - val_accuracy: 0.2516 - val_loss: 3.1306\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6634 - loss: 1.6649\n",
            "Epoch 23: val_accuracy improved from 0.25157 to 0.27673, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6640 - loss: 1.6637 - val_accuracy: 0.2767 - val_loss: 3.0085\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7234 - loss: 1.4275\n",
            "Epoch 24: val_accuracy did not improve from 0.27673\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7226 - loss: 1.4290 - val_accuracy: 0.2642 - val_loss: 2.9957\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7478 - loss: 1.3315\n",
            "Epoch 25: val_accuracy improved from 0.27673 to 0.34591, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7475 - loss: 1.3324 - val_accuracy: 0.3459 - val_loss: 2.4619\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8016 - loss: 1.2375\n",
            "Epoch 26: val_accuracy improved from 0.34591 to 0.49686, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7995 - loss: 1.2410 - val_accuracy: 0.4969 - val_loss: 2.2438\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7348 - loss: 1.3030\n",
            "Epoch 27: val_accuracy improved from 0.49686 to 0.54088, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.7346 - loss: 1.3014 - val_accuracy: 0.5409 - val_loss: 2.0115\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7459 - loss: 1.1684\n",
            "Epoch 28: val_accuracy improved from 0.54088 to 0.55975, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.7456 - loss: 1.1688 - val_accuracy: 0.5597 - val_loss: 1.8203\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8103 - loss: 1.0128\n",
            "Epoch 29: val_accuracy improved from 0.55975 to 0.64780, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8094 - loss: 1.0161 - val_accuracy: 0.6478 - val_loss: 1.6338\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7853 - loss: 1.0000\n",
            "Epoch 30: val_accuracy improved from 0.64780 to 0.71069, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.7863 - loss: 0.9993 - val_accuracy: 0.7107 - val_loss: 1.4351\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8496 - loss: 0.8639\n",
            "Epoch 31: val_accuracy did not improve from 0.71069\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8485 - loss: 0.8655 - val_accuracy: 0.7044 - val_loss: 1.3261\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8412 - loss: 0.8676\n",
            "Epoch 32: val_accuracy improved from 0.71069 to 0.74214, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.8404 - loss: 0.8681 - val_accuracy: 0.7421 - val_loss: 1.2371\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8163 - loss: 0.8801\n",
            "Epoch 33: val_accuracy improved from 0.74214 to 0.80503, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8162 - loss: 0.8797 - val_accuracy: 0.8050 - val_loss: 1.1061\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8162 - loss: 0.8487\n",
            "Epoch 34: val_accuracy improved from 0.80503 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.8171 - loss: 0.8469 - val_accuracy: 0.8176 - val_loss: 1.0199\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8558 - loss: 0.7220\n",
            "Epoch 35: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8550 - loss: 0.7235 - val_accuracy: 0.7987 - val_loss: 0.9632\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8700 - loss: 0.6484\n",
            "Epoch 36: val_accuracy improved from 0.81761 to 0.83019, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8691 - loss: 0.6515 - val_accuracy: 0.8302 - val_loss: 0.8781\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8674 - loss: 0.6036\n",
            "Epoch 37: val_accuracy improved from 0.83019 to 0.86164, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8669 - loss: 0.6060 - val_accuracy: 0.8616 - val_loss: 0.7649\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8379 - loss: 0.6926\n",
            "Epoch 38: val_accuracy improved from 0.86164 to 0.87421, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8386 - loss: 0.6913 - val_accuracy: 0.8742 - val_loss: 0.7643\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8906 - loss: 0.6048\n",
            "Epoch 39: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8901 - loss: 0.6056 - val_accuracy: 0.8742 - val_loss: 0.6554\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8721 - loss: 0.5895\n",
            "Epoch 40: val_accuracy improved from 0.87421 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.8725 - loss: 0.5892 - val_accuracy: 0.8994 - val_loss: 0.6553\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8770 - loss: 0.6046\n",
            "Epoch 41: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8764 - loss: 0.6053 - val_accuracy: 0.8931 - val_loss: 0.6427\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9048 - loss: 0.5483\n",
            "Epoch 42: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9051 - loss: 0.5468 - val_accuracy: 0.8994 - val_loss: 0.5756\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9160 - loss: 0.5022\n",
            "Epoch 43: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9153 - loss: 0.5030 - val_accuracy: 0.8805 - val_loss: 0.5824\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9116 - loss: 0.4728\n",
            "Epoch 44: val_accuracy improved from 0.89937 to 0.90566, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9104 - loss: 0.4747 - val_accuracy: 0.9057 - val_loss: 0.5538\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9384 - loss: 0.4508\n",
            "Epoch 45: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9375 - loss: 0.4507 - val_accuracy: 0.8491 - val_loss: 0.6049\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8979 - loss: 0.4271\n",
            "Epoch 46: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.8973 - loss: 0.4279 - val_accuracy: 0.8931 - val_loss: 0.5301\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9534 - loss: 0.3510\n",
            "Epoch 47: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9524 - loss: 0.3532 - val_accuracy: 0.9057 - val_loss: 0.4889\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9029 - loss: 0.4836\n",
            "Epoch 48: val_accuracy improved from 0.90566 to 0.92453, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9029 - loss: 0.4829 - val_accuracy: 0.9245 - val_loss: 0.5050\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9116 - loss: 0.4213\n",
            "Epoch 49: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9119 - loss: 0.4209 - val_accuracy: 0.8868 - val_loss: 0.4876\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9165 - loss: 0.3912\n",
            "Epoch 50: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9165 - loss: 0.3904 - val_accuracy: 0.8679 - val_loss: 0.4965\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9165 - loss: 0.4172\n",
            "Epoch 51: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9163 - loss: 0.4164 - val_accuracy: 0.8742 - val_loss: 0.5085\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9440 - loss: 0.3594\n",
            "Epoch 52: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9437 - loss: 0.3592 - val_accuracy: 0.9182 - val_loss: 0.4357\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9156 - loss: 0.3534\n",
            "Epoch 53: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9161 - loss: 0.3532 - val_accuracy: 0.8616 - val_loss: 0.5345\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9262 - loss: 0.3214\n",
            "Epoch 54: val_accuracy improved from 0.92453 to 0.94969, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9261 - loss: 0.3221 - val_accuracy: 0.9497 - val_loss: 0.3611\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9350 - loss: 0.3244\n",
            "Epoch 55: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9350 - loss: 0.3240 - val_accuracy: 0.9245 - val_loss: 0.3781\n",
            "Train Accuracy: 0.9848, Test Accuracy: 0.9293\n",
            "Training with batch size: 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_20\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_20\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_40             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_62 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_60     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_60           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_63 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_61     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_61           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_64 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_41             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_62     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_100 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_62           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_81 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_101 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_80 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_82 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_20             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_102 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_83 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_103 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_104 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_40             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_60     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_60           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_61     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_61           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_41             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_62     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_62           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_20             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0026 - loss: 4.6524   \n",
            "Epoch 1: val_accuracy improved from -inf to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - accuracy: 0.0026 - loss: 4.6515 - val_accuracy: 0.0189 - val_loss: 4.5898\n",
            "Epoch 2/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.0083 - loss: 4.5748\n",
            "Epoch 2: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 206ms/step - accuracy: 0.0090 - loss: 4.5741 - val_accuracy: 0.0126 - val_loss: 4.5810\n",
            "Epoch 3/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.0212 - loss: 4.5265\n",
            "Epoch 3: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - accuracy: 0.0224 - loss: 4.5251 - val_accuracy: 0.0189 - val_loss: 4.5717\n",
            "Epoch 4/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.0202 - loss: 4.4812\n",
            "Epoch 4: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.0208 - loss: 4.4796 - val_accuracy: 0.0126 - val_loss: 4.5633\n",
            "Epoch 5/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.0325 - loss: 4.4568\n",
            "Epoch 5: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 205ms/step - accuracy: 0.0335 - loss: 4.4527 - val_accuracy: 0.0063 - val_loss: 4.5538\n",
            "Epoch 6/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.0525 - loss: 4.3346\n",
            "Epoch 6: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.0522 - loss: 4.3366 - val_accuracy: 0.0063 - val_loss: 4.5438\n",
            "Epoch 7/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.0508 - loss: 4.3166\n",
            "Epoch 7: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 337ms/step - accuracy: 0.0511 - loss: 4.3159 - val_accuracy: 0.0126 - val_loss: 4.5344\n",
            "Epoch 8/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.0652 - loss: 4.2625\n",
            "Epoch 8: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 209ms/step - accuracy: 0.0654 - loss: 4.2621 - val_accuracy: 0.0189 - val_loss: 4.5253\n",
            "Epoch 9/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.1196 - loss: 4.1859\n",
            "Epoch 9: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step - accuracy: 0.1198 - loss: 4.1859 - val_accuracy: 0.0189 - val_loss: 4.5163\n",
            "Epoch 10/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.1121 - loss: 4.1474\n",
            "Epoch 10: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.1124 - loss: 4.1465 - val_accuracy: 0.0189 - val_loss: 4.5058\n",
            "Epoch 11/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.1351 - loss: 4.1045\n",
            "Epoch 11: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - accuracy: 0.1344 - loss: 4.1020 - val_accuracy: 0.0189 - val_loss: 4.4929\n",
            "Epoch 12/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.1670 - loss: 3.9707\n",
            "Epoch 12: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - accuracy: 0.1669 - loss: 3.9716 - val_accuracy: 0.0189 - val_loss: 4.4785\n",
            "Epoch 13/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.1679 - loss: 3.9444\n",
            "Epoch 13: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 201ms/step - accuracy: 0.1674 - loss: 3.9430 - val_accuracy: 0.0189 - val_loss: 4.4652\n",
            "Epoch 14/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.1664 - loss: 3.8666\n",
            "Epoch 14: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 199ms/step - accuracy: 0.1682 - loss: 3.8651 - val_accuracy: 0.0126 - val_loss: 4.4469\n",
            "Epoch 15/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2216 - loss: 3.7244\n",
            "Epoch 15: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 205ms/step - accuracy: 0.2217 - loss: 3.7241 - val_accuracy: 0.0126 - val_loss: 4.4311\n",
            "Epoch 16/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.2368 - loss: 3.6863\n",
            "Epoch 16: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - accuracy: 0.2368 - loss: 3.6829 - val_accuracy: 0.0252 - val_loss: 4.4107\n",
            "Epoch 17/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.2765 - loss: 3.5308\n",
            "Epoch 17: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - accuracy: 0.2758 - loss: 3.5294 - val_accuracy: 0.0252 - val_loss: 4.3824\n",
            "Epoch 18/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.2823 - loss: 3.4414\n",
            "Epoch 18: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.2835 - loss: 3.4403 - val_accuracy: 0.0252 - val_loss: 4.3542\n",
            "Epoch 19/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2936 - loss: 3.3482\n",
            "Epoch 19: val_accuracy improved from 0.02516 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.2960 - loss: 3.3454 - val_accuracy: 0.0377 - val_loss: 4.3261\n",
            "Epoch 20/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3602 - loss: 3.2531\n",
            "Epoch 20: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - accuracy: 0.3609 - loss: 3.2462 - val_accuracy: 0.0377 - val_loss: 4.2784\n",
            "Epoch 21/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.3951 - loss: 3.0773\n",
            "Epoch 21: val_accuracy improved from 0.03774 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - accuracy: 0.3951 - loss: 3.0758 - val_accuracy: 0.0566 - val_loss: 4.2312\n",
            "Epoch 22/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.4310 - loss: 2.8871\n",
            "Epoch 22: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step - accuracy: 0.4310 - loss: 2.8889 - val_accuracy: 0.0566 - val_loss: 4.1853\n",
            "Epoch 23/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.4305 - loss: 2.8154\n",
            "Epoch 23: val_accuracy improved from 0.05660 to 0.06918, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - accuracy: 0.4328 - loss: 2.8111 - val_accuracy: 0.0692 - val_loss: 4.1463\n",
            "Epoch 24/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4939 - loss: 2.6685\n",
            "Epoch 24: val_accuracy improved from 0.06918 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 241ms/step - accuracy: 0.4929 - loss: 2.6685 - val_accuracy: 0.0755 - val_loss: 4.0938\n",
            "Epoch 25/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.5166 - loss: 2.5788\n",
            "Epoch 25: val_accuracy improved from 0.07547 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - accuracy: 0.5161 - loss: 2.5749 - val_accuracy: 0.0881 - val_loss: 4.0353\n",
            "Epoch 26/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.5708 - loss: 2.3774\n",
            "Epoch 26: val_accuracy improved from 0.08805 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.5698 - loss: 2.3792 - val_accuracy: 0.1069 - val_loss: 3.9929\n",
            "Epoch 27/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.5707 - loss: 2.2774\n",
            "Epoch 27: val_accuracy improved from 0.10692 to 0.11950, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.5717 - loss: 2.2736 - val_accuracy: 0.1195 - val_loss: 3.9392\n",
            "Epoch 28/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5485 - loss: 2.2353\n",
            "Epoch 28: val_accuracy improved from 0.11950 to 0.14465, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - accuracy: 0.5502 - loss: 2.2308 - val_accuracy: 0.1447 - val_loss: 3.8766\n",
            "Epoch 29/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.6162 - loss: 2.0025\n",
            "Epoch 29: val_accuracy improved from 0.14465 to 0.15094, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.6167 - loss: 2.0018 - val_accuracy: 0.1509 - val_loss: 3.7873\n",
            "Epoch 30/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.6140 - loss: 1.9613\n",
            "Epoch 30: val_accuracy improved from 0.15094 to 0.20755, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 260ms/step - accuracy: 0.6155 - loss: 1.9599 - val_accuracy: 0.2075 - val_loss: 3.6968\n",
            "Epoch 31/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6554 - loss: 1.8238\n",
            "Epoch 31: val_accuracy did not improve from 0.20755\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - accuracy: 0.6547 - loss: 1.8268 - val_accuracy: 0.1950 - val_loss: 3.6306\n",
            "Epoch 32/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.6523 - loss: 1.7773\n",
            "Epoch 32: val_accuracy did not improve from 0.20755\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - accuracy: 0.6535 - loss: 1.7741 - val_accuracy: 0.2075 - val_loss: 3.5098\n",
            "Epoch 33/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7002 - loss: 1.6735\n",
            "Epoch 33: val_accuracy improved from 0.20755 to 0.23270, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.7001 - loss: 1.6690 - val_accuracy: 0.2327 - val_loss: 3.4058\n",
            "Epoch 34/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.6942 - loss: 1.5780\n",
            "Epoch 34: val_accuracy improved from 0.23270 to 0.24528, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 211ms/step - accuracy: 0.6956 - loss: 1.5756 - val_accuracy: 0.2453 - val_loss: 3.2961\n",
            "Epoch 35/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.7488 - loss: 1.3740\n",
            "Epoch 35: val_accuracy improved from 0.24528 to 0.27673, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.7472 - loss: 1.3775 - val_accuracy: 0.2767 - val_loss: 3.1133\n",
            "Epoch 36/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.7309 - loss: 1.3631\n",
            "Epoch 36: val_accuracy improved from 0.27673 to 0.29560, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 234ms/step - accuracy: 0.7309 - loss: 1.3662 - val_accuracy: 0.2956 - val_loss: 3.0351\n",
            "Epoch 37/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.7898 - loss: 1.1860\n",
            "Epoch 37: val_accuracy improved from 0.29560 to 0.30818, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - accuracy: 0.7882 - loss: 1.1920 - val_accuracy: 0.3082 - val_loss: 2.9152\n",
            "Epoch 38/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7839 - loss: 1.2364\n",
            "Epoch 38: val_accuracy improved from 0.30818 to 0.33962, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - accuracy: 0.7825 - loss: 1.2400 - val_accuracy: 0.3396 - val_loss: 2.7854\n",
            "Epoch 39/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7954 - loss: 1.1875\n",
            "Epoch 39: val_accuracy improved from 0.33962 to 0.36478, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 218ms/step - accuracy: 0.7931 - loss: 1.1889 - val_accuracy: 0.3648 - val_loss: 2.7233\n",
            "Epoch 40/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7791 - loss: 1.1255\n",
            "Epoch 40: val_accuracy improved from 0.36478 to 0.38365, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.7793 - loss: 1.1273 - val_accuracy: 0.3836 - val_loss: 2.6744\n",
            "Epoch 41/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7599 - loss: 1.1116\n",
            "Epoch 41: val_accuracy did not improve from 0.38365\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - accuracy: 0.7613 - loss: 1.1093 - val_accuracy: 0.3836 - val_loss: 2.5316\n",
            "Epoch 42/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.7552 - loss: 1.0646\n",
            "Epoch 42: val_accuracy improved from 0.38365 to 0.42767, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - accuracy: 0.7560 - loss: 1.0650 - val_accuracy: 0.4277 - val_loss: 2.3536\n",
            "Epoch 43/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7943 - loss: 1.0046\n",
            "Epoch 43: val_accuracy did not improve from 0.42767\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - accuracy: 0.7949 - loss: 1.0052 - val_accuracy: 0.4214 - val_loss: 2.3330\n",
            "Epoch 44/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.7999 - loss: 0.9648\n",
            "Epoch 44: val_accuracy did not improve from 0.42767\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step - accuracy: 0.7991 - loss: 0.9663 - val_accuracy: 0.3836 - val_loss: 2.4075\n",
            "Epoch 45/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8360 - loss: 0.9133\n",
            "Epoch 45: val_accuracy improved from 0.42767 to 0.43396, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 211ms/step - accuracy: 0.8361 - loss: 0.9135 - val_accuracy: 0.4340 - val_loss: 2.1567\n",
            "Epoch 46/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8121 - loss: 0.9113\n",
            "Epoch 46: val_accuracy did not improve from 0.43396\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step - accuracy: 0.8134 - loss: 0.9085 - val_accuracy: 0.4214 - val_loss: 2.1192\n",
            "Epoch 47/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8287 - loss: 0.7986\n",
            "Epoch 47: val_accuracy improved from 0.43396 to 0.48428, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 213ms/step - accuracy: 0.8282 - loss: 0.8003 - val_accuracy: 0.4843 - val_loss: 2.0005\n",
            "Epoch 48/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8254 - loss: 0.7807\n",
            "Epoch 48: val_accuracy did not improve from 0.48428\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 221ms/step - accuracy: 0.8242 - loss: 0.7849 - val_accuracy: 0.4780 - val_loss: 1.9459\n",
            "Epoch 49/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8525 - loss: 0.7715\n",
            "Epoch 49: val_accuracy improved from 0.48428 to 0.52830, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 217ms/step - accuracy: 0.8529 - loss: 0.7712 - val_accuracy: 0.5283 - val_loss: 1.8003\n",
            "Epoch 50/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.8691 - loss: 0.6820\n",
            "Epoch 50: val_accuracy did not improve from 0.52830\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 199ms/step - accuracy: 0.8705 - loss: 0.6820 - val_accuracy: 0.4717 - val_loss: 1.9278\n",
            "Epoch 51/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8612 - loss: 0.6789\n",
            "Epoch 51: val_accuracy did not improve from 0.52830\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 203ms/step - accuracy: 0.8606 - loss: 0.6799 - val_accuracy: 0.5283 - val_loss: 1.6997\n",
            "Epoch 52/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.8603 - loss: 0.6602\n",
            "Epoch 52: val_accuracy improved from 0.52830 to 0.57233, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - accuracy: 0.8590 - loss: 0.6635 - val_accuracy: 0.5723 - val_loss: 1.5571\n",
            "Epoch 53/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8630 - loss: 0.6480\n",
            "Epoch 53: val_accuracy improved from 0.57233 to 0.66667, saving model to best_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - accuracy: 0.8635 - loss: 0.6500 - val_accuracy: 0.6667 - val_loss: 1.4122\n",
            "Epoch 54/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8492 - loss: 0.6659\n",
            "Epoch 54: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - accuracy: 0.8497 - loss: 0.6647 - val_accuracy: 0.6415 - val_loss: 1.3826\n",
            "Epoch 55/55\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8914 - loss: 0.6253\n",
            "Epoch 55: val_accuracy did not improve from 0.66667\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - accuracy: 0.8908 - loss: 0.6259 - val_accuracy: 0.6038 - val_loss: 1.5003\n",
            "Train Accuracy: 0.7487, Test Accuracy: 0.6313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of epoch counts to experiment with\n",
        "epochs_list = [30, 50, 70]\n",
        "\n",
        "# Loop through each number of epochs and train the model\n",
        "for epochs in epochs_list:\n",
        "    print(f\"Training with epochs: {epochs}\")\n",
        "    training2(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs=epochs, batch_size=32, learning_rate=0.0001,dropout=0.2, optimizer_name=\"adam\", l2_factor=0.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ooisvHfUNxUy",
        "outputId": "0e742e94-3f3f-4a4d-ebb7-3e27e24e7655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with epochs: 30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_42             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_65 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_63     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_63           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_66 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_64     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_64           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_67 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_43             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_65     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_105 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_65           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_85 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_106 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_84 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_86 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_21             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_107 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_87 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_108 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_109 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_42             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_63     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_63           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_64     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_64           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_43             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_65     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_65           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_21             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.0170 - loss: 4.6541\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 379ms/step - accuracy: 0.0171 - loss: 4.6532 - val_accuracy: 0.0063 - val_loss: 4.5792\n",
            "Epoch 2/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0393 - loss: 4.5067\n",
            "Epoch 2: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.0390 - loss: 4.5072 - val_accuracy: 0.0063 - val_loss: 4.5637\n",
            "Epoch 3/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0352 - loss: 4.4501\n",
            "Epoch 3: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.0348 - loss: 4.4503 - val_accuracy: 0.0063 - val_loss: 4.5529\n",
            "Epoch 4/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0539 - loss: 4.3693\n",
            "Epoch 4: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.0538 - loss: 4.3695 - val_accuracy: 0.0063 - val_loss: 4.5432\n",
            "Epoch 5/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0867 - loss: 4.2971\n",
            "Epoch 5: val_accuracy improved from 0.00629 to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.0862 - loss: 4.2978 - val_accuracy: 0.0126 - val_loss: 4.5292\n",
            "Epoch 6/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1210 - loss: 4.1883\n",
            "Epoch 6: val_accuracy improved from 0.01258 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.1203 - loss: 4.1895 - val_accuracy: 0.0189 - val_loss: 4.5159\n",
            "Epoch 7/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1069 - loss: 4.1452\n",
            "Epoch 7: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.1069 - loss: 4.1448 - val_accuracy: 0.0189 - val_loss: 4.5026\n",
            "Epoch 8/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1504 - loss: 4.0042\n",
            "Epoch 8: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.1499 - loss: 4.0045 - val_accuracy: 0.0126 - val_loss: 4.4814\n",
            "Epoch 9/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1650 - loss: 3.9202\n",
            "Epoch 9: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.1656 - loss: 3.9188 - val_accuracy: 0.0252 - val_loss: 4.4592\n",
            "Epoch 10/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2140 - loss: 3.7792\n",
            "Epoch 10: val_accuracy improved from 0.02516 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.2139 - loss: 3.7779 - val_accuracy: 0.0377 - val_loss: 4.4311\n",
            "Epoch 11/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2423 - loss: 3.6315\n",
            "Epoch 11: val_accuracy improved from 0.03774 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.2426 - loss: 3.6290 - val_accuracy: 0.0503 - val_loss: 4.3946\n",
            "Epoch 12/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3384 - loss: 3.3847\n",
            "Epoch 12: val_accuracy improved from 0.05031 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.3364 - loss: 3.3863 - val_accuracy: 0.0566 - val_loss: 4.3340\n",
            "Epoch 13/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3122 - loss: 3.3258\n",
            "Epoch 13: val_accuracy improved from 0.05660 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - accuracy: 0.3129 - loss: 3.3225 - val_accuracy: 0.0629 - val_loss: 4.2800\n",
            "Epoch 14/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3267 - loss: 3.1052\n",
            "Epoch 14: val_accuracy improved from 0.06289 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.3279 - loss: 3.1031 - val_accuracy: 0.1069 - val_loss: 4.1920\n",
            "Epoch 15/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4475 - loss: 2.8558\n",
            "Epoch 15: val_accuracy improved from 0.10692 to 0.13836, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.4470 - loss: 2.8555 - val_accuracy: 0.1384 - val_loss: 4.0406\n",
            "Epoch 16/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4578 - loss: 2.6685\n",
            "Epoch 16: val_accuracy did not improve from 0.13836\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.4567 - loss: 2.6691 - val_accuracy: 0.1195 - val_loss: 3.9780\n",
            "Epoch 17/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5099 - loss: 2.4549\n",
            "Epoch 17: val_accuracy improved from 0.13836 to 0.16981, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.5085 - loss: 2.4575 - val_accuracy: 0.1698 - val_loss: 3.8487\n",
            "Epoch 18/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5520 - loss: 2.3978\n",
            "Epoch 18: val_accuracy did not improve from 0.16981\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5527 - loss: 2.3943 - val_accuracy: 0.1447 - val_loss: 3.6783\n",
            "Epoch 19/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5465 - loss: 2.2228\n",
            "Epoch 19: val_accuracy improved from 0.16981 to 0.20126, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5467 - loss: 2.2215 - val_accuracy: 0.2013 - val_loss: 3.4856\n",
            "Epoch 20/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5967 - loss: 2.0054\n",
            "Epoch 20: val_accuracy improved from 0.20126 to 0.28302, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5965 - loss: 2.0066 - val_accuracy: 0.2830 - val_loss: 3.2938\n",
            "Epoch 21/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6238 - loss: 1.8613\n",
            "Epoch 21: val_accuracy improved from 0.28302 to 0.30189, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.6239 - loss: 1.8616 - val_accuracy: 0.3019 - val_loss: 3.0950\n",
            "Epoch 22/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6421 - loss: 1.7631\n",
            "Epoch 22: val_accuracy improved from 0.30189 to 0.35220, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6420 - loss: 1.7623 - val_accuracy: 0.3522 - val_loss: 2.8629\n",
            "Epoch 23/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7108 - loss: 1.5908\n",
            "Epoch 23: val_accuracy did not improve from 0.35220\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7102 - loss: 1.5912 - val_accuracy: 0.3522 - val_loss: 2.7318\n",
            "Epoch 24/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7074 - loss: 1.4785\n",
            "Epoch 24: val_accuracy improved from 0.35220 to 0.42767, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7079 - loss: 1.4780 - val_accuracy: 0.4277 - val_loss: 2.4221\n",
            "Epoch 25/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7436 - loss: 1.3998\n",
            "Epoch 25: val_accuracy did not improve from 0.42767\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7425 - loss: 1.3999 - val_accuracy: 0.4214 - val_loss: 2.3180\n",
            "Epoch 26/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7847 - loss: 1.2050\n",
            "Epoch 26: val_accuracy improved from 0.42767 to 0.50314, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7829 - loss: 1.2086 - val_accuracy: 0.5031 - val_loss: 2.0680\n",
            "Epoch 27/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7578 - loss: 1.1743\n",
            "Epoch 27: val_accuracy did not improve from 0.50314\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.7576 - loss: 1.1754 - val_accuracy: 0.4403 - val_loss: 2.1948\n",
            "Epoch 28/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7936 - loss: 1.1660\n",
            "Epoch 28: val_accuracy improved from 0.50314 to 0.57862, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.7933 - loss: 1.1654 - val_accuracy: 0.5786 - val_loss: 1.7985\n",
            "Epoch 29/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8404 - loss: 0.9797\n",
            "Epoch 29: val_accuracy improved from 0.57862 to 0.71698, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8383 - loss: 0.9820 - val_accuracy: 0.7170 - val_loss: 1.5480\n",
            "Epoch 30/30\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8091 - loss: 0.9826\n",
            "Epoch 30: val_accuracy did not improve from 0.71698\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8093 - loss: 0.9809 - val_accuracy: 0.6352 - val_loss: 1.5383\n",
            "Train Accuracy: 0.8131, Test Accuracy: 0.6919\n",
            "Training with epochs: 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_22\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_22\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_44             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_68 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_66     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_66           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_69 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_67     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_67           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_70 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_45             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_68     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_110 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_68           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_89 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_110[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_111 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_88 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_90 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_111[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_22             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_112 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_91 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_112[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_113 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_114 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_113[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_44             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_66     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_66           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_67     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_67           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_45             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_68     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_68           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_110[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_111[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_22             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_112[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_113 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_113[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - accuracy: 0.0149 - loss: 4.6551\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 463ms/step - accuracy: 0.0151 - loss: 4.6540 - val_accuracy: 0.0189 - val_loss: 4.5762\n",
            "Epoch 2/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0410 - loss: 4.5132\n",
            "Epoch 2: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.0407 - loss: 4.5129 - val_accuracy: 0.0126 - val_loss: 4.5598\n",
            "Epoch 3/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0429 - loss: 4.4744\n",
            "Epoch 3: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.0431 - loss: 4.4735 - val_accuracy: 0.0063 - val_loss: 4.5446\n",
            "Epoch 4/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0483 - loss: 4.4049\n",
            "Epoch 4: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.0489 - loss: 4.4040 - val_accuracy: 0.0063 - val_loss: 4.5334\n",
            "Epoch 5/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0615 - loss: 4.3447\n",
            "Epoch 5: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.0624 - loss: 4.3435 - val_accuracy: 0.0063 - val_loss: 4.5179\n",
            "Epoch 6/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0615 - loss: 4.2434\n",
            "Epoch 6: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0616 - loss: 4.2429 - val_accuracy: 0.0063 - val_loss: 4.5005\n",
            "Epoch 7/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0937 - loss: 4.1698\n",
            "Epoch 7: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.0943 - loss: 4.1691 - val_accuracy: 0.0063 - val_loss: 4.4866\n",
            "Epoch 8/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1307 - loss: 4.0417\n",
            "Epoch 8: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.1304 - loss: 4.0414 - val_accuracy: 0.0063 - val_loss: 4.4776\n",
            "Epoch 9/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1348 - loss: 3.9377\n",
            "Epoch 9: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.1355 - loss: 3.9366 - val_accuracy: 0.0063 - val_loss: 4.4615\n",
            "Epoch 10/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1681 - loss: 3.8242\n",
            "Epoch 10: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.1683 - loss: 3.8226 - val_accuracy: 0.0063 - val_loss: 4.4464\n",
            "Epoch 11/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1904 - loss: 3.7155\n",
            "Epoch 11: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.1915 - loss: 3.7117 - val_accuracy: 0.0063 - val_loss: 4.4245\n",
            "Epoch 12/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2634 - loss: 3.4687\n",
            "Epoch 12: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.2634 - loss: 3.4679 - val_accuracy: 0.0126 - val_loss: 4.3982\n",
            "Epoch 13/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2874 - loss: 3.3383\n",
            "Epoch 13: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.2876 - loss: 3.3353 - val_accuracy: 0.0252 - val_loss: 4.3611\n",
            "Epoch 14/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3865 - loss: 3.1497\n",
            "Epoch 14: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3860 - loss: 3.1473 - val_accuracy: 0.0314 - val_loss: 4.3752\n",
            "Epoch 15/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3924 - loss: 2.9699\n",
            "Epoch 15: val_accuracy improved from 0.03145 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.3926 - loss: 2.9676 - val_accuracy: 0.0503 - val_loss: 4.2971\n",
            "Epoch 16/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4285 - loss: 2.7434\n",
            "Epoch 16: val_accuracy improved from 0.05031 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.4291 - loss: 2.7426 - val_accuracy: 0.0566 - val_loss: 4.2390\n",
            "Epoch 17/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4850 - loss: 2.4821\n",
            "Epoch 17: val_accuracy improved from 0.05660 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.4851 - loss: 2.4839 - val_accuracy: 0.0943 - val_loss: 4.1594\n",
            "Epoch 18/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.4592 - loss: 2.4600\n",
            "Epoch 18: val_accuracy improved from 0.09434 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.4604 - loss: 2.4574 - val_accuracy: 0.1069 - val_loss: 4.0266\n",
            "Epoch 19/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5439 - loss: 2.1843\n",
            "Epoch 19: val_accuracy improved from 0.10692 to 0.12579, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5426 - loss: 2.1872 - val_accuracy: 0.1258 - val_loss: 3.9242\n",
            "Epoch 20/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5839 - loss: 2.0161\n",
            "Epoch 20: val_accuracy did not improve from 0.12579\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5835 - loss: 2.0176 - val_accuracy: 0.1069 - val_loss: 3.8586\n",
            "Epoch 21/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6605 - loss: 1.8586\n",
            "Epoch 21: val_accuracy improved from 0.12579 to 0.13208, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6594 - loss: 1.8596 - val_accuracy: 0.1321 - val_loss: 3.6665\n",
            "Epoch 22/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6163 - loss: 1.7938\n",
            "Epoch 22: val_accuracy improved from 0.13208 to 0.19497, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.6167 - loss: 1.7944 - val_accuracy: 0.1950 - val_loss: 3.4307\n",
            "Epoch 23/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6640 - loss: 1.6834\n",
            "Epoch 23: val_accuracy did not improve from 0.19497\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.6640 - loss: 1.6821 - val_accuracy: 0.1824 - val_loss: 3.3990\n",
            "Epoch 24/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6990 - loss: 1.5409\n",
            "Epoch 24: val_accuracy improved from 0.19497 to 0.28302, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.6984 - loss: 1.5412 - val_accuracy: 0.2830 - val_loss: 3.0049\n",
            "Epoch 25/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7203 - loss: 1.3992\n",
            "Epoch 25: val_accuracy improved from 0.28302 to 0.33962, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7191 - loss: 1.4004 - val_accuracy: 0.3396 - val_loss: 2.8062\n",
            "Epoch 26/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7611 - loss: 1.3421\n",
            "Epoch 26: val_accuracy did not improve from 0.33962\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7589 - loss: 1.3456 - val_accuracy: 0.3208 - val_loss: 2.8090\n",
            "Epoch 27/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7199 - loss: 1.2779\n",
            "Epoch 27: val_accuracy improved from 0.33962 to 0.38994, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.7200 - loss: 1.2782 - val_accuracy: 0.3899 - val_loss: 2.3560\n",
            "Epoch 28/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6962 - loss: 1.2651\n",
            "Epoch 28: val_accuracy improved from 0.38994 to 0.44025, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.6976 - loss: 1.2633 - val_accuracy: 0.4403 - val_loss: 2.1811\n",
            "Epoch 29/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7552 - loss: 1.1423\n",
            "Epoch 29: val_accuracy improved from 0.44025 to 0.46541, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7552 - loss: 1.1410 - val_accuracy: 0.4654 - val_loss: 2.0109\n",
            "Epoch 30/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7990 - loss: 1.0349\n",
            "Epoch 30: val_accuracy improved from 0.46541 to 0.59119, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.7988 - loss: 1.0356 - val_accuracy: 0.5912 - val_loss: 1.7029\n",
            "Epoch 31/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7657 - loss: 1.0384\n",
            "Epoch 31: val_accuracy improved from 0.59119 to 0.64151, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7663 - loss: 1.0374 - val_accuracy: 0.6415 - val_loss: 1.5887\n",
            "Epoch 32/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8124 - loss: 0.8579\n",
            "Epoch 32: val_accuracy did not improve from 0.64151\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8116 - loss: 0.8600 - val_accuracy: 0.6038 - val_loss: 1.5272\n",
            "Epoch 33/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8189 - loss: 0.8616\n",
            "Epoch 33: val_accuracy improved from 0.64151 to 0.71069, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8188 - loss: 0.8626 - val_accuracy: 0.7107 - val_loss: 1.2731\n",
            "Epoch 34/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8308 - loss: 0.8523\n",
            "Epoch 34: val_accuracy improved from 0.71069 to 0.79245, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8298 - loss: 0.8535 - val_accuracy: 0.7925 - val_loss: 1.1485\n",
            "Epoch 35/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8394 - loss: 0.7684\n",
            "Epoch 35: val_accuracy did not improve from 0.79245\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8383 - loss: 0.7715 - val_accuracy: 0.7799 - val_loss: 1.0809\n",
            "Epoch 36/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8337 - loss: 0.7499\n",
            "Epoch 36: val_accuracy did not improve from 0.79245\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8330 - loss: 0.7508 - val_accuracy: 0.7170 - val_loss: 1.1081\n",
            "Epoch 37/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8773 - loss: 0.6674\n",
            "Epoch 37: val_accuracy improved from 0.79245 to 0.83648, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8762 - loss: 0.6688 - val_accuracy: 0.8365 - val_loss: 0.9398\n",
            "Epoch 38/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8351 - loss: 0.7259\n",
            "Epoch 38: val_accuracy did not improve from 0.83648\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8353 - loss: 0.7255 - val_accuracy: 0.7987 - val_loss: 0.9238\n",
            "Epoch 39/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8716 - loss: 0.6535\n",
            "Epoch 39: val_accuracy did not improve from 0.83648\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8715 - loss: 0.6533 - val_accuracy: 0.8302 - val_loss: 0.8590\n",
            "Epoch 40/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8577 - loss: 0.6215\n",
            "Epoch 40: val_accuracy did not improve from 0.83648\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8574 - loss: 0.6227 - val_accuracy: 0.8239 - val_loss: 0.7756\n",
            "Epoch 41/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8737 - loss: 0.6075\n",
            "Epoch 41: val_accuracy improved from 0.83648 to 0.86164, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8740 - loss: 0.6076 - val_accuracy: 0.8616 - val_loss: 0.7392\n",
            "Epoch 42/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8783 - loss: 0.5593\n",
            "Epoch 42: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8783 - loss: 0.5588 - val_accuracy: 0.8491 - val_loss: 0.7524\n",
            "Epoch 43/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9046 - loss: 0.4965\n",
            "Epoch 43: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9042 - loss: 0.4983 - val_accuracy: 0.8050 - val_loss: 0.7507\n",
            "Epoch 44/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8784 - loss: 0.5339\n",
            "Epoch 44: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8783 - loss: 0.5350 - val_accuracy: 0.8365 - val_loss: 0.6784\n",
            "Epoch 45/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8839 - loss: 0.5144\n",
            "Epoch 45: val_accuracy improved from 0.86164 to 0.90566, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8840 - loss: 0.5151 - val_accuracy: 0.9057 - val_loss: 0.6128\n",
            "Epoch 46/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9070 - loss: 0.4842\n",
            "Epoch 46: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9070 - loss: 0.4844 - val_accuracy: 0.8805 - val_loss: 0.5853\n",
            "Epoch 47/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9105 - loss: 0.4455\n",
            "Epoch 47: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9106 - loss: 0.4458 - val_accuracy: 0.8742 - val_loss: 0.5890\n",
            "Epoch 48/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9054 - loss: 0.4484\n",
            "Epoch 48: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9058 - loss: 0.4477 - val_accuracy: 0.8868 - val_loss: 0.5594\n",
            "Epoch 49/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9199 - loss: 0.4170\n",
            "Epoch 49: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9200 - loss: 0.4173 - val_accuracy: 0.9057 - val_loss: 0.5437\n",
            "Epoch 50/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9156 - loss: 0.4260\n",
            "Epoch 50: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9156 - loss: 0.4267 - val_accuracy: 0.8868 - val_loss: 0.5816\n",
            "Train Accuracy: 0.9735, Test Accuracy: 0.8889\n",
            "Training with epochs: 70\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_46             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_71 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_69     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_69           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_72 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_70     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_70           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_73 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_47             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_71     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_115 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_71           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_93 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_115[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_116 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_92 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_94 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_23             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_117 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_95 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_118 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_119 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_46             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_69     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_69           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_70     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_70           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_47             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_71     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_71           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_115[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_23             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.0010 - loss: 4.6821   \n",
            "Epoch 1: val_accuracy improved from -inf to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 454ms/step - accuracy: 0.0011 - loss: 4.6806 - val_accuracy: 0.0126 - val_loss: 4.6024\n",
            "Epoch 2/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0048 - loss: 4.5693\n",
            "Epoch 2: val_accuracy improved from 0.01258 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.0051 - loss: 4.5681 - val_accuracy: 0.0189 - val_loss: 4.5877\n",
            "Epoch 3/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0339 - loss: 4.4684\n",
            "Epoch 3: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0339 - loss: 4.4682 - val_accuracy: 0.0126 - val_loss: 4.5715\n",
            "Epoch 4/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0416 - loss: 4.3963\n",
            "Epoch 4: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0421 - loss: 4.3955 - val_accuracy: 0.0252 - val_loss: 4.5534\n",
            "Epoch 5/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0723 - loss: 4.3441\n",
            "Epoch 5: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.0721 - loss: 4.3430 - val_accuracy: 0.0189 - val_loss: 4.5349\n",
            "Epoch 6/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1055 - loss: 4.2143\n",
            "Epoch 6: val_accuracy improved from 0.02516 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.1045 - loss: 4.2157 - val_accuracy: 0.0377 - val_loss: 4.5178\n",
            "Epoch 7/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0878 - loss: 4.1978\n",
            "Epoch 7: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0886 - loss: 4.1960 - val_accuracy: 0.0377 - val_loss: 4.4978\n",
            "Epoch 8/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.1341 - loss: 4.0397\n",
            "Epoch 8: val_accuracy improved from 0.03774 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.1338 - loss: 4.0400 - val_accuracy: 0.0503 - val_loss: 4.4772\n",
            "Epoch 9/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1498 - loss: 3.9958\n",
            "Epoch 9: val_accuracy improved from 0.05031 to 0.06918, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.1507 - loss: 3.9929 - val_accuracy: 0.0692 - val_loss: 4.4421\n",
            "Epoch 10/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2030 - loss: 3.7901\n",
            "Epoch 10: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.2033 - loss: 3.7897 - val_accuracy: 0.0692 - val_loss: 4.4050\n",
            "Epoch 11/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2278 - loss: 3.6628\n",
            "Epoch 11: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.2277 - loss: 3.6621 - val_accuracy: 0.0629 - val_loss: 4.3570\n",
            "Epoch 12/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2453 - loss: 3.5195\n",
            "Epoch 12: val_accuracy improved from 0.06918 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.2465 - loss: 3.5182 - val_accuracy: 0.0881 - val_loss: 4.3008\n",
            "Epoch 13/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3243 - loss: 3.3686\n",
            "Epoch 13: val_accuracy did not improve from 0.08805\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.3249 - loss: 3.3663 - val_accuracy: 0.0755 - val_loss: 4.2393\n",
            "Epoch 14/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3803 - loss: 3.1417\n",
            "Epoch 14: val_accuracy did not improve from 0.08805\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.3798 - loss: 3.1416 - val_accuracy: 0.0881 - val_loss: 4.1550\n",
            "Epoch 15/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3606 - loss: 3.0591\n",
            "Epoch 15: val_accuracy improved from 0.08805 to 0.14465, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.3609 - loss: 3.0567 - val_accuracy: 0.1447 - val_loss: 4.0681\n",
            "Epoch 16/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4577 - loss: 2.7438\n",
            "Epoch 16: val_accuracy did not improve from 0.14465\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.4567 - loss: 2.7453 - val_accuracy: 0.1258 - val_loss: 3.9616\n",
            "Epoch 17/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4535 - loss: 2.6597\n",
            "Epoch 17: val_accuracy improved from 0.14465 to 0.15094, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.4544 - loss: 2.6570 - val_accuracy: 0.1509 - val_loss: 3.8090\n",
            "Epoch 18/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5228 - loss: 2.4327\n",
            "Epoch 18: val_accuracy did not improve from 0.15094\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5221 - loss: 2.4330 - val_accuracy: 0.1447 - val_loss: 3.6833\n",
            "Epoch 19/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5185 - loss: 2.2708\n",
            "Epoch 19: val_accuracy improved from 0.15094 to 0.20126, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.5186 - loss: 2.2707 - val_accuracy: 0.2013 - val_loss: 3.5278\n",
            "Epoch 20/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6409 - loss: 2.0396\n",
            "Epoch 20: val_accuracy improved from 0.20126 to 0.22642, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.6394 - loss: 2.0408 - val_accuracy: 0.2264 - val_loss: 3.3830\n",
            "Epoch 21/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6532 - loss: 1.9128\n",
            "Epoch 21: val_accuracy improved from 0.22642 to 0.27044, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6521 - loss: 1.9126 - val_accuracy: 0.2704 - val_loss: 3.1562\n",
            "Epoch 22/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6148 - loss: 1.8635\n",
            "Epoch 22: val_accuracy improved from 0.27044 to 0.30818, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.6144 - loss: 1.8627 - val_accuracy: 0.3082 - val_loss: 2.9915\n",
            "Epoch 23/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6870 - loss: 1.6621\n",
            "Epoch 23: val_accuracy improved from 0.30818 to 0.35220, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6867 - loss: 1.6630 - val_accuracy: 0.3522 - val_loss: 2.7600\n",
            "Epoch 24/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6831 - loss: 1.5569\n",
            "Epoch 24: val_accuracy improved from 0.35220 to 0.41509, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6834 - loss: 1.5580 - val_accuracy: 0.4151 - val_loss: 2.5827\n",
            "Epoch 25/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6950 - loss: 1.4907\n",
            "Epoch 25: val_accuracy did not improve from 0.41509\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.6954 - loss: 1.4892 - val_accuracy: 0.4151 - val_loss: 2.4479\n",
            "Epoch 26/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7367 - loss: 1.3732\n",
            "Epoch 26: val_accuracy improved from 0.41509 to 0.48428, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.7366 - loss: 1.3736 - val_accuracy: 0.4843 - val_loss: 2.1941\n",
            "Epoch 27/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7338 - loss: 1.2841\n",
            "Epoch 27: val_accuracy did not improve from 0.48428\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.7337 - loss: 1.2828 - val_accuracy: 0.4528 - val_loss: 2.1780\n",
            "Epoch 28/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7286 - loss: 1.2269\n",
            "Epoch 28: val_accuracy improved from 0.48428 to 0.55346, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7279 - loss: 1.2283 - val_accuracy: 0.5535 - val_loss: 1.8625\n",
            "Epoch 29/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7939 - loss: 1.0905\n",
            "Epoch 29: val_accuracy improved from 0.55346 to 0.61635, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.7931 - loss: 1.0930 - val_accuracy: 0.6164 - val_loss: 1.6720\n",
            "Epoch 30/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7754 - loss: 1.0501\n",
            "Epoch 30: val_accuracy improved from 0.61635 to 0.64780, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.7751 - loss: 1.0506 - val_accuracy: 0.6478 - val_loss: 1.5629\n",
            "Epoch 31/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7668 - loss: 1.0489\n",
            "Epoch 31: val_accuracy improved from 0.64780 to 0.68553, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.7680 - loss: 1.0463 - val_accuracy: 0.6855 - val_loss: 1.4188\n",
            "Epoch 32/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7964 - loss: 0.9547\n",
            "Epoch 32: val_accuracy did not improve from 0.68553\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.7968 - loss: 0.9534 - val_accuracy: 0.6730 - val_loss: 1.3499\n",
            "Epoch 33/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8260 - loss: 0.8474\n",
            "Epoch 33: val_accuracy improved from 0.68553 to 0.72327, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8260 - loss: 0.8483 - val_accuracy: 0.7233 - val_loss: 1.2337\n",
            "Epoch 34/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8212 - loss: 0.8040\n",
            "Epoch 34: val_accuracy improved from 0.72327 to 0.72956, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.8202 - loss: 0.8064 - val_accuracy: 0.7296 - val_loss: 1.1379\n",
            "Epoch 35/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8612 - loss: 0.7704\n",
            "Epoch 35: val_accuracy improved from 0.72956 to 0.77358, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8612 - loss: 0.7703 - val_accuracy: 0.7736 - val_loss: 1.0321\n",
            "Epoch 36/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8362 - loss: 0.7290\n",
            "Epoch 36: val_accuracy did not improve from 0.77358\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8360 - loss: 0.7305 - val_accuracy: 0.7610 - val_loss: 0.9940\n",
            "Epoch 37/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8624 - loss: 0.6774\n",
            "Epoch 37: val_accuracy improved from 0.77358 to 0.77987, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.8616 - loss: 0.6794 - val_accuracy: 0.7799 - val_loss: 1.0200\n",
            "Epoch 38/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8575 - loss: 0.6857\n",
            "Epoch 38: val_accuracy improved from 0.77987 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8567 - loss: 0.6885 - val_accuracy: 0.8176 - val_loss: 0.8392\n",
            "Epoch 39/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8861 - loss: 0.6293\n",
            "Epoch 39: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.8856 - loss: 0.6296 - val_accuracy: 0.8113 - val_loss: 0.8111\n",
            "Epoch 40/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8957 - loss: 0.6014\n",
            "Epoch 40: val_accuracy improved from 0.81761 to 0.83019, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8957 - loss: 0.6017 - val_accuracy: 0.8302 - val_loss: 0.7396\n",
            "Epoch 41/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9033 - loss: 0.5464\n",
            "Epoch 41: val_accuracy improved from 0.83019 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9025 - loss: 0.5476 - val_accuracy: 0.8491 - val_loss: 0.8038\n",
            "Epoch 42/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9104 - loss: 0.5212\n",
            "Epoch 42: val_accuracy improved from 0.84906 to 0.86164, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9102 - loss: 0.5224 - val_accuracy: 0.8616 - val_loss: 0.6957\n",
            "Epoch 43/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8813 - loss: 0.5986\n",
            "Epoch 43: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.8802 - loss: 0.5989 - val_accuracy: 0.8553 - val_loss: 0.6224\n",
            "Epoch 44/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8966 - loss: 0.5122\n",
            "Epoch 44: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.8966 - loss: 0.5135 - val_accuracy: 0.8553 - val_loss: 0.6630\n",
            "Epoch 45/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8875 - loss: 0.4879\n",
            "Epoch 45: val_accuracy improved from 0.86164 to 0.87421, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8877 - loss: 0.4886 - val_accuracy: 0.8742 - val_loss: 0.5780\n",
            "Epoch 46/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9008 - loss: 0.4983\n",
            "Epoch 46: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9011 - loss: 0.4975 - val_accuracy: 0.8742 - val_loss: 0.5977\n",
            "Epoch 47/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9569 - loss: 0.3598\n",
            "Epoch 47: val_accuracy improved from 0.87421 to 0.89308, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9560 - loss: 0.3615 - val_accuracy: 0.8931 - val_loss: 0.5682\n",
            "Epoch 48/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9126 - loss: 0.4313\n",
            "Epoch 48: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9121 - loss: 0.4320 - val_accuracy: 0.8931 - val_loss: 0.5387\n",
            "Epoch 49/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9329 - loss: 0.3833\n",
            "Epoch 49: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9323 - loss: 0.3848 - val_accuracy: 0.8868 - val_loss: 0.5270\n",
            "Epoch 50/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9232 - loss: 0.3879\n",
            "Epoch 50: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9230 - loss: 0.3891 - val_accuracy: 0.8616 - val_loss: 0.5170\n",
            "Epoch 51/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9346 - loss: 0.3730\n",
            "Epoch 51: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9347 - loss: 0.3724 - val_accuracy: 0.8679 - val_loss: 0.5700\n",
            "Epoch 52/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9096 - loss: 0.3964\n",
            "Epoch 52: val_accuracy improved from 0.89308 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9097 - loss: 0.3966 - val_accuracy: 0.8994 - val_loss: 0.4701\n",
            "Epoch 53/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9378 - loss: 0.3388\n",
            "Epoch 53: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.9375 - loss: 0.3393 - val_accuracy: 0.8868 - val_loss: 0.4582\n",
            "Epoch 54/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9254 - loss: 0.3964\n",
            "Epoch 54: val_accuracy improved from 0.89937 to 0.90566, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9253 - loss: 0.3956 - val_accuracy: 0.9057 - val_loss: 0.4766\n",
            "Epoch 55/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9059 - loss: 0.3614\n",
            "Epoch 55: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9061 - loss: 0.3609 - val_accuracy: 0.8742 - val_loss: 0.4578\n",
            "Epoch 56/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9357 - loss: 0.3232\n",
            "Epoch 56: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9356 - loss: 0.3236 - val_accuracy: 0.8805 - val_loss: 0.4453\n",
            "Epoch 57/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9440 - loss: 0.3228\n",
            "Epoch 57: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9432 - loss: 0.3243 - val_accuracy: 0.8553 - val_loss: 0.4884\n",
            "Epoch 58/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9371 - loss: 0.3195\n",
            "Epoch 58: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9375 - loss: 0.3191 - val_accuracy: 0.9057 - val_loss: 0.3696\n",
            "Epoch 59/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9600 - loss: 0.2826\n",
            "Epoch 59: val_accuracy improved from 0.90566 to 0.92453, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9591 - loss: 0.2841 - val_accuracy: 0.9245 - val_loss: 0.4098\n",
            "Epoch 60/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9407 - loss: 0.3126\n",
            "Epoch 60: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9409 - loss: 0.3115 - val_accuracy: 0.8931 - val_loss: 0.4589\n",
            "Epoch 61/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9480 - loss: 0.2584\n",
            "Epoch 61: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9476 - loss: 0.2592 - val_accuracy: 0.8868 - val_loss: 0.4359\n",
            "Epoch 62/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9607 - loss: 0.2205\n",
            "Epoch 62: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.9607 - loss: 0.2207 - val_accuracy: 0.9057 - val_loss: 0.4156\n",
            "Epoch 63/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9621 - loss: 0.2541\n",
            "Epoch 63: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9620 - loss: 0.2542 - val_accuracy: 0.9182 - val_loss: 0.3650\n",
            "Epoch 64/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9402 - loss: 0.2552\n",
            "Epoch 64: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9399 - loss: 0.2552 - val_accuracy: 0.9245 - val_loss: 0.3123\n",
            "Epoch 65/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9690 - loss: 0.1945\n",
            "Epoch 65: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9683 - loss: 0.1962 - val_accuracy: 0.9245 - val_loss: 0.3394\n",
            "Epoch 66/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9589 - loss: 0.2217\n",
            "Epoch 66: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9586 - loss: 0.2220 - val_accuracy: 0.9057 - val_loss: 0.3690\n",
            "Epoch 67/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9539 - loss: 0.2484\n",
            "Epoch 67: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9540 - loss: 0.2486 - val_accuracy: 0.8742 - val_loss: 0.4281\n",
            "Epoch 68/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9607 - loss: 0.2137\n",
            "Epoch 68: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9604 - loss: 0.2148 - val_accuracy: 0.8994 - val_loss: 0.3508\n",
            "Epoch 69/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9508 - loss: 0.2201\n",
            "Epoch 69: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9506 - loss: 0.2210 - val_accuracy: 0.9245 - val_loss: 0.3333\n",
            "Epoch 70/70\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9603 - loss: 0.2175\n",
            "Epoch 70: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9604 - loss: 0.2179 - val_accuracy: 0.9245 - val_loss: 0.3478\n",
            "Train Accuracy: 0.9848, Test Accuracy: 0.9242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Different dropouts"
      ],
      "metadata": {
        "id": "zzuyWzHnw0hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_list = [0.2 , 0.4 , 0.5]\n",
        "\n",
        "for dropout in dropout_list:\n",
        "    print(f\"Training with dropout: {dropout}\")x\n",
        "    training2(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs=55, batch_size=32, learning_rate=0.0001,dropout=dropout, optimizer_name=\"adam\", l2_factor=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dPlSozqNGIw_",
        "outputId": "af0203d8-a16c-406f-9a37-910f82348cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with dropout: 0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_24\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_24\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_48             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_74 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_72     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_72           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_75 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_73     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_73           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_76 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_49             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_74     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_120 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_74           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_97 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_121 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_96 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_98 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_24             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_122 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_99 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_123 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_99[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_124 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_123[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_48             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_72     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_72           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_73     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_73           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_49             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_74     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_74           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_24             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_123 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.0070 - loss: 4.6098\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 472ms/step - accuracy: 0.0072 - loss: 4.6097 - val_accuracy: 0.0126 - val_loss: 4.5747\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0335 - loss: 4.5254\n",
            "Epoch 2: val_accuracy improved from 0.01258 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.0335 - loss: 4.5249 - val_accuracy: 0.0189 - val_loss: 4.5581\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0359 - loss: 4.4632\n",
            "Epoch 3: val_accuracy improved from 0.01887 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0360 - loss: 4.4625 - val_accuracy: 0.0377 - val_loss: 4.5403\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0512 - loss: 4.3983\n",
            "Epoch 4: val_accuracy improved from 0.03774 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0518 - loss: 4.3974 - val_accuracy: 0.0566 - val_loss: 4.5244\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0786 - loss: 4.3181\n",
            "Epoch 5: val_accuracy improved from 0.05660 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0786 - loss: 4.3169 - val_accuracy: 0.0629 - val_loss: 4.5131\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0845 - loss: 4.2346\n",
            "Epoch 6: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.0849 - loss: 4.2333 - val_accuracy: 0.0629 - val_loss: 4.4961\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1300 - loss: 4.1307\n",
            "Epoch 7: val_accuracy improved from 0.06289 to 0.06918, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.1299 - loss: 4.1297 - val_accuracy: 0.0692 - val_loss: 4.4760\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1431 - loss: 3.9961\n",
            "Epoch 8: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.1433 - loss: 3.9961 - val_accuracy: 0.0692 - val_loss: 4.4485\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1543 - loss: 3.9345\n",
            "Epoch 9: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.1544 - loss: 3.9327 - val_accuracy: 0.0377 - val_loss: 4.4186\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1943 - loss: 3.7452\n",
            "Epoch 10: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.1939 - loss: 3.7457 - val_accuracy: 0.0377 - val_loss: 4.3779\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2279 - loss: 3.6400\n",
            "Epoch 11: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.2283 - loss: 3.6386 - val_accuracy: 0.0566 - val_loss: 4.3229\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2586 - loss: 3.4678\n",
            "Epoch 12: val_accuracy did not improve from 0.06918\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.2592 - loss: 3.4670 - val_accuracy: 0.0629 - val_loss: 4.2702\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2961 - loss: 3.3204\n",
            "Epoch 13: val_accuracy improved from 0.06918 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.2973 - loss: 3.3183 - val_accuracy: 0.0755 - val_loss: 4.1967\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3713 - loss: 3.0500\n",
            "Epoch 14: val_accuracy improved from 0.07547 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.3700 - loss: 3.0514 - val_accuracy: 0.0818 - val_loss: 4.1322\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3891 - loss: 2.9222\n",
            "Epoch 15: val_accuracy improved from 0.08176 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.3879 - loss: 2.9237 - val_accuracy: 0.1069 - val_loss: 4.0010\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4759 - loss: 2.7066\n",
            "Epoch 16: val_accuracy improved from 0.10692 to 0.11321, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.4739 - loss: 2.7083 - val_accuracy: 0.1132 - val_loss: 3.8994\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4493 - loss: 2.6033\n",
            "Epoch 17: val_accuracy did not improve from 0.11321\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.4486 - loss: 2.6032 - val_accuracy: 0.1132 - val_loss: 3.8215\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5259 - loss: 2.4047\n",
            "Epoch 18: val_accuracy improved from 0.11321 to 0.14465, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5260 - loss: 2.4027 - val_accuracy: 0.1447 - val_loss: 3.6783\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5663 - loss: 2.2044\n",
            "Epoch 19: val_accuracy improved from 0.14465 to 0.20126, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.5655 - loss: 2.2063 - val_accuracy: 0.2013 - val_loss: 3.4642\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6223 - loss: 2.0006\n",
            "Epoch 20: val_accuracy improved from 0.20126 to 0.20755, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6216 - loss: 2.0004 - val_accuracy: 0.2075 - val_loss: 3.3870\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6181 - loss: 1.9636\n",
            "Epoch 21: val_accuracy improved from 0.20755 to 0.23899, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6186 - loss: 1.9597 - val_accuracy: 0.2390 - val_loss: 3.2135\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6529 - loss: 1.7970\n",
            "Epoch 22: val_accuracy improved from 0.23899 to 0.34591, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.6538 - loss: 1.7946 - val_accuracy: 0.3459 - val_loss: 2.7896\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6665 - loss: 1.6295\n",
            "Epoch 23: val_accuracy improved from 0.34591 to 0.39623, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.6660 - loss: 1.6285 - val_accuracy: 0.3962 - val_loss: 2.6584\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6668 - loss: 1.6021\n",
            "Epoch 24: val_accuracy did not improve from 0.39623\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.6672 - loss: 1.6010 - val_accuracy: 0.3774 - val_loss: 2.4900\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7094 - loss: 1.4163\n",
            "Epoch 25: val_accuracy improved from 0.39623 to 0.46541, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.7088 - loss: 1.4186 - val_accuracy: 0.4654 - val_loss: 2.3181\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7366 - loss: 1.3440\n",
            "Epoch 26: val_accuracy improved from 0.46541 to 0.53459, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7364 - loss: 1.3438 - val_accuracy: 0.5346 - val_loss: 2.0251\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7434 - loss: 1.2026\n",
            "Epoch 27: val_accuracy improved from 0.53459 to 0.56604, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7425 - loss: 1.2046 - val_accuracy: 0.5660 - val_loss: 1.9243\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7503 - loss: 1.1467\n",
            "Epoch 28: val_accuracy did not improve from 0.56604\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.7503 - loss: 1.1483 - val_accuracy: 0.4969 - val_loss: 2.0120\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7492 - loss: 1.1515\n",
            "Epoch 29: val_accuracy improved from 0.56604 to 0.61635, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7500 - loss: 1.1509 - val_accuracy: 0.6164 - val_loss: 1.6640\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7975 - loss: 1.0565\n",
            "Epoch 30: val_accuracy improved from 0.61635 to 0.67296, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.7972 - loss: 1.0572 - val_accuracy: 0.6730 - val_loss: 1.5066\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8115 - loss: 0.9510\n",
            "Epoch 31: val_accuracy improved from 0.67296 to 0.68553, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.8115 - loss: 0.9513 - val_accuracy: 0.6855 - val_loss: 1.4287\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8193 - loss: 0.8706\n",
            "Epoch 32: val_accuracy improved from 0.68553 to 0.79874, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8179 - loss: 0.8741 - val_accuracy: 0.7987 - val_loss: 1.1356\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8035 - loss: 0.9080\n",
            "Epoch 33: val_accuracy did not improve from 0.79874\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8028 - loss: 0.9074 - val_accuracy: 0.7673 - val_loss: 1.1522\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8398 - loss: 0.7836\n",
            "Epoch 34: val_accuracy improved from 0.79874 to 0.82390, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8392 - loss: 0.7845 - val_accuracy: 0.8239 - val_loss: 0.9974\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8338 - loss: 0.7854\n",
            "Epoch 35: val_accuracy improved from 0.82390 to 0.83019, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8340 - loss: 0.7856 - val_accuracy: 0.8302 - val_loss: 0.9188\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8202 - loss: 0.7200\n",
            "Epoch 36: val_accuracy did not improve from 0.83019\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8203 - loss: 0.7205 - val_accuracy: 0.8302 - val_loss: 0.8679\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8775 - loss: 0.6773\n",
            "Epoch 37: val_accuracy improved from 0.83019 to 0.87421, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.8773 - loss: 0.6767 - val_accuracy: 0.8742 - val_loss: 0.7921\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8559 - loss: 0.6812\n",
            "Epoch 38: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8560 - loss: 0.6816 - val_accuracy: 0.8616 - val_loss: 0.7765\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8701 - loss: 0.5977\n",
            "Epoch 39: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.8703 - loss: 0.5984 - val_accuracy: 0.8616 - val_loss: 0.7513\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8830 - loss: 0.5918\n",
            "Epoch 40: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.8815 - loss: 0.5932 - val_accuracy: 0.8742 - val_loss: 0.6893\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9022 - loss: 0.5354\n",
            "Epoch 41: val_accuracy improved from 0.87421 to 0.89308, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9021 - loss: 0.5358 - val_accuracy: 0.8931 - val_loss: 0.6701\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8890 - loss: 0.5515\n",
            "Epoch 42: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8884 - loss: 0.5528 - val_accuracy: 0.5975 - val_loss: 1.3613\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8689 - loss: 0.5645\n",
            "Epoch 43: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.8691 - loss: 0.5643 - val_accuracy: 0.8239 - val_loss: 0.7820\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8901 - loss: 0.5086\n",
            "Epoch 44: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.8898 - loss: 0.5091 - val_accuracy: 0.8868 - val_loss: 0.5684\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8880 - loss: 0.5058\n",
            "Epoch 45: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.8886 - loss: 0.5055 - val_accuracy: 0.2830 - val_loss: 3.0432\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8873 - loss: 0.5134\n",
            "Epoch 46: val_accuracy improved from 0.89308 to 0.92453, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8876 - loss: 0.5131 - val_accuracy: 0.9245 - val_loss: 0.5052\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9077 - loss: 0.4150\n",
            "Epoch 47: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9072 - loss: 0.4167 - val_accuracy: 0.8616 - val_loss: 0.6237\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9323 - loss: 0.4012\n",
            "Epoch 48: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9318 - loss: 0.4024 - val_accuracy: 0.8931 - val_loss: 0.5693\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9046 - loss: 0.4263\n",
            "Epoch 49: val_accuracy improved from 0.92453 to 0.94969, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.9044 - loss: 0.4273 - val_accuracy: 0.9497 - val_loss: 0.4351\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9208 - loss: 0.4022\n",
            "Epoch 50: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9207 - loss: 0.4024 - val_accuracy: 0.8742 - val_loss: 0.5769\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9422 - loss: 0.3949\n",
            "Epoch 51: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9414 - loss: 0.3954 - val_accuracy: 0.9119 - val_loss: 0.4681\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9187 - loss: 0.3837\n",
            "Epoch 52: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.9191 - loss: 0.3837 - val_accuracy: 0.9182 - val_loss: 0.4838\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9271 - loss: 0.3795\n",
            "Epoch 53: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9272 - loss: 0.3788 - val_accuracy: 0.8994 - val_loss: 0.5081\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9493 - loss: 0.3081\n",
            "Epoch 54: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9488 - loss: 0.3089 - val_accuracy: 0.8868 - val_loss: 0.4754\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9382 - loss: 0.3039\n",
            "Epoch 55: val_accuracy did not improve from 0.94969\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9376 - loss: 0.3064 - val_accuracy: 0.9371 - val_loss: 0.3708\n",
            "Train Accuracy: 0.9861, Test Accuracy: 0.9293\n",
            "Training with dropout: 0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_25\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_25\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_50             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_77 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_75     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_75           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_78 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_76     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_76           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_79 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_51             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_77     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_125 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_77           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_101 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_125[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_126 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_100 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_102 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_25             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dense_127 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_103 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_128 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dense_129 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_50             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_75     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_75           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_76     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_76           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_51             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_77     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_77           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_125[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_25             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dense_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dense_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.0053 - loss: 4.7256\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 436ms/step - accuracy: 0.0054 - loss: 4.7247 - val_accuracy: 0.0126 - val_loss: 4.6181\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0141 - loss: 4.6370\n",
            "Epoch 2: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 105ms/step - accuracy: 0.0141 - loss: 4.6363 - val_accuracy: 0.0126 - val_loss: 4.6085\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0336 - loss: 4.5639\n",
            "Epoch 3: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0335 - loss: 4.5641 - val_accuracy: 0.0000e+00 - val_loss: 4.6011\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0194 - loss: 4.5608\n",
            "Epoch 4: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0196 - loss: 4.5596 - val_accuracy: 0.0063 - val_loss: 4.5929\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0351 - loss: 4.4889\n",
            "Epoch 5: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0349 - loss: 4.4894 - val_accuracy: 0.0126 - val_loss: 4.5894\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0362 - loss: 4.4571\n",
            "Epoch 6: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.0362 - loss: 4.4584 - val_accuracy: 0.0126 - val_loss: 4.5832\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0430 - loss: 4.4089\n",
            "Epoch 7: val_accuracy improved from 0.01258 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.0430 - loss: 4.4090 - val_accuracy: 0.0189 - val_loss: 4.5752\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0372 - loss: 4.4166\n",
            "Epoch 8: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.0370 - loss: 4.4157 - val_accuracy: 0.0189 - val_loss: 4.5645\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0453 - loss: 4.3586\n",
            "Epoch 9: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0452 - loss: 4.3590 - val_accuracy: 0.0126 - val_loss: 4.5566\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0452 - loss: 4.3152\n",
            "Epoch 10: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0453 - loss: 4.3155 - val_accuracy: 0.0189 - val_loss: 4.5492\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0685 - loss: 4.2751\n",
            "Epoch 11: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.0681 - loss: 4.2750 - val_accuracy: 0.0126 - val_loss: 4.5410\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0455 - loss: 4.2441\n",
            "Epoch 12: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.0457 - loss: 4.2430 - val_accuracy: 0.0126 - val_loss: 4.5325\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0743 - loss: 4.2085\n",
            "Epoch 13: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0742 - loss: 4.2088 - val_accuracy: 0.0189 - val_loss: 4.5130\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0681 - loss: 4.1805\n",
            "Epoch 14: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.0681 - loss: 4.1793 - val_accuracy: 0.0252 - val_loss: 4.4962\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0802 - loss: 4.1283\n",
            "Epoch 15: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.0803 - loss: 4.1266 - val_accuracy: 0.0314 - val_loss: 4.4725\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1038 - loss: 4.0354\n",
            "Epoch 16: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.1039 - loss: 4.0350 - val_accuracy: 0.0252 - val_loss: 4.4626\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0993 - loss: 4.0034\n",
            "Epoch 17: val_accuracy improved from 0.03145 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.1002 - loss: 4.0019 - val_accuracy: 0.0377 - val_loss: 4.4360\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1426 - loss: 3.8662\n",
            "Epoch 18: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.1417 - loss: 3.8676 - val_accuracy: 0.0377 - val_loss: 4.4156\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1266 - loss: 3.8404\n",
            "Epoch 19: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.1264 - loss: 3.8413 - val_accuracy: 0.0377 - val_loss: 4.3714\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1317 - loss: 3.8307\n",
            "Epoch 20: val_accuracy improved from 0.03774 to 0.06918, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.1326 - loss: 3.8286 - val_accuracy: 0.0692 - val_loss: 4.3078\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1798 - loss: 3.7077\n",
            "Epoch 21: val_accuracy improved from 0.06918 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.1791 - loss: 3.7070 - val_accuracy: 0.0818 - val_loss: 4.2547\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1734 - loss: 3.6106\n",
            "Epoch 22: val_accuracy did not improve from 0.08176\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.1737 - loss: 3.6113 - val_accuracy: 0.0818 - val_loss: 4.2188\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1805 - loss: 3.5403\n",
            "Epoch 23: val_accuracy improved from 0.08176 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.1812 - loss: 3.5395 - val_accuracy: 0.0943 - val_loss: 4.0819\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2452 - loss: 3.3691\n",
            "Epoch 24: val_accuracy did not improve from 0.09434\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.2437 - loss: 3.3720 - val_accuracy: 0.0943 - val_loss: 4.0811\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2318 - loss: 3.2887\n",
            "Epoch 25: val_accuracy did not improve from 0.09434\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.2316 - loss: 3.2895 - val_accuracy: 0.0818 - val_loss: 3.9797\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2343 - loss: 3.3220\n",
            "Epoch 26: val_accuracy improved from 0.09434 to 0.12579, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.2337 - loss: 3.3221 - val_accuracy: 0.1258 - val_loss: 3.8779\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2665 - loss: 3.2101\n",
            "Epoch 27: val_accuracy did not improve from 0.12579\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.2669 - loss: 3.2077 - val_accuracy: 0.1258 - val_loss: 3.8284\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2725 - loss: 3.0750\n",
            "Epoch 28: val_accuracy improved from 0.12579 to 0.15723, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.2729 - loss: 3.0741 - val_accuracy: 0.1572 - val_loss: 3.6860\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2616 - loss: 3.0149\n",
            "Epoch 29: val_accuracy did not improve from 0.15723\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.2632 - loss: 3.0132 - val_accuracy: 0.1572 - val_loss: 3.5529\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3071 - loss: 2.8515\n",
            "Epoch 30: val_accuracy improved from 0.15723 to 0.18239, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.3069 - loss: 2.8528 - val_accuracy: 0.1824 - val_loss: 3.4726\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2883 - loss: 2.8383\n",
            "Epoch 31: val_accuracy improved from 0.18239 to 0.20755, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.2886 - loss: 2.8394 - val_accuracy: 0.2075 - val_loss: 3.3299\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3059 - loss: 2.7781\n",
            "Epoch 32: val_accuracy improved from 0.20755 to 0.23270, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.3060 - loss: 2.7784 - val_accuracy: 0.2327 - val_loss: 3.2542\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3531 - loss: 2.6605\n",
            "Epoch 33: val_accuracy improved from 0.23270 to 0.24528, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3525 - loss: 2.6606 - val_accuracy: 0.2453 - val_loss: 3.1670\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3906 - loss: 2.4899\n",
            "Epoch 34: val_accuracy improved from 0.24528 to 0.32704, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.3895 - loss: 2.4939 - val_accuracy: 0.3270 - val_loss: 2.8695\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3686 - loss: 2.4030\n",
            "Epoch 35: val_accuracy did not improve from 0.32704\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.3687 - loss: 2.4054 - val_accuracy: 0.2516 - val_loss: 3.0241\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3688 - loss: 2.4390\n",
            "Epoch 36: val_accuracy did not improve from 0.32704\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.3693 - loss: 2.4389 - val_accuracy: 0.2893 - val_loss: 2.7752\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3882 - loss: 2.3407\n",
            "Epoch 37: val_accuracy improved from 0.32704 to 0.43396, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3883 - loss: 2.3414 - val_accuracy: 0.4340 - val_loss: 2.5242\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4414 - loss: 2.2147\n",
            "Epoch 38: val_accuracy did not improve from 0.43396\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.4397 - loss: 2.2179 - val_accuracy: 0.4214 - val_loss: 2.3963\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.4577 - loss: 2.2184\n",
            "Epoch 39: val_accuracy did not improve from 0.43396\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.4567 - loss: 2.2199 - val_accuracy: 0.3899 - val_loss: 2.3546\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4895 - loss: 2.1220\n",
            "Epoch 40: val_accuracy improved from 0.43396 to 0.49057, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.4887 - loss: 2.1234 - val_accuracy: 0.4906 - val_loss: 2.1700\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.4679 - loss: 2.1316\n",
            "Epoch 41: val_accuracy did not improve from 0.49057\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.4678 - loss: 2.1309 - val_accuracy: 0.4780 - val_loss: 2.1932\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.4522 - loss: 2.0371\n",
            "Epoch 42: val_accuracy improved from 0.49057 to 0.52201, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.4529 - loss: 2.0384 - val_accuracy: 0.5220 - val_loss: 1.9057\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4274 - loss: 2.0956\n",
            "Epoch 43: val_accuracy did not improve from 0.52201\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.4284 - loss: 2.0927 - val_accuracy: 0.5031 - val_loss: 1.9226\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4987 - loss: 1.9631\n",
            "Epoch 44: val_accuracy did not improve from 0.52201\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.4982 - loss: 1.9625 - val_accuracy: 0.4969 - val_loss: 1.9562\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4272 - loss: 2.0227\n",
            "Epoch 45: val_accuracy improved from 0.52201 to 0.55346, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4283 - loss: 2.0196 - val_accuracy: 0.5535 - val_loss: 1.8700\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5378 - loss: 1.8118\n",
            "Epoch 46: val_accuracy improved from 0.55346 to 0.57233, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.5364 - loss: 1.8128 - val_accuracy: 0.5723 - val_loss: 1.8038\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5398 - loss: 1.7539\n",
            "Epoch 47: val_accuracy improved from 0.57233 to 0.62893, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.5393 - loss: 1.7560 - val_accuracy: 0.6289 - val_loss: 1.5971\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5312 - loss: 1.7148\n",
            "Epoch 48: val_accuracy did not improve from 0.62893\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5312 - loss: 1.7156 - val_accuracy: 0.5597 - val_loss: 1.6324\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5223 - loss: 1.7754\n",
            "Epoch 49: val_accuracy did not improve from 0.62893\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5221 - loss: 1.7743 - val_accuracy: 0.5849 - val_loss: 1.6561\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5735 - loss: 1.6037\n",
            "Epoch 50: val_accuracy did not improve from 0.62893\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5719 - loss: 1.6061 - val_accuracy: 0.5535 - val_loss: 1.6515\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5597 - loss: 1.6786\n",
            "Epoch 51: val_accuracy improved from 0.62893 to 0.64780, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5587 - loss: 1.6801 - val_accuracy: 0.6478 - val_loss: 1.4455\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5687 - loss: 1.5568\n",
            "Epoch 52: val_accuracy did not improve from 0.64780\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5698 - loss: 1.5549 - val_accuracy: 0.6352 - val_loss: 1.3724\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5754 - loss: 1.5865\n",
            "Epoch 53: val_accuracy improved from 0.64780 to 0.69182, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.5753 - loss: 1.5844 - val_accuracy: 0.6918 - val_loss: 1.3721\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5996 - loss: 1.4358\n",
            "Epoch 54: val_accuracy did not improve from 0.69182\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5996 - loss: 1.4377 - val_accuracy: 0.6541 - val_loss: 1.3110\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6233 - loss: 1.4365\n",
            "Epoch 55: val_accuracy improved from 0.69182 to 0.71698, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6225 - loss: 1.4384 - val_accuracy: 0.7170 - val_loss: 1.2652\n",
            "Train Accuracy: 0.8876, Test Accuracy: 0.8232\n",
            "Training with dropout: 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_26\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_26\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_52             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_80 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_78     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_78           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_81 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_79     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_79           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_82 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_53             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_80     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_130 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_80           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_105 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_131 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_104 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_106 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_26             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dense_132 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_107 (\u001b[38;5;33mDropout\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_132[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_133 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dense_134 (\u001b[38;5;33mDense\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_52             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_78     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_78           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_79     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_79           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_53             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_80     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_80           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_26             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dense_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_132[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dense_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.0025 - loss: 4.8266\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 504ms/step - accuracy: 0.0026 - loss: 4.8257 - val_accuracy: 0.0063 - val_loss: 4.5823\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0147 - loss: 4.7063\n",
            "Epoch 2: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.0144 - loss: 4.7063 - val_accuracy: 0.0063 - val_loss: 4.5812\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0032 - loss: 4.6383\n",
            "Epoch 3: val_accuracy improved from 0.00629 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0033 - loss: 4.6395 - val_accuracy: 0.0189 - val_loss: 4.5820\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0208 - loss: 4.6323\n",
            "Epoch 4: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.0208 - loss: 4.6315 - val_accuracy: 0.0189 - val_loss: 4.5816\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0143 - loss: 4.5874\n",
            "Epoch 5: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.0145 - loss: 4.5881 - val_accuracy: 0.0252 - val_loss: 4.5797\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0177 - loss: 4.5974\n",
            "Epoch 6: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0178 - loss: 4.5966 - val_accuracy: 0.0252 - val_loss: 4.5761\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0126 - loss: 4.5505\n",
            "Epoch 7: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0128 - loss: 4.5504 - val_accuracy: 0.0252 - val_loss: 4.5733\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0110 - loss: 4.5390\n",
            "Epoch 8: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.0112 - loss: 4.5384 - val_accuracy: 0.0314 - val_loss: 4.5729\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0335 - loss: 4.4803\n",
            "Epoch 9: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0335 - loss: 4.4803 - val_accuracy: 0.0314 - val_loss: 4.5704\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0206 - loss: 4.4989\n",
            "Epoch 10: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0213 - loss: 4.4982 - val_accuracy: 0.0314 - val_loss: 4.5660\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0284 - loss: 4.4760\n",
            "Epoch 11: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.0284 - loss: 4.4765 - val_accuracy: 0.0314 - val_loss: 4.5638\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0445 - loss: 4.4571\n",
            "Epoch 12: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.0442 - loss: 4.4572 - val_accuracy: 0.0314 - val_loss: 4.5566\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0416 - loss: 4.4438\n",
            "Epoch 13: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0416 - loss: 4.4426 - val_accuracy: 0.0314 - val_loss: 4.5522\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0378 - loss: 4.4106\n",
            "Epoch 14: val_accuracy improved from 0.03145 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.0381 - loss: 4.4101 - val_accuracy: 0.0377 - val_loss: 4.5498\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0517 - loss: 4.3469\n",
            "Epoch 15: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0510 - loss: 4.3486 - val_accuracy: 0.0314 - val_loss: 4.5395\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0504 - loss: 4.3514\n",
            "Epoch 16: val_accuracy improved from 0.03774 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.0509 - loss: 4.3501 - val_accuracy: 0.0440 - val_loss: 4.5228\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0286 - loss: 4.3654\n",
            "Epoch 17: val_accuracy improved from 0.04403 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.0294 - loss: 4.3636 - val_accuracy: 0.0503 - val_loss: 4.5027\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.0512 - loss: 4.3359\n",
            "Epoch 18: val_accuracy improved from 0.05031 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.0516 - loss: 4.3340 - val_accuracy: 0.0629 - val_loss: 4.4844\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0605 - loss: 4.2115\n",
            "Epoch 19: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.0602 - loss: 4.2124 - val_accuracy: 0.0566 - val_loss: 4.4721\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0568 - loss: 4.2649\n",
            "Epoch 20: val_accuracy improved from 0.06289 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0574 - loss: 4.2634 - val_accuracy: 0.0755 - val_loss: 4.4523\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0616 - loss: 4.2061\n",
            "Epoch 21: val_accuracy did not improve from 0.07547\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0622 - loss: 4.2057 - val_accuracy: 0.0692 - val_loss: 4.4226\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0763 - loss: 4.1649\n",
            "Epoch 22: val_accuracy improved from 0.07547 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.0761 - loss: 4.1650 - val_accuracy: 0.0818 - val_loss: 4.3950\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0824 - loss: 4.0955\n",
            "Epoch 23: val_accuracy did not improve from 0.08176\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.0824 - loss: 4.0973 - val_accuracy: 0.0692 - val_loss: 4.3587\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0698 - loss: 4.0691\n",
            "Epoch 24: val_accuracy did not improve from 0.08176\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0700 - loss: 4.0705 - val_accuracy: 0.0692 - val_loss: 4.3431\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0837 - loss: 4.0731\n",
            "Epoch 25: val_accuracy improved from 0.08176 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0843 - loss: 4.0721 - val_accuracy: 0.0943 - val_loss: 4.2806\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1220 - loss: 3.9512\n",
            "Epoch 26: val_accuracy improved from 0.09434 to 0.11321, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.1219 - loss: 3.9518 - val_accuracy: 0.1132 - val_loss: 4.2376\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1179 - loss: 3.9260\n",
            "Epoch 27: val_accuracy did not improve from 0.11321\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.1173 - loss: 3.9272 - val_accuracy: 0.0818 - val_loss: 4.2300\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1346 - loss: 3.9005\n",
            "Epoch 28: val_accuracy did not improve from 0.11321\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.1342 - loss: 3.9004 - val_accuracy: 0.1006 - val_loss: 4.1674\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.1183 - loss: 3.8474\n",
            "Epoch 29: val_accuracy did not improve from 0.11321\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.1184 - loss: 3.8483 - val_accuracy: 0.0755 - val_loss: 4.1280\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1061 - loss: 3.8252\n",
            "Epoch 30: val_accuracy improved from 0.11321 to 0.12579, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.1065 - loss: 3.8250 - val_accuracy: 0.1258 - val_loss: 4.0374\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1411 - loss: 3.7252\n",
            "Epoch 31: val_accuracy improved from 0.12579 to 0.14465, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.1413 - loss: 3.7272 - val_accuracy: 0.1447 - val_loss: 3.9792\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1349 - loss: 3.7100\n",
            "Epoch 32: val_accuracy did not improve from 0.14465\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.1346 - loss: 3.7094 - val_accuracy: 0.1258 - val_loss: 3.9512\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1610 - loss: 3.7073\n",
            "Epoch 33: val_accuracy improved from 0.14465 to 0.15723, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.1608 - loss: 3.7061 - val_accuracy: 0.1572 - val_loss: 3.8852\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1548 - loss: 3.5773\n",
            "Epoch 34: val_accuracy improved from 0.15723 to 0.16981, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.1547 - loss: 3.5787 - val_accuracy: 0.1698 - val_loss: 3.7876\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1544 - loss: 3.5858\n",
            "Epoch 35: val_accuracy did not improve from 0.16981\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.1545 - loss: 3.5852 - val_accuracy: 0.1509 - val_loss: 3.7367\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1813 - loss: 3.5087\n",
            "Epoch 36: val_accuracy improved from 0.16981 to 0.17610, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.1815 - loss: 3.5071 - val_accuracy: 0.1761 - val_loss: 3.6608\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1556 - loss: 3.5524\n",
            "Epoch 37: val_accuracy improved from 0.17610 to 0.24528, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.1554 - loss: 3.5488 - val_accuracy: 0.2453 - val_loss: 3.5611\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2409 - loss: 3.3236\n",
            "Epoch 38: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.2395 - loss: 3.3252 - val_accuracy: 0.2138 - val_loss: 3.5637\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1781 - loss: 3.3831\n",
            "Epoch 39: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.1783 - loss: 3.3820 - val_accuracy: 0.1950 - val_loss: 3.4909\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2073 - loss: 3.3129\n",
            "Epoch 40: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.2069 - loss: 3.3124 - val_accuracy: 0.2201 - val_loss: 3.3584\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1865 - loss: 3.2486\n",
            "Epoch 41: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.1868 - loss: 3.2491 - val_accuracy: 0.2138 - val_loss: 3.3163\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2430 - loss: 3.1409\n",
            "Epoch 42: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.2421 - loss: 3.1416 - val_accuracy: 0.2075 - val_loss: 3.2595\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2470 - loss: 3.0898\n",
            "Epoch 43: val_accuracy did not improve from 0.24528\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.2472 - loss: 3.0876 - val_accuracy: 0.1572 - val_loss: 3.3848\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2696 - loss: 3.0147\n",
            "Epoch 44: val_accuracy improved from 0.24528 to 0.29560, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.2687 - loss: 3.0145 - val_accuracy: 0.2956 - val_loss: 3.0473\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2148 - loss: 3.0106\n",
            "Epoch 45: val_accuracy did not improve from 0.29560\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.2156 - loss: 3.0116 - val_accuracy: 0.2893 - val_loss: 3.0248\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2502 - loss: 2.9537\n",
            "Epoch 46: val_accuracy improved from 0.29560 to 0.31447, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.2501 - loss: 2.9543 - val_accuracy: 0.3145 - val_loss: 2.9654\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2571 - loss: 2.9525\n",
            "Epoch 47: val_accuracy did not improve from 0.31447\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.2578 - loss: 2.9485 - val_accuracy: 0.2075 - val_loss: 3.0972\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2921 - loss: 2.8129\n",
            "Epoch 48: val_accuracy improved from 0.31447 to 0.36478, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.2927 - loss: 2.8116 - val_accuracy: 0.3648 - val_loss: 2.7658\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2828 - loss: 2.8212\n",
            "Epoch 49: val_accuracy did not improve from 0.36478\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.2834 - loss: 2.8187 - val_accuracy: 0.2830 - val_loss: 2.8036\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3239 - loss: 2.6491\n",
            "Epoch 50: val_accuracy did not improve from 0.36478\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.3221 - loss: 2.6537 - val_accuracy: 0.3522 - val_loss: 2.8086\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3282 - loss: 2.6340\n",
            "Epoch 51: val_accuracy did not improve from 0.36478\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.3272 - loss: 2.6366 - val_accuracy: 0.2767 - val_loss: 2.8319\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.3428 - loss: 2.6236\n",
            "Epoch 52: val_accuracy did not improve from 0.36478\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3413 - loss: 2.6256 - val_accuracy: 0.3522 - val_loss: 2.5982\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2753 - loss: 2.6456\n",
            "Epoch 53: val_accuracy improved from 0.36478 to 0.40252, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.2760 - loss: 2.6462 - val_accuracy: 0.4025 - val_loss: 2.4747\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.3283 - loss: 2.6310\n",
            "Epoch 54: val_accuracy did not improve from 0.40252\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.3289 - loss: 2.6254 - val_accuracy: 0.3774 - val_loss: 2.4932\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3572 - loss: 2.4918\n",
            "Epoch 55: val_accuracy improved from 0.40252 to 0.49686, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.3569 - loss: 2.4910 - val_accuracy: 0.4969 - val_loss: 2.3445\n",
            "Train Accuracy: 0.7020, Test Accuracy: 0.5404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Different Optimizers"
      ],
      "metadata": {
        "id": "LsQuG-ARw8QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of optimizers to experiment with\n",
        "optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "\n",
        "# Loop through each optimizer and train the model\n",
        "for optimizer in optimizers:\n",
        "    print(f\"Training with optimizer: {optimizer}\")\n",
        "    training2(X_train_images, X_train_tabular, y_train, X_test_images, X_test_tabular, y_test, epochs=55, batch_size=32, learning_rate=0.0001, dropout=0.2, optimizer_name=optimizer, l2_factor=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OO8tnDzDGKAN",
        "outputId": "9a765a26-faba-4c9b-a12f-40a809949a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with optimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              "\n",
              " conv2d (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " batch_normalization        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_1            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " input_layer_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " max_pooling2d_2            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              "\n",
              " global_average_pooling2d   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dropout (\u001b[38;5;33mDropout\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " concatenate (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
              "                                                                    dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              "\n",
              " conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " batch_normalization        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_1            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " input_layer_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " max_pooling2d_2            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              "\n",
              " global_average_pooling2d   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
              "                                                                    dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659ms/step - accuracy: 0.0242 - loss: 4.6572\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.0240 - loss: 4.6564 - val_accuracy: 0.0063 - val_loss: 4.6276\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0194 - loss: 4.5201\n",
            "Epoch 2: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.0194 - loss: 4.5202 - val_accuracy: 0.0063 - val_loss: 4.6112\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0332 - loss: 4.4974\n",
            "Epoch 3: val_accuracy improved from 0.00629 to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.0330 - loss: 4.4969 - val_accuracy: 0.0126 - val_loss: 4.6024\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0441 - loss: 4.4403\n",
            "Epoch 4: val_accuracy improved from 0.01258 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0442 - loss: 4.4393 - val_accuracy: 0.0252 - val_loss: 4.5980\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0577 - loss: 4.3529\n",
            "Epoch 5: val_accuracy improved from 0.02516 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0576 - loss: 4.3524 - val_accuracy: 0.0440 - val_loss: 4.5921\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0747 - loss: 4.2525\n",
            "Epoch 6: val_accuracy improved from 0.04403 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.0748 - loss: 4.2530 - val_accuracy: 0.0503 - val_loss: 4.5880\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1059 - loss: 4.1726\n",
            "Epoch 7: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.1068 - loss: 4.1725 - val_accuracy: 0.0440 - val_loss: 4.5836\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1340 - loss: 4.1000\n",
            "Epoch 8: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.1350 - loss: 4.0982 - val_accuracy: 0.0377 - val_loss: 4.5677\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1763 - loss: 4.0166\n",
            "Epoch 9: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.1764 - loss: 4.0144 - val_accuracy: 0.0377 - val_loss: 4.5454\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1843 - loss: 3.8937\n",
            "Epoch 10: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.1854 - loss: 3.8913 - val_accuracy: 0.0189 - val_loss: 4.5296\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2167 - loss: 3.7387\n",
            "Epoch 11: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.2167 - loss: 3.7379 - val_accuracy: 0.0252 - val_loss: 4.4918\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.3093 - loss: 3.5596\n",
            "Epoch 12: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.3088 - loss: 3.5590 - val_accuracy: 0.0314 - val_loss: 4.4763\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3422 - loss: 3.4059\n",
            "Epoch 13: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.3411 - loss: 3.4042 - val_accuracy: 0.0252 - val_loss: 4.4358\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3530 - loss: 3.2310\n",
            "Epoch 14: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.3516 - loss: 3.2305 - val_accuracy: 0.0252 - val_loss: 4.4243\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3466 - loss: 3.0724\n",
            "Epoch 15: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.3477 - loss: 3.0695 - val_accuracy: 0.0314 - val_loss: 4.3139\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4298 - loss: 2.8463\n",
            "Epoch 16: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.4290 - loss: 2.8465 - val_accuracy: 0.0252 - val_loss: 4.3244\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4711 - loss: 2.6540\n",
            "Epoch 17: val_accuracy improved from 0.05031 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4708 - loss: 2.6538 - val_accuracy: 0.0566 - val_loss: 4.1468\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5296 - loss: 2.4803\n",
            "Epoch 18: val_accuracy improved from 0.05660 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.5289 - loss: 2.4793 - val_accuracy: 0.0629 - val_loss: 4.0498\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5396 - loss: 2.2445\n",
            "Epoch 19: val_accuracy improved from 0.06289 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5383 - loss: 2.2459 - val_accuracy: 0.0818 - val_loss: 3.9034\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5690 - loss: 2.1803\n",
            "Epoch 20: val_accuracy improved from 0.08176 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.5691 - loss: 2.1786 - val_accuracy: 0.0881 - val_loss: 3.7286\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6081 - loss: 2.0080\n",
            "Epoch 21: val_accuracy improved from 0.08805 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6079 - loss: 2.0065 - val_accuracy: 0.1069 - val_loss: 3.7060\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6433 - loss: 1.8432\n",
            "Epoch 22: val_accuracy improved from 0.10692 to 0.16352, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.6427 - loss: 1.8439 - val_accuracy: 0.1635 - val_loss: 3.3995\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6773 - loss: 1.6862\n",
            "Epoch 23: val_accuracy improved from 0.16352 to 0.21384, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.6770 - loss: 1.6858 - val_accuracy: 0.2138 - val_loss: 3.2813\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6544 - loss: 1.6448\n",
            "Epoch 24: val_accuracy did not improve from 0.21384\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.6553 - loss: 1.6421 - val_accuracy: 0.2138 - val_loss: 3.1049\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6965 - loss: 1.5128\n",
            "Epoch 25: val_accuracy improved from 0.21384 to 0.35849, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6971 - loss: 1.5108 - val_accuracy: 0.3585 - val_loss: 2.6549\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7032 - loss: 1.4081\n",
            "Epoch 26: val_accuracy did not improve from 0.35849\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7043 - loss: 1.4066 - val_accuracy: 0.3459 - val_loss: 2.6478\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7456 - loss: 1.2159\n",
            "Epoch 27: val_accuracy improved from 0.35849 to 0.45912, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.7445 - loss: 1.2184 - val_accuracy: 0.4591 - val_loss: 2.2618\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7708 - loss: 1.1855\n",
            "Epoch 28: val_accuracy improved from 0.45912 to 0.51572, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7706 - loss: 1.1856 - val_accuracy: 0.5157 - val_loss: 2.0686\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7978 - loss: 1.1080\n",
            "Epoch 29: val_accuracy improved from 0.51572 to 0.52201, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.7972 - loss: 1.1084 - val_accuracy: 0.5220 - val_loss: 1.9166\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8216 - loss: 0.9712\n",
            "Epoch 30: val_accuracy improved from 0.52201 to 0.69182, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.8207 - loss: 0.9737 - val_accuracy: 0.6918 - val_loss: 1.6323\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8155 - loss: 0.9993\n",
            "Epoch 31: val_accuracy did not improve from 0.69182\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8143 - loss: 1.0007 - val_accuracy: 0.6478 - val_loss: 1.5483\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7990 - loss: 0.9255\n",
            "Epoch 32: val_accuracy did not improve from 0.69182\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7990 - loss: 0.9259 - val_accuracy: 0.5660 - val_loss: 1.6847\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8120 - loss: 0.9513\n",
            "Epoch 33: val_accuracy improved from 0.69182 to 0.71069, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8121 - loss: 0.9489 - val_accuracy: 0.7107 - val_loss: 1.3683\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8414 - loss: 0.7905\n",
            "Epoch 34: val_accuracy improved from 0.71069 to 0.71698, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8410 - loss: 0.7910 - val_accuracy: 0.7170 - val_loss: 1.2682\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8647 - loss: 0.7510\n",
            "Epoch 35: val_accuracy improved from 0.71698 to 0.78616, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.8642 - loss: 0.7527 - val_accuracy: 0.7862 - val_loss: 1.1340\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8555 - loss: 0.6915\n",
            "Epoch 36: val_accuracy improved from 0.78616 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.8548 - loss: 0.6937 - val_accuracy: 0.8176 - val_loss: 1.0083\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8625 - loss: 0.7217\n",
            "Epoch 37: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.8621 - loss: 0.7219 - val_accuracy: 0.7799 - val_loss: 1.0050\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8811 - loss: 0.6753\n",
            "Epoch 38: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8809 - loss: 0.6746 - val_accuracy: 0.8176 - val_loss: 0.8917\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8866 - loss: 0.6113\n",
            "Epoch 39: val_accuracy improved from 0.81761 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8856 - loss: 0.6133 - val_accuracy: 0.8491 - val_loss: 0.8002\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8890 - loss: 0.5790\n",
            "Epoch 40: val_accuracy improved from 0.84906 to 0.85535, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8878 - loss: 0.5810 - val_accuracy: 0.8553 - val_loss: 0.7664\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8850 - loss: 0.5953\n",
            "Epoch 41: val_accuracy improved from 0.85535 to 0.88679, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8845 - loss: 0.5954 - val_accuracy: 0.8868 - val_loss: 0.6942\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8754 - loss: 0.5715\n",
            "Epoch 42: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.8759 - loss: 0.5713 - val_accuracy: 0.8491 - val_loss: 0.7143\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8930 - loss: 0.5047\n",
            "Epoch 43: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8925 - loss: 0.5063 - val_accuracy: 0.7987 - val_loss: 0.7426\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8828 - loss: 0.5084\n",
            "Epoch 44: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8827 - loss: 0.5089 - val_accuracy: 0.8868 - val_loss: 0.6438\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8739 - loss: 0.4997\n",
            "Epoch 45: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8751 - loss: 0.4987 - val_accuracy: 0.8679 - val_loss: 0.6871\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8974 - loss: 0.4853\n",
            "Epoch 46: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8979 - loss: 0.4841 - val_accuracy: 0.8491 - val_loss: 0.6147\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8953 - loss: 0.5041\n",
            "Epoch 47: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.8955 - loss: 0.5029 - val_accuracy: 0.8868 - val_loss: 0.5605\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9103 - loss: 0.4829\n",
            "Epoch 48: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9097 - loss: 0.4827 - val_accuracy: 0.8742 - val_loss: 0.5685\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9124 - loss: 0.4355\n",
            "Epoch 49: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9122 - loss: 0.4365 - val_accuracy: 0.8428 - val_loss: 0.5478\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9292 - loss: 0.3870\n",
            "Epoch 50: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9289 - loss: 0.3872 - val_accuracy: 0.8868 - val_loss: 0.4894\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9132 - loss: 0.3818\n",
            "Epoch 51: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9131 - loss: 0.3816 - val_accuracy: 0.8742 - val_loss: 0.5249\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9254 - loss: 0.3621\n",
            "Epoch 52: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9253 - loss: 0.3624 - val_accuracy: 0.8868 - val_loss: 0.5227\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9439 - loss: 0.3518\n",
            "Epoch 53: val_accuracy improved from 0.88679 to 0.89308, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9438 - loss: 0.3515 - val_accuracy: 0.8931 - val_loss: 0.4966\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9491 - loss: 0.3181\n",
            "Epoch 54: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.9481 - loss: 0.3196 - val_accuracy: 0.8868 - val_loss: 0.4516\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9360 - loss: 0.3094\n",
            "Epoch 55: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9358 - loss: 0.3108 - val_accuracy: 0.8868 - val_loss: 0.4386\n",
            "Train Accuracy: 0.9760, Test Accuracy: 0.9040\n",
            "Training with optimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization_3      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_3            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " batch_normalization_4      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_4            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " input_layer_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_5      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " max_pooling2d_5            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_5 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " concatenate_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_7 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_7 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dense_9 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization_3      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_3            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " batch_normalization_4      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_4            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " input_layer_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_5      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " max_pooling2d_5            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " concatenate_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.0055 - loss: 4.6942\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 471ms/step - accuracy: 0.0056 - loss: 4.6942 - val_accuracy: 0.0063 - val_loss: 4.6360\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0065 - loss: 4.6890\n",
            "Epoch 2: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.0064 - loss: 4.6886 - val_accuracy: 0.0063 - val_loss: 4.6352\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0125 - loss: 4.6743\n",
            "Epoch 3: val_accuracy improved from 0.00629 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.0124 - loss: 4.6740 - val_accuracy: 0.0189 - val_loss: 4.6368\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0027 - loss: 4.6853\n",
            "Epoch 4: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.0028 - loss: 4.6850 - val_accuracy: 0.0189 - val_loss: 4.6390\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0166 - loss: 4.6641\n",
            "Epoch 5: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0163 - loss: 4.6656 - val_accuracy: 0.0189 - val_loss: 4.6410\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0173 - loss: 4.6819\n",
            "Epoch 6: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0170 - loss: 4.6820 - val_accuracy: 0.0189 - val_loss: 4.6436\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0138 - loss: 4.6722\n",
            "Epoch 7: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.0135 - loss: 4.6721 - val_accuracy: 0.0189 - val_loss: 4.6460\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0016 - loss: 4.6860\n",
            "Epoch 8: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0017 - loss: 4.6854 - val_accuracy: 0.0189 - val_loss: 4.6482\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0157 - loss: 4.6699\n",
            "Epoch 9: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.0155 - loss: 4.6701 - val_accuracy: 0.0189 - val_loss: 4.6497\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0110 - loss: 4.6888\n",
            "Epoch 10: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.0110 - loss: 4.6885 - val_accuracy: 0.0189 - val_loss: 4.6504\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0105 - loss: 4.6616\n",
            "Epoch 11: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0105 - loss: 4.6619 - val_accuracy: 0.0189 - val_loss: 4.6506\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0032 - loss: 4.6884\n",
            "Epoch 12: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0034 - loss: 4.6875 - val_accuracy: 0.0252 - val_loss: 4.6503\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0025 - loss: 4.6871\n",
            "Epoch 13: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.0027 - loss: 4.6859 - val_accuracy: 0.0252 - val_loss: 4.6495\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0130 - loss: 4.6706\n",
            "Epoch 14: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0130 - loss: 4.6705 - val_accuracy: 0.0252 - val_loss: 4.6491\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0114 - loss: 4.6498\n",
            "Epoch 15: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.0113 - loss: 4.6502 - val_accuracy: 0.0252 - val_loss: 4.6487\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0018 - loss: 4.6897\n",
            "Epoch 16: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.0019 - loss: 4.6893 - val_accuracy: 0.0252 - val_loss: 4.6481\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0186 - loss: 4.6711\n",
            "Epoch 17: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0183 - loss: 4.6718 - val_accuracy: 0.0252 - val_loss: 4.6477\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0175 - loss: 4.6604\n",
            "Epoch 18: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.0173 - loss: 4.6604 - val_accuracy: 0.0252 - val_loss: 4.6469\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0021 - loss: 4.6834\n",
            "Epoch 19: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0020 - loss: 4.6828 - val_accuracy: 0.0252 - val_loss: 4.6457\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0126 - loss: 4.6314\n",
            "Epoch 20: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0126 - loss: 4.6325 - val_accuracy: 0.0252 - val_loss: 4.6441\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0083 - loss: 4.6742\n",
            "Epoch 21: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.0084 - loss: 4.6741 - val_accuracy: 0.0189 - val_loss: 4.6423\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0097 - loss: 4.6805\n",
            "Epoch 22: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.0097 - loss: 4.6792 - val_accuracy: 0.0189 - val_loss: 4.6402\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0102 - loss: 4.6735\n",
            "Epoch 23: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0103 - loss: 4.6731 - val_accuracy: 0.0189 - val_loss: 4.6382\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0087 - loss: 4.6764\n",
            "Epoch 24: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0086 - loss: 4.6758 - val_accuracy: 0.0189 - val_loss: 4.6365\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0094 - loss: 4.6599\n",
            "Epoch 25: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0093 - loss: 4.6597 - val_accuracy: 0.0189 - val_loss: 4.6355\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0194 - loss: 4.6439\n",
            "Epoch 26: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0192 - loss: 4.6446 - val_accuracy: 0.0189 - val_loss: 4.6364\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0140 - loss: 4.6452\n",
            "Epoch 27: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0137 - loss: 4.6456 - val_accuracy: 0.0189 - val_loss: 4.6384\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0070 - loss: 4.6853\n",
            "Epoch 28: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.0070 - loss: 4.6840 - val_accuracy: 0.0126 - val_loss: 4.6408\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0163 - loss: 4.6803\n",
            "Epoch 29: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0160 - loss: 4.6790 - val_accuracy: 0.0063 - val_loss: 4.6430\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0043 - loss: 4.6593\n",
            "Epoch 30: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0044 - loss: 4.6589 - val_accuracy: 0.0063 - val_loss: 4.6447\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0065 - loss: 4.6511\n",
            "Epoch 31: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0065 - loss: 4.6516 - val_accuracy: 0.0063 - val_loss: 4.6465\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0114 - loss: 4.6449\n",
            "Epoch 32: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0113 - loss: 4.6449 - val_accuracy: 0.0000e+00 - val_loss: 4.6476\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0091 - loss: 4.6386\n",
            "Epoch 33: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0092 - loss: 4.6383 - val_accuracy: 0.0000e+00 - val_loss: 4.6486\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0068 - loss: 4.6431\n",
            "Epoch 34: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.0070 - loss: 4.6438 - val_accuracy: 0.0000e+00 - val_loss: 4.6494\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0106 - loss: 4.6455\n",
            "Epoch 35: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.0107 - loss: 4.6453 - val_accuracy: 0.0000e+00 - val_loss: 4.6495\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0218 - loss: 4.6382\n",
            "Epoch 36: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.0214 - loss: 4.6384 - val_accuracy: 0.0000e+00 - val_loss: 4.6497\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0042 - loss: 4.6594\n",
            "Epoch 37: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.0043 - loss: 4.6594 - val_accuracy: 0.0000e+00 - val_loss: 4.6500\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0108 - loss: 4.6629\n",
            "Epoch 38: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.0107 - loss: 4.6621 - val_accuracy: 0.0000e+00 - val_loss: 4.6501\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0029 - loss: 4.6802\n",
            "Epoch 39: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.0029 - loss: 4.6793 - val_accuracy: 0.0000e+00 - val_loss: 4.6504\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0147 - loss: 4.6383\n",
            "Epoch 40: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.0145 - loss: 4.6385 - val_accuracy: 0.0000e+00 - val_loss: 4.6506\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.0044 - loss: 4.6446\n",
            "Epoch 41: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.0045 - loss: 4.6448 - val_accuracy: 0.0000e+00 - val_loss: 4.6510\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0082 - loss: 4.6266\n",
            "Epoch 42: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0084 - loss: 4.6272 - val_accuracy: 0.0000e+00 - val_loss: 4.6516\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0092 - loss: 4.6343\n",
            "Epoch 43: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.0090 - loss: 4.6349 - val_accuracy: 0.0000e+00 - val_loss: 4.6522\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0060 - loss: 4.6791\n",
            "Epoch 44: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0061 - loss: 4.6781 - val_accuracy: 0.0000e+00 - val_loss: 4.6526\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0105 - loss: 4.6493\n",
            "Epoch 45: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.0106 - loss: 4.6491 - val_accuracy: 0.0000e+00 - val_loss: 4.6527\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0143 - loss: 4.6488\n",
            "Epoch 46: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.0141 - loss: 4.6488 - val_accuracy: 0.0000e+00 - val_loss: 4.6526\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0054 - loss: 4.6445\n",
            "Epoch 47: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.0055 - loss: 4.6442 - val_accuracy: 0.0000e+00 - val_loss: 4.6525\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0084 - loss: 4.6677\n",
            "Epoch 48: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.0083 - loss: 4.6664 - val_accuracy: 0.0000e+00 - val_loss: 4.6525\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0094 - loss: 4.6301\n",
            "Epoch 49: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0094 - loss: 4.6307 - val_accuracy: 0.0000e+00 - val_loss: 4.6522\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0067 - loss: 4.6215\n",
            "Epoch 50: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.0067 - loss: 4.6222 - val_accuracy: 0.0000e+00 - val_loss: 4.6520\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0041 - loss: 4.6414\n",
            "Epoch 51: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0043 - loss: 4.6412 - val_accuracy: 0.0000e+00 - val_loss: 4.6522\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0114 - loss: 4.6334\n",
            "Epoch 52: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.0114 - loss: 4.6335 - val_accuracy: 0.0000e+00 - val_loss: 4.6518\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0027 - loss: 4.6247\n",
            "Epoch 53: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.0029 - loss: 4.6254 - val_accuracy: 0.0000e+00 - val_loss: 4.6512\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0092 - loss: 4.6525\n",
            "Epoch 54: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0092 - loss: 4.6520 - val_accuracy: 0.0000e+00 - val_loss: 4.6510\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0108 - loss: 4.6303\n",
            "Epoch 55: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.0107 - loss: 4.6302 - val_accuracy: 0.0000e+00 - val_loss: 4.6506\n",
            "Train Accuracy: 0.0063, Test Accuracy: 0.0152\n",
            "Training with optimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_4              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization_6      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_6            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " batch_normalization_7      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_7            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " input_layer_5              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_8      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_10 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " max_pooling2d_8            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_9 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_11 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " dropout_8 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_10 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_12 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_11 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_13 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_14 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_4              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization_6      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_6            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " batch_normalization_7      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_7            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " input_layer_5              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_8      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " max_pooling2d_8            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.0128 - loss: 4.6226\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 406ms/step - accuracy: 0.0129 - loss: 4.6215 - val_accuracy: 0.0126 - val_loss: 4.5711\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0110 - loss: 4.5265\n",
            "Epoch 2: val_accuracy improved from 0.01258 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.0110 - loss: 4.5260 - val_accuracy: 0.0252 - val_loss: 4.5607\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0293 - loss: 4.4792\n",
            "Epoch 3: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.0297 - loss: 4.4783 - val_accuracy: 0.0252 - val_loss: 4.5540\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0846 - loss: 4.3992\n",
            "Epoch 4: val_accuracy did not improve from 0.02516\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0842 - loss: 4.3988 - val_accuracy: 0.0252 - val_loss: 4.5461\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1101 - loss: 4.3003\n",
            "Epoch 5: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.1088 - loss: 4.3016 - val_accuracy: 0.0314 - val_loss: 4.5386\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1141 - loss: 4.2490\n",
            "Epoch 6: val_accuracy improved from 0.03145 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.1140 - loss: 4.2489 - val_accuracy: 0.0377 - val_loss: 4.5277\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1197 - loss: 4.1603\n",
            "Epoch 7: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.1198 - loss: 4.1604 - val_accuracy: 0.0189 - val_loss: 4.5204\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.1540 - loss: 4.0716\n",
            "Epoch 8: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.1535 - loss: 4.0723 - val_accuracy: 0.0377 - val_loss: 4.4998\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1992 - loss: 3.9857\n",
            "Epoch 9: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.1983 - loss: 3.9865 - val_accuracy: 0.0377 - val_loss: 4.4847\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1988 - loss: 3.9296\n",
            "Epoch 10: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.1980 - loss: 3.9290 - val_accuracy: 0.0314 - val_loss: 4.4625\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1974 - loss: 3.8340\n",
            "Epoch 11: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.1977 - loss: 3.8334 - val_accuracy: 0.0252 - val_loss: 4.4373\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2550 - loss: 3.6959\n",
            "Epoch 12: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.2549 - loss: 3.6963 - val_accuracy: 0.0189 - val_loss: 4.3965\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2197 - loss: 3.6903\n",
            "Epoch 13: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.2218 - loss: 3.6853 - val_accuracy: 0.0189 - val_loss: 4.3663\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2771 - loss: 3.4818\n",
            "Epoch 14: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.2772 - loss: 3.4819 - val_accuracy: 0.0189 - val_loss: 4.3328\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2582 - loss: 3.4169\n",
            "Epoch 15: val_accuracy did not improve from 0.03774\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.2590 - loss: 3.4158 - val_accuracy: 0.0377 - val_loss: 4.2590\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3374 - loss: 3.2297\n",
            "Epoch 16: val_accuracy improved from 0.03774 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3380 - loss: 3.2307 - val_accuracy: 0.0629 - val_loss: 4.1981\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3567 - loss: 3.0886\n",
            "Epoch 17: val_accuracy improved from 0.06289 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3567 - loss: 3.0889 - val_accuracy: 0.0755 - val_loss: 4.1449\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3884 - loss: 2.9579\n",
            "Epoch 18: val_accuracy improved from 0.07547 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.3882 - loss: 2.9590 - val_accuracy: 0.0943 - val_loss: 4.0338\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4177 - loss: 2.8556\n",
            "Epoch 19: val_accuracy improved from 0.09434 to 0.10063, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.4179 - loss: 2.8542 - val_accuracy: 0.1006 - val_loss: 3.9582\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.4326 - loss: 2.7189\n",
            "Epoch 20: val_accuracy improved from 0.10063 to 0.15094, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.4321 - loss: 2.7199 - val_accuracy: 0.1509 - val_loss: 3.8031\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.4639 - loss: 2.5988\n",
            "Epoch 21: val_accuracy improved from 0.15094 to 0.15723, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.4645 - loss: 2.5987 - val_accuracy: 0.1572 - val_loss: 3.6732\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4688 - loss: 2.5098\n",
            "Epoch 22: val_accuracy improved from 0.15723 to 0.19497, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4699 - loss: 2.5077 - val_accuracy: 0.1950 - val_loss: 3.5916\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4989 - loss: 2.3751\n",
            "Epoch 23: val_accuracy improved from 0.19497 to 0.20126, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.4999 - loss: 2.3752 - val_accuracy: 0.2013 - val_loss: 3.4700\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5245 - loss: 2.3614\n",
            "Epoch 24: val_accuracy improved from 0.20126 to 0.25786, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.5249 - loss: 2.3568 - val_accuracy: 0.2579 - val_loss: 3.1959\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5672 - loss: 2.1253\n",
            "Epoch 25: val_accuracy improved from 0.25786 to 0.27044, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5669 - loss: 2.1258 - val_accuracy: 0.2704 - val_loss: 3.1292\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5611 - loss: 2.1364\n",
            "Epoch 26: val_accuracy did not improve from 0.27044\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.5613 - loss: 2.1337 - val_accuracy: 0.2390 - val_loss: 3.0826\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6248 - loss: 1.9414\n",
            "Epoch 27: val_accuracy improved from 0.27044 to 0.36478, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.6250 - loss: 1.9404 - val_accuracy: 0.3648 - val_loss: 2.7887\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6574 - loss: 1.8305\n",
            "Epoch 28: val_accuracy improved from 0.36478 to 0.46541, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6562 - loss: 1.8312 - val_accuracy: 0.4654 - val_loss: 2.5737\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6713 - loss: 1.7175\n",
            "Epoch 29: val_accuracy improved from 0.46541 to 0.47170, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6697 - loss: 1.7199 - val_accuracy: 0.4717 - val_loss: 2.4531\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6758 - loss: 1.6026\n",
            "Epoch 30: val_accuracy improved from 0.47170 to 0.50314, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.6743 - loss: 1.6061 - val_accuracy: 0.5031 - val_loss: 2.3072\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6628 - loss: 1.6059\n",
            "Epoch 31: val_accuracy improved from 0.50314 to 0.53459, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6648 - loss: 1.6034 - val_accuracy: 0.5346 - val_loss: 2.1434\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7049 - loss: 1.5466\n",
            "Epoch 32: val_accuracy improved from 0.53459 to 0.61635, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.7051 - loss: 1.5451 - val_accuracy: 0.6164 - val_loss: 1.9230\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6835 - loss: 1.4733\n",
            "Epoch 33: val_accuracy improved from 0.61635 to 0.65409, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.6846 - loss: 1.4722 - val_accuracy: 0.6541 - val_loss: 1.8387\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7329 - loss: 1.3000\n",
            "Epoch 34: val_accuracy did not improve from 0.65409\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7323 - loss: 1.3019 - val_accuracy: 0.6541 - val_loss: 1.7694\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7402 - loss: 1.2475\n",
            "Epoch 35: val_accuracy improved from 0.65409 to 0.68553, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7393 - loss: 1.2501 - val_accuracy: 0.6855 - val_loss: 1.6561\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7665 - loss: 1.1710\n",
            "Epoch 36: val_accuracy improved from 0.68553 to 0.74843, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7660 - loss: 1.1722 - val_accuracy: 0.7484 - val_loss: 1.4347\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7471 - loss: 1.1712\n",
            "Epoch 37: val_accuracy did not improve from 0.74843\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7468 - loss: 1.1712 - val_accuracy: 0.7421 - val_loss: 1.3870\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7961 - loss: 1.0205\n",
            "Epoch 38: val_accuracy did not improve from 0.74843\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.7948 - loss: 1.0237 - val_accuracy: 0.7233 - val_loss: 1.3541\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8114 - loss: 1.0124\n",
            "Epoch 39: val_accuracy improved from 0.74843 to 0.81132, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.8095 - loss: 1.0150 - val_accuracy: 0.8113 - val_loss: 1.1496\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8090 - loss: 0.9991\n",
            "Epoch 40: val_accuracy did not improve from 0.81132\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8090 - loss: 0.9984 - val_accuracy: 0.7547 - val_loss: 1.1913\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7971 - loss: 0.9771\n",
            "Epoch 41: val_accuracy did not improve from 0.81132\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7971 - loss: 0.9768 - val_accuracy: 0.6730 - val_loss: 1.3675\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8383 - loss: 0.9227\n",
            "Epoch 42: val_accuracy improved from 0.81132 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8374 - loss: 0.9222 - val_accuracy: 0.8491 - val_loss: 1.0099\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7983 - loss: 0.8935\n",
            "Epoch 43: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.7995 - loss: 0.8917 - val_accuracy: 0.8050 - val_loss: 0.9968\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8417 - loss: 0.7931\n",
            "Epoch 44: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.8420 - loss: 0.7946 - val_accuracy: 0.7799 - val_loss: 1.0495\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8568 - loss: 0.7538\n",
            "Epoch 45: val_accuracy improved from 0.84906 to 0.88679, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.8561 - loss: 0.7553 - val_accuracy: 0.8868 - val_loss: 0.8328\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8610 - loss: 0.7357\n",
            "Epoch 46: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8607 - loss: 0.7373 - val_accuracy: 0.8239 - val_loss: 0.8909\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8329 - loss: 0.7427\n",
            "Epoch 47: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8325 - loss: 0.7440 - val_accuracy: 0.8742 - val_loss: 0.7574\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8906 - loss: 0.6457\n",
            "Epoch 48: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8895 - loss: 0.6475 - val_accuracy: 0.8868 - val_loss: 0.7257\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8453 - loss: 0.7207\n",
            "Epoch 49: val_accuracy improved from 0.88679 to 0.91195, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.8449 - loss: 0.7208 - val_accuracy: 0.9119 - val_loss: 0.6437\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8787 - loss: 0.6361\n",
            "Epoch 50: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8773 - loss: 0.6384 - val_accuracy: 0.9057 - val_loss: 0.6853\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8532 - loss: 0.6381\n",
            "Epoch 51: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8534 - loss: 0.6381 - val_accuracy: 0.9057 - val_loss: 0.6244\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8781 - loss: 0.5778\n",
            "Epoch 52: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8777 - loss: 0.5790 - val_accuracy: 0.9057 - val_loss: 0.6278\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8688 - loss: 0.5825\n",
            "Epoch 53: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.8687 - loss: 0.5821 - val_accuracy: 0.8931 - val_loss: 0.6142\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8772 - loss: 0.5391\n",
            "Epoch 54: val_accuracy did not improve from 0.91195\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8772 - loss: 0.5398 - val_accuracy: 0.9119 - val_loss: 0.5575\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8849 - loss: 0.5254\n",
            "Epoch 55: val_accuracy improved from 0.91195 to 0.93711, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8854 - loss: 0.5256 - val_accuracy: 0.9371 - val_loss: 0.5261\n",
            "Train Accuracy: 0.9836, Test Accuracy: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try different L2 Regularization Factors"
      ],
      "metadata": {
        "id": "p8cYpK7xxBjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List of L2 regularization factors to experiment with\n",
        "l2_factors = [0.001, 0.01, 0.0001]\n",
        "\n",
        "# Loop through each L2 factor and train the model\n",
        "for l2_factor in l2_factors:\n",
        "    print(f\"Training with L2 regularization factor: {l2_factor}\")\n",
        "\n",
        "    training2(\n",
        "        X_train_images, X_train_tabular, y_train,\n",
        "        X_test_images, X_test_tabular, y_test,\n",
        "        epochs=55, batch_size=32, learning_rate=0.0001,\n",
        "        dropout=0.2, optimizer_name='adam', l2_factor=l2_factor\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1uQtbg6pGqmv",
        "outputId": "e41ebe2a-510f-4a44-f54c-aae56eec5f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with L2 regularization factor: 0.001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_6              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization_9      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_9            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " batch_normalization_10     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_10           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_7              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_11     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_15 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " max_pooling2d_11           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_13 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_16 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_12 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_14 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_17 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_15 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_18 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_19 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_6              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization_9      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_9            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " batch_normalization_10     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_10           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_7              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_11     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " max_pooling2d_11           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.0115 - loss: 4.6616\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 492ms/step - accuracy: 0.0114 - loss: 4.6601 - val_accuracy: 0.0189 - val_loss: 4.5925\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.0254 - loss: 4.5339\n",
            "Epoch 2: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.0257 - loss: 4.5335 - val_accuracy: 0.0126 - val_loss: 4.5821\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0331 - loss: 4.4909\n",
            "Epoch 3: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.0339 - loss: 4.4899 - val_accuracy: 0.0063 - val_loss: 4.5739\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0695 - loss: 4.3960\n",
            "Epoch 4: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0694 - loss: 4.3959 - val_accuracy: 0.0189 - val_loss: 4.5675\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0782 - loss: 4.3238\n",
            "Epoch 5: val_accuracy improved from 0.01887 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0778 - loss: 4.3235 - val_accuracy: 0.0252 - val_loss: 4.5618\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0876 - loss: 4.2484\n",
            "Epoch 6: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0877 - loss: 4.2477 - val_accuracy: 0.0314 - val_loss: 4.5516\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0973 - loss: 4.1447\n",
            "Epoch 7: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.0976 - loss: 4.1445 - val_accuracy: 0.0314 - val_loss: 4.5353\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.1423 - loss: 4.0557\n",
            "Epoch 8: val_accuracy improved from 0.03145 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.1416 - loss: 4.0552 - val_accuracy: 0.0377 - val_loss: 4.5181\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1611 - loss: 3.9250\n",
            "Epoch 9: val_accuracy improved from 0.03774 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.1614 - loss: 3.9240 - val_accuracy: 0.0440 - val_loss: 4.4977\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1849 - loss: 3.8144\n",
            "Epoch 10: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.1853 - loss: 3.8135 - val_accuracy: 0.0377 - val_loss: 4.4719\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2448 - loss: 3.6386\n",
            "Epoch 11: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.2453 - loss: 3.6383 - val_accuracy: 0.0377 - val_loss: 4.4412\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2576 - loss: 3.4386\n",
            "Epoch 12: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.2588 - loss: 3.4388 - val_accuracy: 0.0377 - val_loss: 4.4085\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3276 - loss: 3.2744\n",
            "Epoch 13: val_accuracy improved from 0.04403 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.3263 - loss: 3.2753 - val_accuracy: 0.0566 - val_loss: 4.3337\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3222 - loss: 3.1708\n",
            "Epoch 14: val_accuracy did not improve from 0.05660\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.3238 - loss: 3.1678 - val_accuracy: 0.0440 - val_loss: 4.2936\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3866 - loss: 2.9384\n",
            "Epoch 15: val_accuracy improved from 0.05660 to 0.06918, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.3857 - loss: 2.9379 - val_accuracy: 0.0692 - val_loss: 4.1973\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4012 - loss: 2.7792\n",
            "Epoch 16: val_accuracy improved from 0.06918 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.4033 - loss: 2.7763 - val_accuracy: 0.0818 - val_loss: 4.1374\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4774 - loss: 2.6204\n",
            "Epoch 17: val_accuracy improved from 0.08176 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.4767 - loss: 2.6176 - val_accuracy: 0.0943 - val_loss: 3.9776\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5321 - loss: 2.3774\n",
            "Epoch 18: val_accuracy improved from 0.09434 to 0.11950, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.5319 - loss: 2.3761 - val_accuracy: 0.1195 - val_loss: 3.8000\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5175 - loss: 2.2574\n",
            "Epoch 19: val_accuracy did not improve from 0.11950\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5169 - loss: 2.2577 - val_accuracy: 0.1195 - val_loss: 3.6776\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5544 - loss: 2.0811\n",
            "Epoch 20: val_accuracy improved from 0.11950 to 0.15723, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.5546 - loss: 2.0810 - val_accuracy: 0.1572 - val_loss: 3.5078\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6204 - loss: 1.9364\n",
            "Epoch 21: val_accuracy improved from 0.15723 to 0.20126, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6200 - loss: 1.9369 - val_accuracy: 0.2013 - val_loss: 3.2884\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6314 - loss: 1.8141\n",
            "Epoch 22: val_accuracy improved from 0.20126 to 0.25157, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6315 - loss: 1.8134 - val_accuracy: 0.2516 - val_loss: 2.9845\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6246 - loss: 1.6876\n",
            "Epoch 23: val_accuracy improved from 0.25157 to 0.27673, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6257 - loss: 1.6849 - val_accuracy: 0.2767 - val_loss: 2.8375\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6955 - loss: 1.5247\n",
            "Epoch 24: val_accuracy improved from 0.27673 to 0.35220, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.6937 - loss: 1.5288 - val_accuracy: 0.3522 - val_loss: 2.5319\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7160 - loss: 1.4078\n",
            "Epoch 25: val_accuracy did not improve from 0.35220\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.7157 - loss: 1.4091 - val_accuracy: 0.3145 - val_loss: 2.5217\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6915 - loss: 1.4117\n",
            "Epoch 26: val_accuracy improved from 0.35220 to 0.50314, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.6919 - loss: 1.4111 - val_accuracy: 0.5031 - val_loss: 2.1173\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7492 - loss: 1.2422\n",
            "Epoch 27: val_accuracy improved from 0.50314 to 0.51572, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7482 - loss: 1.2446 - val_accuracy: 0.5157 - val_loss: 2.0360\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7884 - loss: 1.1851\n",
            "Epoch 28: val_accuracy improved from 0.51572 to 0.52201, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7878 - loss: 1.1862 - val_accuracy: 0.5220 - val_loss: 1.9162\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7550 - loss: 1.0826\n",
            "Epoch 29: val_accuracy improved from 0.52201 to 0.55975, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7555 - loss: 1.0839 - val_accuracy: 0.5597 - val_loss: 1.7324\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7606 - loss: 1.0368\n",
            "Epoch 30: val_accuracy improved from 0.55975 to 0.64151, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7611 - loss: 1.0372 - val_accuracy: 0.6415 - val_loss: 1.5628\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8247 - loss: 0.9741\n",
            "Epoch 31: val_accuracy improved from 0.64151 to 0.70440, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.8236 - loss: 0.9758 - val_accuracy: 0.7044 - val_loss: 1.4060\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8100 - loss: 0.9584\n",
            "Epoch 32: val_accuracy improved from 0.70440 to 0.71698, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8102 - loss: 0.9574 - val_accuracy: 0.7170 - val_loss: 1.3288\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7734 - loss: 0.9627\n",
            "Epoch 33: val_accuracy improved from 0.71698 to 0.77358, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7742 - loss: 0.9614 - val_accuracy: 0.7736 - val_loss: 1.2048\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8071 - loss: 0.8570\n",
            "Epoch 34: val_accuracy improved from 0.77358 to 0.77987, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8067 - loss: 0.8578 - val_accuracy: 0.7799 - val_loss: 1.0562\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8311 - loss: 0.8194\n",
            "Epoch 35: val_accuracy improved from 0.77987 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.8308 - loss: 0.8188 - val_accuracy: 0.8491 - val_loss: 0.9569\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8507 - loss: 0.7840\n",
            "Epoch 36: val_accuracy improved from 0.84906 to 0.86164, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8508 - loss: 0.7816 - val_accuracy: 0.8616 - val_loss: 0.8969\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8893 - loss: 0.6816\n",
            "Epoch 37: val_accuracy did not improve from 0.86164\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.8883 - loss: 0.6819 - val_accuracy: 0.8050 - val_loss: 0.8837\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8558 - loss: 0.6527\n",
            "Epoch 38: val_accuracy improved from 0.86164 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8555 - loss: 0.6558 - val_accuracy: 0.8994 - val_loss: 0.7492\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8802 - loss: 0.6232\n",
            "Epoch 39: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8796 - loss: 0.6241 - val_accuracy: 0.8679 - val_loss: 0.7395\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8841 - loss: 0.5799\n",
            "Epoch 40: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.8840 - loss: 0.5811 - val_accuracy: 0.8742 - val_loss: 0.6953\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8888 - loss: 0.5958\n",
            "Epoch 41: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8884 - loss: 0.5969 - val_accuracy: 0.8868 - val_loss: 0.6582\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8883 - loss: 0.5757\n",
            "Epoch 42: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8883 - loss: 0.5757 - val_accuracy: 0.8302 - val_loss: 0.7136\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8774 - loss: 0.5567\n",
            "Epoch 43: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8769 - loss: 0.5573 - val_accuracy: 0.8868 - val_loss: 0.6267\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8686 - loss: 0.5537\n",
            "Epoch 44: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.8686 - loss: 0.5536 - val_accuracy: 0.8931 - val_loss: 0.5997\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8595 - loss: 0.5598\n",
            "Epoch 45: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8611 - loss: 0.5573 - val_accuracy: 0.8679 - val_loss: 0.6052\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8807 - loss: 0.5033\n",
            "Epoch 46: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8800 - loss: 0.5054 - val_accuracy: 0.8868 - val_loss: 0.5429\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9070 - loss: 0.4908\n",
            "Epoch 47: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9060 - loss: 0.4907 - val_accuracy: 0.8805 - val_loss: 0.5351\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9279 - loss: 0.4021\n",
            "Epoch 48: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9277 - loss: 0.4027 - val_accuracy: 0.8994 - val_loss: 0.5612\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9384 - loss: 0.4305\n",
            "Epoch 49: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9380 - loss: 0.4310 - val_accuracy: 0.8994 - val_loss: 0.5246\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9114 - loss: 0.4117\n",
            "Epoch 50: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9114 - loss: 0.4116 - val_accuracy: 0.8868 - val_loss: 0.5151\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9221 - loss: 0.4079\n",
            "Epoch 51: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9216 - loss: 0.4076 - val_accuracy: 0.8994 - val_loss: 0.4549\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9213 - loss: 0.3860\n",
            "Epoch 52: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9206 - loss: 0.3875 - val_accuracy: 0.8805 - val_loss: 0.5281\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9254 - loss: 0.3587\n",
            "Epoch 53: val_accuracy improved from 0.89937 to 0.91824, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9249 - loss: 0.3604 - val_accuracy: 0.9182 - val_loss: 0.4477\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9508 - loss: 0.3269\n",
            "Epoch 54: val_accuracy did not improve from 0.91824\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9499 - loss: 0.3284 - val_accuracy: 0.8931 - val_loss: 0.4382\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9240 - loss: 0.3494\n",
            "Epoch 55: val_accuracy did not improve from 0.91824\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9239 - loss: 0.3499 - val_accuracy: 0.9119 - val_loss: 0.4036\n",
            "Train Accuracy: 0.9811, Test Accuracy: 0.8939\n",
            "Training with L2 regularization factor: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_8              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization_12     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_12           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_13     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_13           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_9              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_14     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_20 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " max_pooling2d_14           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_17 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_21 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_16 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_18 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_4              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_22 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_19 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_23 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_24 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_8              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization_12     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_12           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_13     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_13           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_9              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_14     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " max_pooling2d_14           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_4              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.0099 - loss: 4.6466\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 561ms/step - accuracy: 0.0102 - loss: 4.6457 - val_accuracy: 0.0126 - val_loss: 4.5890\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0228 - loss: 4.5589\n",
            "Epoch 2: val_accuracy did not improve from 0.01258\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.0232 - loss: 4.5579 - val_accuracy: 0.0126 - val_loss: 4.5741\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0256 - loss: 4.4819\n",
            "Epoch 3: val_accuracy improved from 0.01258 to 0.02516, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.0258 - loss: 4.4810 - val_accuracy: 0.0252 - val_loss: 4.5651\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0453 - loss: 4.4202\n",
            "Epoch 4: val_accuracy improved from 0.02516 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.0456 - loss: 4.4192 - val_accuracy: 0.0314 - val_loss: 4.5570\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0639 - loss: 4.3283\n",
            "Epoch 5: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0640 - loss: 4.3279 - val_accuracy: 0.0314 - val_loss: 4.5439\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0654 - loss: 4.2706\n",
            "Epoch 6: val_accuracy improved from 0.03145 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.0655 - loss: 4.2692 - val_accuracy: 0.0440 - val_loss: 4.5311\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1225 - loss: 4.1531\n",
            "Epoch 7: val_accuracy improved from 0.04403 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.1216 - loss: 4.1532 - val_accuracy: 0.0503 - val_loss: 4.5188\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1373 - loss: 4.0523\n",
            "Epoch 8: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.1372 - loss: 4.0516 - val_accuracy: 0.0503 - val_loss: 4.4997\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1653 - loss: 3.9414\n",
            "Epoch 9: val_accuracy improved from 0.05031 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.1656 - loss: 3.9399 - val_accuracy: 0.0755 - val_loss: 4.4782\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.1972 - loss: 3.7708\n",
            "Epoch 10: val_accuracy improved from 0.07547 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.1979 - loss: 3.7707 - val_accuracy: 0.0943 - val_loss: 4.4587\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2316 - loss: 3.6251\n",
            "Epoch 11: val_accuracy did not improve from 0.09434\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.2318 - loss: 3.6256 - val_accuracy: 0.0943 - val_loss: 4.4345\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2973 - loss: 3.4658\n",
            "Epoch 12: val_accuracy did not improve from 0.09434\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.2977 - loss: 3.4651 - val_accuracy: 0.0943 - val_loss: 4.4157\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3271 - loss: 3.3000\n",
            "Epoch 13: val_accuracy did not improve from 0.09434\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.3276 - loss: 3.2985 - val_accuracy: 0.0881 - val_loss: 4.3880\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3508 - loss: 3.1267\n",
            "Epoch 14: val_accuracy improved from 0.09434 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.3508 - loss: 3.1251 - val_accuracy: 0.1069 - val_loss: 4.3544\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3715 - loss: 2.9701\n",
            "Epoch 15: val_accuracy improved from 0.10692 to 0.13208, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.3731 - loss: 2.9671 - val_accuracy: 0.1321 - val_loss: 4.3110\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.4987 - loss: 2.6395\n",
            "Epoch 16: val_accuracy improved from 0.13208 to 0.13836, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.4970 - loss: 2.6421 - val_accuracy: 0.1384 - val_loss: 4.2310\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.4214 - loss: 2.5685\n",
            "Epoch 17: val_accuracy did not improve from 0.13836\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.4229 - loss: 2.5673 - val_accuracy: 0.1132 - val_loss: 4.2582\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5004 - loss: 2.4151\n",
            "Epoch 18: val_accuracy did not improve from 0.13836\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.5007 - loss: 2.4126 - val_accuracy: 0.1321 - val_loss: 4.1652\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5484 - loss: 2.1432\n",
            "Epoch 19: val_accuracy improved from 0.13836 to 0.16981, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5469 - loss: 2.1462 - val_accuracy: 0.1698 - val_loss: 3.9212\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6038 - loss: 2.0836\n",
            "Epoch 20: val_accuracy did not improve from 0.16981\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.6036 - loss: 2.0814 - val_accuracy: 0.1509 - val_loss: 3.9175\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5948 - loss: 1.9580\n",
            "Epoch 21: val_accuracy improved from 0.16981 to 0.21384, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.5955 - loss: 1.9554 - val_accuracy: 0.2138 - val_loss: 3.6337\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6255 - loss: 1.6963\n",
            "Epoch 22: val_accuracy improved from 0.21384 to 0.23899, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.6252 - loss: 1.6984 - val_accuracy: 0.2390 - val_loss: 3.4121\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6433 - loss: 1.6556\n",
            "Epoch 23: val_accuracy did not improve from 0.23899\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.6448 - loss: 1.6531 - val_accuracy: 0.1950 - val_loss: 3.4081\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6783 - loss: 1.5206\n",
            "Epoch 24: val_accuracy improved from 0.23899 to 0.27673, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6785 - loss: 1.5196 - val_accuracy: 0.2767 - val_loss: 3.0923\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7187 - loss: 1.3752\n",
            "Epoch 25: val_accuracy improved from 0.27673 to 0.30189, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.7180 - loss: 1.3760 - val_accuracy: 0.3019 - val_loss: 2.9309\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7562 - loss: 1.3090\n",
            "Epoch 26: val_accuracy improved from 0.30189 to 0.32704, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.7550 - loss: 1.3102 - val_accuracy: 0.3270 - val_loss: 2.6856\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7722 - loss: 1.2175\n",
            "Epoch 27: val_accuracy improved from 0.32704 to 0.37107, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.7709 - loss: 1.2178 - val_accuracy: 0.3711 - val_loss: 2.4959\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7845 - loss: 1.1480\n",
            "Epoch 28: val_accuracy improved from 0.37107 to 0.43396, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.7841 - loss: 1.1505 - val_accuracy: 0.4340 - val_loss: 2.1000\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7474 - loss: 1.1643\n",
            "Epoch 29: val_accuracy did not improve from 0.43396\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.7481 - loss: 1.1632 - val_accuracy: 0.4340 - val_loss: 2.0538\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7842 - loss: 1.0396\n",
            "Epoch 30: val_accuracy improved from 0.43396 to 0.51572, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.7841 - loss: 1.0401 - val_accuracy: 0.5157 - val_loss: 1.8480\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8170 - loss: 0.9949\n",
            "Epoch 31: val_accuracy improved from 0.51572 to 0.59119, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.8160 - loss: 0.9952 - val_accuracy: 0.5912 - val_loss: 1.6578\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7767 - loss: 0.9943\n",
            "Epoch 32: val_accuracy improved from 0.59119 to 0.65409, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.7763 - loss: 0.9947 - val_accuracy: 0.6541 - val_loss: 1.4507\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8246 - loss: 0.8615\n",
            "Epoch 33: val_accuracy improved from 0.65409 to 0.72327, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.8226 - loss: 0.8641 - val_accuracy: 0.7233 - val_loss: 1.2963\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8185 - loss: 0.8564\n",
            "Epoch 34: val_accuracy did not improve from 0.72327\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8191 - loss: 0.8545 - val_accuracy: 0.6352 - val_loss: 1.3408\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8376 - loss: 0.7881\n",
            "Epoch 35: val_accuracy did not improve from 0.72327\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8375 - loss: 0.7888 - val_accuracy: 0.7044 - val_loss: 1.1699\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8348 - loss: 0.7949\n",
            "Epoch 36: val_accuracy improved from 0.72327 to 0.80503, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8354 - loss: 0.7931 - val_accuracy: 0.8050 - val_loss: 0.9508\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8908 - loss: 0.6532\n",
            "Epoch 37: val_accuracy did not improve from 0.80503\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.8895 - loss: 0.6552 - val_accuracy: 0.8050 - val_loss: 0.8843\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8544 - loss: 0.6977\n",
            "Epoch 38: val_accuracy improved from 0.80503 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.8542 - loss: 0.6976 - val_accuracy: 0.8176 - val_loss: 0.8889\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8453 - loss: 0.7253\n",
            "Epoch 39: val_accuracy improved from 0.81761 to 0.82390, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.8453 - loss: 0.7248 - val_accuracy: 0.8239 - val_loss: 0.8305\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8880 - loss: 0.6180\n",
            "Epoch 40: val_accuracy did not improve from 0.82390\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8872 - loss: 0.6191 - val_accuracy: 0.7484 - val_loss: 0.9210\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8882 - loss: 0.5744\n",
            "Epoch 41: val_accuracy improved from 0.82390 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.8880 - loss: 0.5744 - val_accuracy: 0.8491 - val_loss: 0.7651\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9073 - loss: 0.5285\n",
            "Epoch 42: val_accuracy improved from 0.84906 to 0.85535, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.9063 - loss: 0.5304 - val_accuracy: 0.8553 - val_loss: 0.6635\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8958 - loss: 0.5329\n",
            "Epoch 43: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.8951 - loss: 0.5328 - val_accuracy: 0.8428 - val_loss: 0.7241\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9420 - loss: 0.4713\n",
            "Epoch 44: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.9405 - loss: 0.4733 - val_accuracy: 0.8239 - val_loss: 0.7325\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9199 - loss: 0.4614\n",
            "Epoch 45: val_accuracy improved from 0.85535 to 0.86792, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9193 - loss: 0.4620 - val_accuracy: 0.8679 - val_loss: 0.6300\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9032 - loss: 0.4943\n",
            "Epoch 46: val_accuracy improved from 0.86792 to 0.87421, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.9026 - loss: 0.4947 - val_accuracy: 0.8742 - val_loss: 0.5927\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9077 - loss: 0.4738\n",
            "Epoch 47: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9084 - loss: 0.4727 - val_accuracy: 0.8742 - val_loss: 0.5966\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8816 - loss: 0.4822\n",
            "Epoch 48: val_accuracy improved from 0.87421 to 0.88679, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.8822 - loss: 0.4814 - val_accuracy: 0.8868 - val_loss: 0.5645\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9331 - loss: 0.3712\n",
            "Epoch 49: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9328 - loss: 0.3729 - val_accuracy: 0.8679 - val_loss: 0.5682\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8925 - loss: 0.4541\n",
            "Epoch 50: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.8936 - loss: 0.4520 - val_accuracy: 0.8742 - val_loss: 0.5767\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9254 - loss: 0.3976\n",
            "Epoch 51: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9254 - loss: 0.3980 - val_accuracy: 0.8491 - val_loss: 0.6532\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9071 - loss: 0.4005\n",
            "Epoch 52: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9074 - loss: 0.4007 - val_accuracy: 0.8868 - val_loss: 0.4747\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9332 - loss: 0.3876\n",
            "Epoch 53: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9329 - loss: 0.3878 - val_accuracy: 0.8868 - val_loss: 0.5231\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9070 - loss: 0.3902\n",
            "Epoch 54: val_accuracy did not improve from 0.88679\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.9072 - loss: 0.3907 - val_accuracy: 0.8742 - val_loss: 0.5260\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9306 - loss: 0.3288\n",
            "Epoch 55: val_accuracy improved from 0.88679 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9302 - loss: 0.3296 - val_accuracy: 0.8994 - val_loss: 0.4680\n",
            "Train Accuracy: 0.9785, Test Accuracy: 0.9192\n",
            "Training with L2 regularization factor: 0.0001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_10             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_15     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_15           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_16     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_16           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_11             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_17     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_25 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_17           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_21 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_26 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_20 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_22 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_5              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_27 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_23 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_28 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_29 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_10             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_15     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_15           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_16     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_16           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_11             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_17     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_17           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_5              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.0125 - loss: 4.6709\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 560ms/step - accuracy: 0.0126 - loss: 4.6699 - val_accuracy: 0.0000e+00 - val_loss: 4.6002\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0219 - loss: 4.5584\n",
            "Epoch 2: val_accuracy did not improve from 0.00000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.0222 - loss: 4.5573 - val_accuracy: 0.0000e+00 - val_loss: 4.5894\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0431 - loss: 4.4819\n",
            "Epoch 3: val_accuracy improved from 0.00000 to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.0434 - loss: 4.4807 - val_accuracy: 0.0063 - val_loss: 4.5778\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.0622 - loss: 4.4195\n",
            "Epoch 4: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.0623 - loss: 4.4183 - val_accuracy: 0.0063 - val_loss: 4.5676\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.0835 - loss: 4.3245\n",
            "Epoch 5: val_accuracy did not improve from 0.00629\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.0833 - loss: 4.3236 - val_accuracy: 0.0063 - val_loss: 4.5536\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0798 - loss: 4.2184\n",
            "Epoch 6: val_accuracy improved from 0.00629 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0802 - loss: 4.2177 - val_accuracy: 0.0189 - val_loss: 4.5394\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1205 - loss: 4.1335\n",
            "Epoch 7: val_accuracy did not improve from 0.01887\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.1207 - loss: 4.1332 - val_accuracy: 0.0189 - val_loss: 4.5219\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1449 - loss: 4.0008\n",
            "Epoch 8: val_accuracy improved from 0.01887 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.1441 - loss: 4.0015 - val_accuracy: 0.0314 - val_loss: 4.5020\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1880 - loss: 3.8697\n",
            "Epoch 9: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.1878 - loss: 3.8709 - val_accuracy: 0.0314 - val_loss: 4.4808\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2194 - loss: 3.7358\n",
            "Epoch 10: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.2192 - loss: 3.7356 - val_accuracy: 0.0314 - val_loss: 4.4506\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2468 - loss: 3.6299\n",
            "Epoch 11: val_accuracy improved from 0.03145 to 0.05031, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.2471 - loss: 3.6298 - val_accuracy: 0.0503 - val_loss: 4.4093\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2921 - loss: 3.4830\n",
            "Epoch 12: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.2921 - loss: 3.4803 - val_accuracy: 0.0503 - val_loss: 4.3617\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3143 - loss: 3.3108\n",
            "Epoch 13: val_accuracy did not improve from 0.05031\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.3150 - loss: 3.3084 - val_accuracy: 0.0503 - val_loss: 4.3257\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3722 - loss: 3.0588\n",
            "Epoch 14: val_accuracy improved from 0.05031 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.3717 - loss: 3.0607 - val_accuracy: 0.0566 - val_loss: 4.2546\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3853 - loss: 2.9725\n",
            "Epoch 15: val_accuracy improved from 0.05660 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.3854 - loss: 2.9703 - val_accuracy: 0.0881 - val_loss: 4.2054\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.4419 - loss: 2.7877\n",
            "Epoch 16: val_accuracy did not improve from 0.08805\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.4422 - loss: 2.7860 - val_accuracy: 0.0818 - val_loss: 4.1263\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.4823 - loss: 2.5756\n",
            "Epoch 17: val_accuracy improved from 0.08805 to 0.11950, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.4818 - loss: 2.5755 - val_accuracy: 0.1195 - val_loss: 3.9874\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5170 - loss: 2.4189\n",
            "Epoch 18: val_accuracy did not improve from 0.11950\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5158 - loss: 2.4201 - val_accuracy: 0.0943 - val_loss: 3.9362\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5528 - loss: 2.2561\n",
            "Epoch 19: val_accuracy improved from 0.11950 to 0.13836, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5526 - loss: 2.2566 - val_accuracy: 0.1384 - val_loss: 3.7396\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5699 - loss: 2.1724\n",
            "Epoch 20: val_accuracy improved from 0.13836 to 0.15723, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5696 - loss: 2.1708 - val_accuracy: 0.1572 - val_loss: 3.6025\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6311 - loss: 2.0025\n",
            "Epoch 21: val_accuracy improved from 0.15723 to 0.16981, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.6307 - loss: 2.0002 - val_accuracy: 0.1698 - val_loss: 3.4768\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6369 - loss: 1.8022\n",
            "Epoch 22: val_accuracy improved from 0.16981 to 0.18868, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.6362 - loss: 1.8026 - val_accuracy: 0.1887 - val_loss: 3.2651\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6454 - loss: 1.7455\n",
            "Epoch 23: val_accuracy did not improve from 0.18868\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.6448 - loss: 1.7441 - val_accuracy: 0.1887 - val_loss: 3.2435\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6350 - loss: 1.6107\n",
            "Epoch 24: val_accuracy improved from 0.18868 to 0.23270, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6354 - loss: 1.6101 - val_accuracy: 0.2327 - val_loss: 2.9854\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6572 - loss: 1.5291\n",
            "Epoch 25: val_accuracy improved from 0.23270 to 0.25157, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6578 - loss: 1.5269 - val_accuracy: 0.2516 - val_loss: 2.7608\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7337 - loss: 1.3830\n",
            "Epoch 26: val_accuracy improved from 0.25157 to 0.28931, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7328 - loss: 1.3851 - val_accuracy: 0.2893 - val_loss: 2.5825\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7404 - loss: 1.2796\n",
            "Epoch 27: val_accuracy improved from 0.28931 to 0.35849, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7397 - loss: 1.2805 - val_accuracy: 0.3585 - val_loss: 2.3725\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7504 - loss: 1.2318\n",
            "Epoch 28: val_accuracy improved from 0.35849 to 0.42767, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.7487 - loss: 1.2351 - val_accuracy: 0.4277 - val_loss: 2.1205\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7696 - loss: 1.1592\n",
            "Epoch 29: val_accuracy improved from 0.42767 to 0.51572, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.7694 - loss: 1.1596 - val_accuracy: 0.5157 - val_loss: 1.9083\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7695 - loss: 1.0757\n",
            "Epoch 30: val_accuracy did not improve from 0.51572\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7701 - loss: 1.0754 - val_accuracy: 0.4843 - val_loss: 1.8155\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7532 - loss: 1.0719\n",
            "Epoch 31: val_accuracy improved from 0.51572 to 0.61006, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7532 - loss: 1.0731 - val_accuracy: 0.6101 - val_loss: 1.5668\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8000 - loss: 1.0026\n",
            "Epoch 32: val_accuracy improved from 0.61006 to 0.62264, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7999 - loss: 1.0011 - val_accuracy: 0.6226 - val_loss: 1.5003\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8078 - loss: 0.9246\n",
            "Epoch 33: val_accuracy improved from 0.62264 to 0.70440, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8086 - loss: 0.9241 - val_accuracy: 0.7044 - val_loss: 1.3088\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7804 - loss: 0.8795\n",
            "Epoch 34: val_accuracy improved from 0.70440 to 0.73585, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.7809 - loss: 0.8797 - val_accuracy: 0.7358 - val_loss: 1.2600\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8352 - loss: 0.8715\n",
            "Epoch 35: val_accuracy improved from 0.73585 to 0.78616, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8345 - loss: 0.8712 - val_accuracy: 0.7862 - val_loss: 1.0485\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8229 - loss: 0.8294\n",
            "Epoch 36: val_accuracy did not improve from 0.78616\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8236 - loss: 0.8283 - val_accuracy: 0.7673 - val_loss: 1.0713\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8280 - loss: 0.7825\n",
            "Epoch 37: val_accuracy did not improve from 0.78616\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8282 - loss: 0.7821 - val_accuracy: 0.7673 - val_loss: 0.9970\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8807 - loss: 0.7052\n",
            "Epoch 38: val_accuracy improved from 0.78616 to 0.81761, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.8798 - loss: 0.7054 - val_accuracy: 0.8176 - val_loss: 0.8775\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8315 - loss: 0.7368\n",
            "Epoch 39: val_accuracy did not improve from 0.81761\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8320 - loss: 0.7370 - val_accuracy: 0.7987 - val_loss: 0.8932\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8708 - loss: 0.6670\n",
            "Epoch 40: val_accuracy improved from 0.81761 to 0.82390, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.8707 - loss: 0.6661 - val_accuracy: 0.8239 - val_loss: 0.8373\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8816 - loss: 0.6291\n",
            "Epoch 41: val_accuracy improved from 0.82390 to 0.88050, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.8812 - loss: 0.6304 - val_accuracy: 0.8805 - val_loss: 0.7535\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8974 - loss: 0.5378\n",
            "Epoch 42: val_accuracy did not improve from 0.88050\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8963 - loss: 0.5403 - val_accuracy: 0.8742 - val_loss: 0.7104\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9053 - loss: 0.5599\n",
            "Epoch 43: val_accuracy did not improve from 0.88050\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9051 - loss: 0.5593 - val_accuracy: 0.8805 - val_loss: 0.6419\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9128 - loss: 0.4888\n",
            "Epoch 44: val_accuracy improved from 0.88050 to 0.89308, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9123 - loss: 0.4898 - val_accuracy: 0.8931 - val_loss: 0.5938\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8801 - loss: 0.5285\n",
            "Epoch 45: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8797 - loss: 0.5293 - val_accuracy: 0.8742 - val_loss: 0.6340\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8765 - loss: 0.5567\n",
            "Epoch 46: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.8768 - loss: 0.5547 - val_accuracy: 0.8742 - val_loss: 0.6027\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8742 - loss: 0.5139\n",
            "Epoch 47: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8745 - loss: 0.5136 - val_accuracy: 0.8553 - val_loss: 0.5767\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9096 - loss: 0.4691\n",
            "Epoch 48: val_accuracy improved from 0.89308 to 0.92453, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.9088 - loss: 0.4694 - val_accuracy: 0.9245 - val_loss: 0.5490\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9103 - loss: 0.4617\n",
            "Epoch 49: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9102 - loss: 0.4611 - val_accuracy: 0.9119 - val_loss: 0.5192\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8872 - loss: 0.4599\n",
            "Epoch 50: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8881 - loss: 0.4589 - val_accuracy: 0.8994 - val_loss: 0.4851\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9102 - loss: 0.4349\n",
            "Epoch 51: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.9102 - loss: 0.4359 - val_accuracy: 0.8931 - val_loss: 0.4824\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9271 - loss: 0.3906\n",
            "Epoch 52: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9272 - loss: 0.3902 - val_accuracy: 0.8805 - val_loss: 0.5194\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9253 - loss: 0.3858\n",
            "Epoch 53: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9252 - loss: 0.3857 - val_accuracy: 0.9245 - val_loss: 0.4832\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8993 - loss: 0.4116\n",
            "Epoch 54: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8995 - loss: 0.4121 - val_accuracy: 0.8742 - val_loss: 0.5187\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9198 - loss: 0.3947\n",
            "Epoch 55: val_accuracy did not improve from 0.92453\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.9196 - loss: 0.3951 - val_accuracy: 0.9119 - val_loss: 0.4375\n",
            "Train Accuracy: 0.9823, Test Accuracy: 0.9040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Different number of layers"
      ],
      "metadata": {
        "id": "_mrBaAEMxKRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    training(\n",
        "        X_train_images, X_train_tabular, y_train,\n",
        "        X_test_images, X_test_tabular, y_test,\n",
        "        epochs=55, batch_size=32, learning_rate=0.0001,\n",
        "        dropout=0.2, optimizer_name='adam', l2_factor=0.0\n",
        "    )\n",
        "\n",
        "    training2(\n",
        "        X_train_images, X_train_tabular, y_train,\n",
        "        X_test_images, X_test_tabular, y_test,\n",
        "        epochs=55, batch_size=32, learning_rate=0.0001,\n",
        "        dropout=0.2, optimizer_name='adam', l2_factor=0.0\n",
        "    )\n",
        "\n",
        "    training3(\n",
        "        X_train_images, X_train_tabular, y_train,\n",
        "        X_test_images, X_test_tabular, y_test,\n",
        "        epochs=55, batch_size=32, learning_rate=0.0001,\n",
        "        dropout=0.2, optimizer_name='adam', l2_factor=0.0\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EAz3SMDoIRih",
        "outputId": "1007d5eb-f8ee-43aa-a465-7fe67909835b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_12             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_18     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_18           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_13             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_19     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_30 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_19           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_25 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_31 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_24 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_26 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_6              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_32 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m98,816\u001b[0m  concatenate_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_27 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_33 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_34 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_12             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_18     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_18           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_13             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_19     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_19           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_6              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span>  concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m349,731\u001b[0m (1.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,731</span> (1.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m349,347\u001b[0m (1.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">349,347</span> (1.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.0122 - loss: 4.6168\n",
            "Epoch 1: val_accuracy improved from -inf to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 447ms/step - accuracy: 0.0121 - loss: 4.6167 - val_accuracy: 0.0314 - val_loss: 4.6063\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.0174 - loss: 4.5721\n",
            "Epoch 2: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 80ms/step - accuracy: 0.0176 - loss: 4.5717 - val_accuracy: 0.0252 - val_loss: 4.5753\n",
            "Epoch 3/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0312 - loss: 4.4950\n",
            "Epoch 3: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.0316 - loss: 4.4941 - val_accuracy: 0.0252 - val_loss: 4.5514\n",
            "Epoch 4/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.0579 - loss: 4.4448\n",
            "Epoch 4: val_accuracy improved from 0.03145 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.0578 - loss: 4.4437 - val_accuracy: 0.0440 - val_loss: 4.5283\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0610 - loss: 4.3902\n",
            "Epoch 5: val_accuracy improved from 0.04403 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.0615 - loss: 4.3891 - val_accuracy: 0.0629 - val_loss: 4.4998\n",
            "Epoch 6/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1021 - loss: 4.3248\n",
            "Epoch 6: val_accuracy improved from 0.06289 to 0.07547, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.1039 - loss: 4.3210 - val_accuracy: 0.0755 - val_loss: 4.4672\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1450 - loss: 4.2308\n",
            "Epoch 7: val_accuracy improved from 0.07547 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.1453 - loss: 4.2298 - val_accuracy: 0.0818 - val_loss: 4.4297\n",
            "Epoch 8/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1622 - loss: 4.1580\n",
            "Epoch 8: val_accuracy improved from 0.08176 to 0.10063, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.1624 - loss: 4.1542 - val_accuracy: 0.1006 - val_loss: 4.3820\n",
            "Epoch 9/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1758 - loss: 4.0266\n",
            "Epoch 9: val_accuracy improved from 0.10063 to 0.10692, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.1768 - loss: 4.0235 - val_accuracy: 0.1069 - val_loss: 4.3228\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.2288 - loss: 3.8692\n",
            "Epoch 10: val_accuracy did not improve from 0.10692\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.2280 - loss: 3.8694 - val_accuracy: 0.1069 - val_loss: 4.2497\n",
            "Epoch 11/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.2363 - loss: 3.7701\n",
            "Epoch 11: val_accuracy improved from 0.10692 to 0.11321, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.2377 - loss: 3.7658 - val_accuracy: 0.1132 - val_loss: 4.1669\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2753 - loss: 3.6410\n",
            "Epoch 12: val_accuracy improved from 0.11321 to 0.11950, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.2761 - loss: 3.6385 - val_accuracy: 0.1195 - val_loss: 4.0862\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3035 - loss: 3.3903\n",
            "Epoch 13: val_accuracy improved from 0.11950 to 0.15723, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.3034 - loss: 3.3919 - val_accuracy: 0.1572 - val_loss: 3.9759\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3464 - loss: 3.2517\n",
            "Epoch 14: val_accuracy improved from 0.15723 to 0.19497, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.3460 - loss: 3.2509 - val_accuracy: 0.1950 - val_loss: 3.8717\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3638 - loss: 3.1385\n",
            "Epoch 15: val_accuracy improved from 0.19497 to 0.22642, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.3649 - loss: 3.1355 - val_accuracy: 0.2264 - val_loss: 3.7366\n",
            "Epoch 16/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4129 - loss: 2.9117\n",
            "Epoch 16: val_accuracy improved from 0.22642 to 0.23270, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.4130 - loss: 2.9100 - val_accuracy: 0.2327 - val_loss: 3.5926\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4397 - loss: 2.7577\n",
            "Epoch 17: val_accuracy improved from 0.23270 to 0.25786, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4402 - loss: 2.7557 - val_accuracy: 0.2579 - val_loss: 3.4579\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.4674 - loss: 2.5123\n",
            "Epoch 18: val_accuracy improved from 0.25786 to 0.28931, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4664 - loss: 2.5133 - val_accuracy: 0.2893 - val_loss: 3.2956\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4964 - loss: 2.4172\n",
            "Epoch 19: val_accuracy improved from 0.28931 to 0.30189, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4964 - loss: 2.4161 - val_accuracy: 0.3019 - val_loss: 3.1433\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5241 - loss: 2.2568\n",
            "Epoch 20: val_accuracy improved from 0.30189 to 0.32075, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.5241 - loss: 2.2542 - val_accuracy: 0.3208 - val_loss: 2.9560\n",
            "Epoch 21/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5996 - loss: 1.9678\n",
            "Epoch 21: val_accuracy improved from 0.32075 to 0.33962, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.5966 - loss: 1.9752 - val_accuracy: 0.3396 - val_loss: 2.8171\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5711 - loss: 2.0138\n",
            "Epoch 22: val_accuracy improved from 0.33962 to 0.37107, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.5723 - loss: 2.0119 - val_accuracy: 0.3711 - val_loss: 2.6375\n",
            "Epoch 23/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5960 - loss: 1.8806\n",
            "Epoch 23: val_accuracy improved from 0.37107 to 0.42138, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.5969 - loss: 1.8784 - val_accuracy: 0.4214 - val_loss: 2.4774\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6308 - loss: 1.7269\n",
            "Epoch 24: val_accuracy improved from 0.42138 to 0.49057, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.6311 - loss: 1.7256 - val_accuracy: 0.4906 - val_loss: 2.3381\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6316 - loss: 1.7125\n",
            "Epoch 25: val_accuracy did not improve from 0.49057\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.6330 - loss: 1.7089 - val_accuracy: 0.4654 - val_loss: 2.2345\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6576 - loss: 1.5535\n",
            "Epoch 26: val_accuracy improved from 0.49057 to 0.50314, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.6573 - loss: 1.5547 - val_accuracy: 0.5031 - val_loss: 2.0900\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6865 - loss: 1.4585\n",
            "Epoch 27: val_accuracy improved from 0.50314 to 0.55346, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.6860 - loss: 1.4574 - val_accuracy: 0.5535 - val_loss: 1.9120\n",
            "Epoch 28/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6818 - loss: 1.4320\n",
            "Epoch 28: val_accuracy improved from 0.55346 to 0.58491, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.6841 - loss: 1.4263 - val_accuracy: 0.5849 - val_loss: 1.7912\n",
            "Epoch 29/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7284 - loss: 1.2661\n",
            "Epoch 29: val_accuracy improved from 0.58491 to 0.60377, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.7256 - loss: 1.2689 - val_accuracy: 0.6038 - val_loss: 1.6911\n",
            "Epoch 30/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7228 - loss: 1.2148\n",
            "Epoch 30: val_accuracy improved from 0.60377 to 0.67296, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.7203 - loss: 1.2199 - val_accuracy: 0.6730 - val_loss: 1.5175\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7165 - loss: 1.2142\n",
            "Epoch 31: val_accuracy did not improve from 0.67296\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.7158 - loss: 1.2170 - val_accuracy: 0.6604 - val_loss: 1.4493\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7524 - loss: 1.1083\n",
            "Epoch 32: val_accuracy did not improve from 0.67296\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.7519 - loss: 1.1094 - val_accuracy: 0.6541 - val_loss: 1.4163\n",
            "Epoch 33/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7658 - loss: 1.0819\n",
            "Epoch 33: val_accuracy improved from 0.67296 to 0.70440, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.7655 - loss: 1.0814 - val_accuracy: 0.7044 - val_loss: 1.2847\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7169 - loss: 1.1132\n",
            "Epoch 34: val_accuracy improved from 0.70440 to 0.76730, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.7183 - loss: 1.1093 - val_accuracy: 0.7673 - val_loss: 1.1774\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8115 - loss: 0.9501\n",
            "Epoch 35: val_accuracy did not improve from 0.76730\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8088 - loss: 0.9538 - val_accuracy: 0.7421 - val_loss: 1.1464\n",
            "Epoch 36/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7623 - loss: 0.9962\n",
            "Epoch 36: val_accuracy did not improve from 0.76730\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.7606 - loss: 0.9993 - val_accuracy: 0.7610 - val_loss: 1.0110\n",
            "Epoch 37/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7941 - loss: 0.9061\n",
            "Epoch 37: val_accuracy did not improve from 0.76730\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.7913 - loss: 0.9085 - val_accuracy: 0.7610 - val_loss: 1.0211\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8060 - loss: 0.8914\n",
            "Epoch 38: val_accuracy improved from 0.76730 to 0.79245, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.8066 - loss: 0.8905 - val_accuracy: 0.7925 - val_loss: 0.9334\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8059 - loss: 0.8280\n",
            "Epoch 39: val_accuracy improved from 0.79245 to 0.83019, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.8058 - loss: 0.8282 - val_accuracy: 0.8302 - val_loss: 0.8605\n",
            "Epoch 40/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8075 - loss: 0.8300\n",
            "Epoch 40: val_accuracy did not improve from 0.83019\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.8069 - loss: 0.8304 - val_accuracy: 0.8176 - val_loss: 0.8529\n",
            "Epoch 41/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8176 - loss: 0.8222\n",
            "Epoch 41: val_accuracy did not improve from 0.83019\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8157 - loss: 0.8228 - val_accuracy: 0.8239 - val_loss: 0.8130\n",
            "Epoch 42/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8489 - loss: 0.7529\n",
            "Epoch 42: val_accuracy improved from 0.83019 to 0.84906, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.8461 - loss: 0.7552 - val_accuracy: 0.8491 - val_loss: 0.7503\n",
            "Epoch 43/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8378 - loss: 0.7117\n",
            "Epoch 43: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8385 - loss: 0.7112 - val_accuracy: 0.8428 - val_loss: 0.7559\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8581 - loss: 0.6590\n",
            "Epoch 44: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.8577 - loss: 0.6596 - val_accuracy: 0.8302 - val_loss: 0.7257\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8568 - loss: 0.6660\n",
            "Epoch 45: val_accuracy did not improve from 0.84906\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8556 - loss: 0.6672 - val_accuracy: 0.8491 - val_loss: 0.6782\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8548 - loss: 0.6401\n",
            "Epoch 46: val_accuracy improved from 0.84906 to 0.86792, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8540 - loss: 0.6407 - val_accuracy: 0.8679 - val_loss: 0.6433\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8110 - loss: 0.6957\n",
            "Epoch 47: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.8116 - loss: 0.6944 - val_accuracy: 0.8553 - val_loss: 0.6380\n",
            "Epoch 48/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8249 - loss: 0.6467\n",
            "Epoch 48: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8255 - loss: 0.6461 - val_accuracy: 0.8050 - val_loss: 0.7808\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8638 - loss: 0.5911\n",
            "Epoch 49: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8637 - loss: 0.5921 - val_accuracy: 0.8428 - val_loss: 0.6044\n",
            "Epoch 50/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8467 - loss: 0.6244\n",
            "Epoch 50: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.8464 - loss: 0.6264 - val_accuracy: 0.8616 - val_loss: 0.5740\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8494 - loss: 0.6126\n",
            "Epoch 51: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8496 - loss: 0.6112 - val_accuracy: 0.8679 - val_loss: 0.5583\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8473 - loss: 0.5724\n",
            "Epoch 52: val_accuracy did not improve from 0.86792\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8474 - loss: 0.5722 - val_accuracy: 0.8491 - val_loss: 0.5571\n",
            "Epoch 53/55\n",
            "\u001b[1m19/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8930 - loss: 0.4678\n",
            "Epoch 53: val_accuracy improved from 0.86792 to 0.89308, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.8917 - loss: 0.4708 - val_accuracy: 0.8931 - val_loss: 0.5079\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8833 - loss: 0.4873\n",
            "Epoch 54: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8828 - loss: 0.4875 - val_accuracy: 0.8553 - val_loss: 0.5042\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9062 - loss: 0.4526\n",
            "Epoch 55: val_accuracy did not improve from 0.89308\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.9055 - loss: 0.4535 - val_accuracy: 0.8868 - val_loss: 0.5079\n",
            "Train Accuracy: 0.9735, Test Accuracy: 0.9141\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_14             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_20     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_20           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_21     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_21           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_15             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_22     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_35 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_22           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_29 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_36 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_28 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_30 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_7              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_37 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m164,352\u001b[0m  concatenate_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_31 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_38 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_39 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_14             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_20     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_20           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_21     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_21           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_15             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_22     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_22           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_7              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span>  concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m711,459\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">711,459</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m710,563\u001b[0m (2.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">710,563</span> (2.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.0192 - loss: 4.6673\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 522ms/step - accuracy: 0.0192 - loss: 4.6661 - val_accuracy: 0.0000e+00 - val_loss: 4.5986\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0358 - loss: 4.5492\n",
            "Epoch 2: val_accuracy improved from 0.00000 to 0.01258, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 113ms/step - accuracy: 0.0355 - loss: 4.5484 - val_accuracy: 0.0126 - val_loss: 4.5885\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.0554 - loss: 4.4631\n",
            "Epoch 3: val_accuracy improved from 0.01258 to 0.03145, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.0551 - loss: 4.4632 - val_accuracy: 0.0314 - val_loss: 4.5837\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0554 - loss: 4.4019\n",
            "Epoch 4: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.0552 - loss: 4.4017 - val_accuracy: 0.0314 - val_loss: 4.5783\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0511 - loss: 4.3096\n",
            "Epoch 5: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.0511 - loss: 4.3095 - val_accuracy: 0.0252 - val_loss: 4.5673\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.0735 - loss: 4.2224\n",
            "Epoch 6: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.0743 - loss: 4.2229 - val_accuracy: 0.0252 - val_loss: 4.5544\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.0836 - loss: 4.2140\n",
            "Epoch 7: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.0851 - loss: 4.2104 - val_accuracy: 0.0189 - val_loss: 4.5341\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.1708 - loss: 4.0302\n",
            "Epoch 8: val_accuracy did not improve from 0.03145\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.1713 - loss: 4.0295 - val_accuracy: 0.0252 - val_loss: 4.5139\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.2163 - loss: 3.8902\n",
            "Epoch 9: val_accuracy improved from 0.03145 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 126ms/step - accuracy: 0.2152 - loss: 3.8918 - val_accuracy: 0.0377 - val_loss: 4.4870\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2287 - loss: 3.7301\n",
            "Epoch 10: val_accuracy improved from 0.03774 to 0.06289, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.2289 - loss: 3.7315 - val_accuracy: 0.0629 - val_loss: 4.4517\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2547 - loss: 3.6044\n",
            "Epoch 11: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.2554 - loss: 3.6040 - val_accuracy: 0.0566 - val_loss: 4.4084\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3253 - loss: 3.3593\n",
            "Epoch 12: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.3243 - loss: 3.3615 - val_accuracy: 0.0629 - val_loss: 4.3645\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.3304 - loss: 3.3585\n",
            "Epoch 13: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.3312 - loss: 3.3549 - val_accuracy: 0.0566 - val_loss: 4.2947\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4271 - loss: 3.0221\n",
            "Epoch 14: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.4250 - loss: 3.0244 - val_accuracy: 0.0566 - val_loss: 4.2354\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.4236 - loss: 2.8975\n",
            "Epoch 15: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.4236 - loss: 2.8964 - val_accuracy: 0.0566 - val_loss: 4.1691\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.4622 - loss: 2.6647\n",
            "Epoch 16: val_accuracy did not improve from 0.06289\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.4616 - loss: 2.6657 - val_accuracy: 0.0629 - val_loss: 4.0839\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5070 - loss: 2.5144\n",
            "Epoch 17: val_accuracy improved from 0.06289 to 0.10063, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.5057 - loss: 2.5155 - val_accuracy: 0.1006 - val_loss: 3.9269\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5435 - loss: 2.3266\n",
            "Epoch 18: val_accuracy did not improve from 0.10063\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.5425 - loss: 2.3284 - val_accuracy: 0.1006 - val_loss: 3.8192\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5492 - loss: 2.2564\n",
            "Epoch 19: val_accuracy improved from 0.10063 to 0.14465, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.5492 - loss: 2.2540 - val_accuracy: 0.1447 - val_loss: 3.6643\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5957 - loss: 2.0601\n",
            "Epoch 20: val_accuracy improved from 0.14465 to 0.20126, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.5940 - loss: 2.0613 - val_accuracy: 0.2013 - val_loss: 3.4309\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6393 - loss: 1.9030\n",
            "Epoch 21: val_accuracy improved from 0.20126 to 0.20755, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.6385 - loss: 1.9028 - val_accuracy: 0.2075 - val_loss: 3.2422\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6689 - loss: 1.7346\n",
            "Epoch 22: val_accuracy did not improve from 0.20755\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.6677 - loss: 1.7366 - val_accuracy: 0.2075 - val_loss: 3.1062\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6778 - loss: 1.6796\n",
            "Epoch 23: val_accuracy improved from 0.20755 to 0.30189, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.6777 - loss: 1.6777 - val_accuracy: 0.3019 - val_loss: 2.8295\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7176 - loss: 1.5179\n",
            "Epoch 24: val_accuracy improved from 0.30189 to 0.33333, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7169 - loss: 1.5176 - val_accuracy: 0.3333 - val_loss: 2.6040\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7090 - loss: 1.4631\n",
            "Epoch 25: val_accuracy improved from 0.33333 to 0.41509, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.7090 - loss: 1.4629 - val_accuracy: 0.4151 - val_loss: 2.4220\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7364 - loss: 1.3578\n",
            "Epoch 26: val_accuracy did not improve from 0.41509\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.7366 - loss: 1.3559 - val_accuracy: 0.4025 - val_loss: 2.2805\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7770 - loss: 1.2219\n",
            "Epoch 27: val_accuracy improved from 0.41509 to 0.52830, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.7757 - loss: 1.2229 - val_accuracy: 0.5283 - val_loss: 1.9961\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7630 - loss: 1.1529\n",
            "Epoch 28: val_accuracy improved from 0.52830 to 0.56604, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.7626 - loss: 1.1543 - val_accuracy: 0.5660 - val_loss: 1.8353\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7577 - loss: 1.1181\n",
            "Epoch 29: val_accuracy improved from 0.56604 to 0.59748, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7567 - loss: 1.1188 - val_accuracy: 0.5975 - val_loss: 1.6917\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7718 - loss: 1.0439\n",
            "Epoch 30: val_accuracy improved from 0.59748 to 0.64151, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.7721 - loss: 1.0443 - val_accuracy: 0.6415 - val_loss: 1.5541\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7810 - loss: 1.0560\n",
            "Epoch 31: val_accuracy did not improve from 0.64151\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.7807 - loss: 1.0562 - val_accuracy: 0.6164 - val_loss: 1.5374\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7826 - loss: 1.0103\n",
            "Epoch 32: val_accuracy improved from 0.64151 to 0.70440, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.7824 - loss: 1.0093 - val_accuracy: 0.7044 - val_loss: 1.2744\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8037 - loss: 0.9394\n",
            "Epoch 33: val_accuracy did not improve from 0.70440\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.8043 - loss: 0.9379 - val_accuracy: 0.6981 - val_loss: 1.1925\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8295 - loss: 0.7977\n",
            "Epoch 34: val_accuracy improved from 0.70440 to 0.76101, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.8287 - loss: 0.8007 - val_accuracy: 0.7610 - val_loss: 1.1432\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8515 - loss: 0.7675\n",
            "Epoch 35: val_accuracy improved from 0.76101 to 0.79245, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.8507 - loss: 0.7689 - val_accuracy: 0.7925 - val_loss: 1.0419\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8377 - loss: 0.7673\n",
            "Epoch 36: val_accuracy did not improve from 0.79245\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8370 - loss: 0.7685 - val_accuracy: 0.7862 - val_loss: 0.9865\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8302 - loss: 0.7517\n",
            "Epoch 37: val_accuracy did not improve from 0.79245\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8304 - loss: 0.7512 - val_accuracy: 0.7610 - val_loss: 0.9732\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8581 - loss: 0.6709\n",
            "Epoch 38: val_accuracy improved from 0.79245 to 0.79874, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8580 - loss: 0.6708 - val_accuracy: 0.7987 - val_loss: 0.9068\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8509 - loss: 0.6575\n",
            "Epoch 39: val_accuracy improved from 0.79874 to 0.83648, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.8506 - loss: 0.6575 - val_accuracy: 0.8365 - val_loss: 0.8114\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8652 - loss: 0.6508\n",
            "Epoch 40: val_accuracy improved from 0.83648 to 0.85535, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.8655 - loss: 0.6498 - val_accuracy: 0.8553 - val_loss: 0.7406\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8680 - loss: 0.6190\n",
            "Epoch 41: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8679 - loss: 0.6186 - val_accuracy: 0.8302 - val_loss: 0.7794\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8656 - loss: 0.5941\n",
            "Epoch 42: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8665 - loss: 0.5922 - val_accuracy: 0.8491 - val_loss: 0.7113\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8924 - loss: 0.5513\n",
            "Epoch 43: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.8915 - loss: 0.5528 - val_accuracy: 0.8239 - val_loss: 0.7361\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8853 - loss: 0.5422\n",
            "Epoch 44: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8850 - loss: 0.5427 - val_accuracy: 0.8553 - val_loss: 0.6846\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9086 - loss: 0.4751\n",
            "Epoch 45: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9077 - loss: 0.4776 - val_accuracy: 0.8428 - val_loss: 0.7070\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8971 - loss: 0.5064\n",
            "Epoch 46: val_accuracy did not improve from 0.85535\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 106ms/step - accuracy: 0.8972 - loss: 0.5060 - val_accuracy: 0.7799 - val_loss: 0.8345\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8844 - loss: 0.5033\n",
            "Epoch 47: val_accuracy improved from 0.85535 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.8845 - loss: 0.5027 - val_accuracy: 0.8994 - val_loss: 0.5658\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9039 - loss: 0.4483\n",
            "Epoch 48: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9034 - loss: 0.4483 - val_accuracy: 0.8302 - val_loss: 0.6607\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9033 - loss: 0.4810\n",
            "Epoch 49: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.9040 - loss: 0.4788 - val_accuracy: 0.8868 - val_loss: 0.5020\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9001 - loss: 0.4263\n",
            "Epoch 50: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.9005 - loss: 0.4261 - val_accuracy: 0.8805 - val_loss: 0.5109\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9243 - loss: 0.4209\n",
            "Epoch 51: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.9238 - loss: 0.4202 - val_accuracy: 0.8868 - val_loss: 0.5173\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9101 - loss: 0.3987\n",
            "Epoch 52: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9108 - loss: 0.3984 - val_accuracy: 0.8742 - val_loss: 0.5461\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9382 - loss: 0.3093\n",
            "Epoch 53: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.9378 - loss: 0.3111 - val_accuracy: 0.8553 - val_loss: 0.4900\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9369 - loss: 0.3758\n",
            "Epoch 54: val_accuracy improved from 0.89937 to 0.91824, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.9367 - loss: 0.3752 - val_accuracy: 0.9182 - val_loss: 0.4515\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9040 - loss: 0.3646\n",
            "Epoch 55: val_accuracy did not improve from 0.91824\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9042 - loss: 0.3642 - val_accuracy: 0.8994 - val_loss: 0.4674\n",
            "Train Accuracy: 0.9760, Test Accuracy: 0.8838\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_16             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m1,792\u001b[0m  input_layer_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " batch_normalization_23     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m256\u001b[0m  conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_23           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,856\u001b[0m  max_pooling2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_24     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_24           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m295,168\u001b[0m  max_pooling2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " batch_normalization_25     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " max_pooling2d_25           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m512\u001b[0m)          \u001b[38;5;34m1,180,160\u001b[0m  max_pooling2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " input_layer_17             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " batch_normalization_26     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m2,048\u001b[0m  conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_40 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m9,472\u001b[0m  input_layer_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " max_pooling2d_26           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              " (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n",
              "\n",
              " dropout_33 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  max_pooling2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " dense_41 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m8,256\u001b[0m  dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_32 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n",
              "\n",
              " dropout_34 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dense_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " concatenate_8              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_42 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m295,424\u001b[0m  concatenate_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_35 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  dense_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " dense_43 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m131,328\u001b[0m  dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_44 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)                      \u001b[38;5;34m25,443\u001b[0m  dense_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_16             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " batch_normalization_23     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_23           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  max_pooling2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_24     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_24           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  max_pooling2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " batch_normalization_25     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " max_pooling2d_25           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span>  max_pooling2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " input_layer_17             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " batch_normalization_26     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " max_pooling2d_26           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n",
              "\n",
              " dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  max_pooling2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n",
              "\n",
              " dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " concatenate_8              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,424</span>  concatenate_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span>  dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,443</span>  dense_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,024,739\u001b[0m (7.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,024,739</span> (7.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,022,819\u001b[0m (7.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,022,819</span> (7.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,920\u001b[0m (7.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> (7.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.0050 - loss: 4.6647\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00629, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 760ms/step - accuracy: 0.0052 - loss: 4.6627 - val_accuracy: 0.0063 - val_loss: 4.5762\n",
            "Epoch 2/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.0343 - loss: 4.4494\n",
            "Epoch 2: val_accuracy improved from 0.00629 to 0.01887, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 153ms/step - accuracy: 0.0342 - loss: 4.4488 - val_accuracy: 0.0189 - val_loss: 4.5697\n",
            "Epoch 3/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.0671 - loss: 4.3000\n",
            "Epoch 3: val_accuracy improved from 0.01887 to 0.03774, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.0662 - loss: 4.3021 - val_accuracy: 0.0377 - val_loss: 4.5666\n",
            "Epoch 4/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.0916 - loss: 4.2175\n",
            "Epoch 4: val_accuracy improved from 0.03774 to 0.04403, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.0912 - loss: 4.2185 - val_accuracy: 0.0440 - val_loss: 4.5641\n",
            "Epoch 5/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.1025 - loss: 4.1420\n",
            "Epoch 5: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.1023 - loss: 4.1412 - val_accuracy: 0.0189 - val_loss: 4.5637\n",
            "Epoch 6/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.1582 - loss: 3.9293\n",
            "Epoch 6: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.1572 - loss: 3.9308 - val_accuracy: 0.0189 - val_loss: 4.5772\n",
            "Epoch 7/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.1679 - loss: 3.8011\n",
            "Epoch 7: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 158ms/step - accuracy: 0.1684 - loss: 3.8003 - val_accuracy: 0.0252 - val_loss: 4.5880\n",
            "Epoch 8/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.2044 - loss: 3.7042\n",
            "Epoch 8: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 0.2060 - loss: 3.6998 - val_accuracy: 0.0252 - val_loss: 4.6183\n",
            "Epoch 9/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.2773 - loss: 3.4324\n",
            "Epoch 9: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.2782 - loss: 3.4297 - val_accuracy: 0.0314 - val_loss: 4.6593\n",
            "Epoch 10/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.3642 - loss: 3.1691\n",
            "Epoch 10: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.3632 - loss: 3.1693 - val_accuracy: 0.0314 - val_loss: 4.6587\n",
            "Epoch 11/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.4447 - loss: 2.8836\n",
            "Epoch 11: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 161ms/step - accuracy: 0.4438 - loss: 2.8841 - val_accuracy: 0.0314 - val_loss: 4.7755\n",
            "Epoch 12/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.4283 - loss: 2.8166\n",
            "Epoch 12: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.4291 - loss: 2.8118 - val_accuracy: 0.0252 - val_loss: 4.8020\n",
            "Epoch 13/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5414 - loss: 2.4323\n",
            "Epoch 13: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - accuracy: 0.5404 - loss: 2.4321 - val_accuracy: 0.0189 - val_loss: 4.8086\n",
            "Epoch 14/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5815 - loss: 2.2609\n",
            "Epoch 14: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 159ms/step - accuracy: 0.5822 - loss: 2.2586 - val_accuracy: 0.0189 - val_loss: 4.8110\n",
            "Epoch 15/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6305 - loss: 2.0011\n",
            "Epoch 15: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - accuracy: 0.6304 - loss: 2.0012 - val_accuracy: 0.0189 - val_loss: 4.7835\n",
            "Epoch 16/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6927 - loss: 1.7627\n",
            "Epoch 16: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - accuracy: 0.6922 - loss: 1.7635 - val_accuracy: 0.0189 - val_loss: 4.7246\n",
            "Epoch 17/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7091 - loss: 1.6048\n",
            "Epoch 17: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 158ms/step - accuracy: 0.7093 - loss: 1.6052 - val_accuracy: 0.0377 - val_loss: 4.6982\n",
            "Epoch 18/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.7310 - loss: 1.5288\n",
            "Epoch 18: val_accuracy did not improve from 0.04403\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 164ms/step - accuracy: 0.7313 - loss: 1.5256 - val_accuracy: 0.0377 - val_loss: 4.5396\n",
            "Epoch 19/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7843 - loss: 1.2817\n",
            "Epoch 19: val_accuracy improved from 0.04403 to 0.05660, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.7837 - loss: 1.2818 - val_accuracy: 0.0566 - val_loss: 4.4832\n",
            "Epoch 20/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8278 - loss: 1.1233\n",
            "Epoch 20: val_accuracy improved from 0.05660 to 0.08176, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.8276 - loss: 1.1226 - val_accuracy: 0.0818 - val_loss: 4.3774\n",
            "Epoch 21/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8711 - loss: 0.9783\n",
            "Epoch 21: val_accuracy improved from 0.08176 to 0.08805, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.8713 - loss: 0.9772 - val_accuracy: 0.0881 - val_loss: 4.1947\n",
            "Epoch 22/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8922 - loss: 0.8374\n",
            "Epoch 22: val_accuracy improved from 0.08805 to 0.09434, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.8920 - loss: 0.8376 - val_accuracy: 0.0943 - val_loss: 4.1012\n",
            "Epoch 23/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9212 - loss: 0.7334\n",
            "Epoch 23: val_accuracy improved from 0.09434 to 0.14465, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.9210 - loss: 0.7338 - val_accuracy: 0.1447 - val_loss: 3.8858\n",
            "Epoch 24/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9164 - loss: 0.6427\n",
            "Epoch 24: val_accuracy improved from 0.14465 to 0.15094, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.9165 - loss: 0.6432 - val_accuracy: 0.1509 - val_loss: 3.8138\n",
            "Epoch 25/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9674 - loss: 0.5679\n",
            "Epoch 25: val_accuracy improved from 0.15094 to 0.21384, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step - accuracy: 0.9671 - loss: 0.5678 - val_accuracy: 0.2138 - val_loss: 3.5228\n",
            "Epoch 26/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9742 - loss: 0.4959\n",
            "Epoch 26: val_accuracy improved from 0.21384 to 0.25157, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 0.9738 - loss: 0.4964 - val_accuracy: 0.2516 - val_loss: 3.3562\n",
            "Epoch 27/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9698 - loss: 0.4410\n",
            "Epoch 27: val_accuracy did not improve from 0.25157\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.9695 - loss: 0.4414 - val_accuracy: 0.2013 - val_loss: 3.3967\n",
            "Epoch 28/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9670 - loss: 0.3740\n",
            "Epoch 28: val_accuracy improved from 0.25157 to 0.26415, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.9674 - loss: 0.3741 - val_accuracy: 0.2642 - val_loss: 3.0314\n",
            "Epoch 29/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9876 - loss: 0.2866\n",
            "Epoch 29: val_accuracy improved from 0.26415 to 0.29560, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 171ms/step - accuracy: 0.9874 - loss: 0.2877 - val_accuracy: 0.2956 - val_loss: 2.8247\n",
            "Epoch 30/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9899 - loss: 0.2952\n",
            "Epoch 30: val_accuracy improved from 0.29560 to 0.44025, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 162ms/step - accuracy: 0.9897 - loss: 0.2958 - val_accuracy: 0.4403 - val_loss: 2.4444\n",
            "Epoch 31/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9883 - loss: 0.2424\n",
            "Epoch 31: val_accuracy improved from 0.44025 to 0.49686, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 0.9882 - loss: 0.2426 - val_accuracy: 0.4969 - val_loss: 2.1794\n",
            "Epoch 32/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9937 - loss: 0.2337\n",
            "Epoch 32: val_accuracy did not improve from 0.49686\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.9935 - loss: 0.2333 - val_accuracy: 0.4906 - val_loss: 2.0263\n",
            "Epoch 33/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9915 - loss: 0.2149\n",
            "Epoch 33: val_accuracy improved from 0.49686 to 0.57233, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 190ms/step - accuracy: 0.9916 - loss: 0.2146 - val_accuracy: 0.5723 - val_loss: 1.8356\n",
            "Epoch 34/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9899 - loss: 0.1927\n",
            "Epoch 34: val_accuracy improved from 0.57233 to 0.58491, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step - accuracy: 0.9900 - loss: 0.1924 - val_accuracy: 0.5849 - val_loss: 1.7294\n",
            "Epoch 35/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9969 - loss: 0.1539\n",
            "Epoch 35: val_accuracy improved from 0.58491 to 0.66038, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 156ms/step - accuracy: 0.9967 - loss: 0.1540 - val_accuracy: 0.6604 - val_loss: 1.4900\n",
            "Epoch 36/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9934 - loss: 0.1619\n",
            "Epoch 36: val_accuracy did not improve from 0.66038\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.9934 - loss: 0.1615 - val_accuracy: 0.6289 - val_loss: 1.5274\n",
            "Epoch 37/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9917 - loss: 0.1552\n",
            "Epoch 37: val_accuracy improved from 0.66038 to 0.77358, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - accuracy: 0.9917 - loss: 0.1549 - val_accuracy: 0.7736 - val_loss: 1.2411\n",
            "Epoch 38/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9923 - loss: 0.1479\n",
            "Epoch 38: val_accuracy did not improve from 0.77358\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - accuracy: 0.9924 - loss: 0.1478 - val_accuracy: 0.6730 - val_loss: 1.2921\n",
            "Epoch 39/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.1089\n",
            "Epoch 39: val_accuracy improved from 0.77358 to 0.81132, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.1090 - val_accuracy: 0.8113 - val_loss: 1.0846\n",
            "Epoch 40/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9986 - loss: 0.1044\n",
            "Epoch 40: val_accuracy did not improve from 0.81132\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.9986 - loss: 0.1047 - val_accuracy: 0.7987 - val_loss: 1.0491\n",
            "Epoch 41/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9988 - loss: 0.0922\n",
            "Epoch 41: val_accuracy improved from 0.81132 to 0.83648, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.9988 - loss: 0.0921 - val_accuracy: 0.8365 - val_loss: 0.9778\n",
            "Epoch 42/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9997 - loss: 0.0918\n",
            "Epoch 42: val_accuracy did not improve from 0.83648\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - accuracy: 0.9996 - loss: 0.0917 - val_accuracy: 0.8176 - val_loss: 0.9329\n",
            "Epoch 43/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9972 - loss: 0.0917\n",
            "Epoch 43: val_accuracy did not improve from 0.83648\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - accuracy: 0.9971 - loss: 0.0916 - val_accuracy: 0.8113 - val_loss: 0.8987\n",
            "Epoch 44/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9981 - loss: 0.0819\n",
            "Epoch 44: val_accuracy did not improve from 0.83648\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 160ms/step - accuracy: 0.9980 - loss: 0.0820 - val_accuracy: 0.8365 - val_loss: 0.8920\n",
            "Epoch 45/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 0.0758\n",
            "Epoch 45: val_accuracy improved from 0.83648 to 0.87421, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0759 - val_accuracy: 0.8742 - val_loss: 0.8303\n",
            "Epoch 46/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9979 - loss: 0.0812\n",
            "Epoch 46: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.9978 - loss: 0.0810 - val_accuracy: 0.8428 - val_loss: 0.8450\n",
            "Epoch 47/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9982 - loss: 0.0693\n",
            "Epoch 47: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - accuracy: 0.9981 - loss: 0.0695 - val_accuracy: 0.8491 - val_loss: 0.7910\n",
            "Epoch 48/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 0.0687\n",
            "Epoch 48: val_accuracy did not improve from 0.87421\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0685 - val_accuracy: 0.8491 - val_loss: 0.7602\n",
            "Epoch 49/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.0538\n",
            "Epoch 49: val_accuracy improved from 0.87421 to 0.89937, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.0538 - val_accuracy: 0.8994 - val_loss: 0.7011\n",
            "Epoch 50/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0510\n",
            "Epoch 50: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.0511 - val_accuracy: 0.8805 - val_loss: 0.7245\n",
            "Epoch 51/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.0500\n",
            "Epoch 51: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.0502 - val_accuracy: 0.8553 - val_loss: 0.6962\n",
            "Epoch 52/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 0.0446\n",
            "Epoch 52: val_accuracy did not improve from 0.89937\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0446 - val_accuracy: 0.8805 - val_loss: 0.7034\n",
            "Epoch 53/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9996 - loss: 0.0404\n",
            "Epoch 53: val_accuracy improved from 0.89937 to 0.90566, saving model to best_model.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9995 - loss: 0.0406 - val_accuracy: 0.9057 - val_loss: 0.6574\n",
            "Epoch 54/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9999 - loss: 0.0397\n",
            "Epoch 54: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.9998 - loss: 0.0398 - val_accuracy: 0.8616 - val_loss: 0.7377\n",
            "Epoch 55/55\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 0.0376\n",
            "Epoch 55: val_accuracy did not improve from 0.90566\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.0377 - val_accuracy: 0.8616 - val_loss: 0.7152\n",
            "Train Accuracy: 0.9722, Test Accuracy: 0.8737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Best Model & Tensorboard logs"
      ],
      "metadata": {
        "id": "idfvoQK7xRQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the best model\n",
        "files.download('best_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "t9FR1am9SPfv",
        "outputId": "0d2cf6ea-3bad-4a4e-e7c9-8bb6db93a866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5da13a48-adc8-4f76-9344-f4c1ab7be9b3\", \"best_model.keras\", 24386659)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Specify the correct path for the log directory\n",
        "log_dir = 'logs/fit/'  # Adjust this path if needed\n",
        "\n",
        "# Zip the log directory\n",
        "shutil.make_archive('/content/tensorboard_logs', 'zip', log_dir)\n",
        "\n",
        "# Provide a download link\n",
        "from google.colab import files\n",
        "files.download('/content/tensorboard_logs.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "T0Gn4wcRk5WO",
        "outputId": "640b55d5-c140-4eb0-a84a-545f97eed0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c5346d4-c3d8-4738-a52c-ac92cd78c83d\", \"tensorboard_logs.zip\", 19991773)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}